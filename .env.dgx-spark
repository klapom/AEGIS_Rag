# ==============================================================================
# AEGIS RAG - DGX Spark Environment Configuration
# ==============================================================================
# Optimized for NVIDIA DGX Spark (Grace Blackwell, ARM64, 128GB Unified Memory)
#
# Copy this file to .env and adjust values as needed
# Usage: docker compose -f docker-compose.dgx-spark.yml up -d
# ==============================================================================

# ==============================================================================
# Application Settings
# ==============================================================================
APP_NAME=aegis-rag
APP_VERSION=0.1.0
ENVIRONMENT=development
DEBUG=false
LOG_LEVEL=INFO
JSON_LOGS=false

# ==============================================================================
# API Server Configuration
# ==============================================================================
API_HOST=0.0.0.0
API_PORT=8000
API_WORKERS=4
API_RELOAD=true

# ==============================================================================
# LLM Configuration - Ollama with Qwen3 Models (DGX Spark)
# ==============================================================================
# Ollama Server (running in Docker)
OLLAMA_BASE_URL=http://ollama:11434

# ==============================================================================
# DGX SPARK MODELS (Pull after starting Ollama):
#
#   docker exec aegis-ollama ollama pull qwen3:32b
#   docker exec aegis-ollama ollama pull qwen3-vl:32b
#   docker exec aegis-ollama ollama pull nomic-embed-text
#
# Memory Usage:
#   - qwen3:32b     = ~20GB (Q4_K_M) - Generation + Entity Extraction
#   - qwen3-vl:32b  = ~21GB (Q4_K_M) - Vision Language Model
#   - Both loaded   = ~41GB + overhead (~50GB total)
#   - Reserve       = ~78GB for DB + System
# ==============================================================================

# Primary Generation Model (Qwen3 32B - comparable to Qwen2.5-72B!)
OLLAMA_MODEL_GENERATION=qwen3:32b

# Query Understanding (same model, excellent multilingual)
OLLAMA_MODEL_QUERY=qwen3:32b

# Entity/Relation Extraction (same model, high quality)
OLLAMA_MODEL_EXTRACTION=qwen3:32b

# Vision Language Model for images/PDFs
OLLAMA_MODEL_VLM=qwen3-vl:32b

# ==============================================================================
# VLM Backend Selection (Feature 36.8 - Sprint 36)
# ==============================================================================
# Choose VLM backend: "ollama" (local) or "dashscope" (cloud)
VLM_BACKEND=ollama

# ==============================================================================
# Embeddings - BGE-M3 (1024-dim, multilingual) via sentence-transformers
# ==============================================================================
# Sprint 35 Feature 35.8: High-performance GPU batch processing
# Throughput: ~500-1000 embeddings/sec (vs ~50-100 with Ollama HTTP)
# GPU Utilization: 90%+ (vs 30-50% with Ollama)
#
# Backend Selection:
EMBEDDING_BACKEND=sentence-transformers

# Sentence-Transformers Settings (only used if EMBEDDING_BACKEND=sentence-transformers):
ST_MODEL_NAME=BAAI/bge-m3
ST_DEVICE=auto
ST_BATCH_SIZE=64

# ==============================================================================
# Azure OpenAI (Optional - Production Fallback)
# ==============================================================================
USE_AZURE_LLM=false

# ==============================================================================
# Alibaba Cloud DashScope (Optional - Cloud Backup)
# ==============================================================================
# Useful when local Ollama is busy or for comparison
# ALIBABA_CLOUD_API_KEY=sk-...
# ALIBABA_CLOUD_BASE_URL=https://dashscope-intl.aliyuncs.com/compatible-mode/v1
# MONTHLY_BUDGET_ALIBABA_CLOUD=10.0

# ==============================================================================
# Qdrant Vector Database
# ==============================================================================
QDRANT_HOST=qdrant
QDRANT_PORT=6333
QDRANT_GRPC_PORT=6334
QDRANT_COLLECTION=documents_v1
QDRANT_USE_GRPC=true

# ==============================================================================
# Neo4j Graph Database
# ==============================================================================
NEO4J_URI=bolt://neo4j:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=aegis-rag-neo4j-password
NEO4J_DATABASE=neo4j

# ==============================================================================
# Redis Cache & Short-Term Memory
# ==============================================================================
REDIS_HOST=redis
REDIS_PORT=6379
REDIS_DB=0
REDIS_TTL=3600

# ==============================================================================
# Docling Container (ARM64 Build)
# ==============================================================================
# Build first: docker build -f docker/Dockerfile.docling-spark -t aegis-docling-spark:latest .
DOCLING_BASE_URL=http://docling:5001
DOCLING_TIMEOUT_SECONDS=900
DOCLING_MAX_RETRIES=3

# ==============================================================================
# LangSmith Observability (Optional)
# ==============================================================================
LANGSMITH_TRACING=false

# ==============================================================================
# MCP Server
# ==============================================================================
MCP_SERVER_PORT=3000
MCP_AUTH_ENABLED=false

# ==============================================================================
# Performance Configuration (DGX Spark Optimized)
# ==============================================================================
MAX_CONCURRENT_REQUESTS=100
REQUEST_TIMEOUT=120

# ==============================================================================
# Retrieval Configuration
# ==============================================================================
RETRIEVAL_TOP_K=10
RETRIEVAL_SCORE_THRESHOLD=0.65
