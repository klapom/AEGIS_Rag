# ==============================================================================
# AEGIS RAG - DGX Spark Environment Configuration
# ==============================================================================
# Optimized for NVIDIA DGX Spark (Grace Blackwell, ARM64, 128GB Unified Memory)
#
# Copy this file to .env and adjust values as needed
# Usage: docker compose -f docker-compose.dgx-spark.yml up -d
# ==============================================================================

# ==============================================================================
# Network Settings (Sprint 92: External Access)
# ==============================================================================
# External IP of the DGX Spark for browser access
# The frontend runs in Docker but the browser needs the external IP
DGX_SPARK_IP=192.168.178.10

# ==============================================================================
# Application Settings
# ==============================================================================
APP_NAME=aegis-rag
APP_VERSION=0.1.0
ENVIRONMENT=development
DEBUG=false
LOG_LEVEL=INFO
JSON_LOGS=false

# ==============================================================================
# API Server Configuration
# ==============================================================================
API_HOST=0.0.0.0
API_PORT=8000
API_WORKERS=4  # DGX Spark: More workers with 20 CPU cores
API_RELOAD=true

# ==============================================================================
# LLM Configuration - Ollama with Qwen3 Models (DGX Spark)
# ==============================================================================
# Ollama Server (running in Docker)
OLLAMA_BASE_URL=http://localhost:11434

# ==============================================================================
# DGX SPARK MODELS (Pull after starting Ollama):
#
#   docker exec aegis-ollama ollama pull qwen3:32b
#   docker exec aegis-ollama ollama pull qwen3-vl:32b
#   docker exec aegis-ollama ollama pull nomic-embed-text
#
# Memory Usage:
#   - qwen3:32b     = ~20GB (Q4_K_M) - Generation + Entity Extraction
#   - qwen3-vl:32b  = ~21GB (Q4_K_M) - Vision Language Model
#   - Both loaded   = ~41GB + overhead (~50GB total)
#   - Reserve       = ~78GB for DB + System
# ==============================================================================

# Primary Generation Model (Sprint 48: Nemotron-No-Think for better RAG responses)
OLLAMA_MODEL_GENERATION=nemotron-no-think:latest

# Query Understanding (same model, excellent multilingual)
OLLAMA_MODEL_QUERY=qwen3:32b

# Entity/Relation Extraction (same model, high quality)
OLLAMA_MODEL_EXTRACTION=qwen3:32b

# Vision Language Model for images/PDFs
OLLAMA_MODEL_VLM=qwen3-vl:32b

# ==============================================================================
# VLM Backend Selection (Feature 36.8 - Sprint 36)
# ==============================================================================
# Choose VLM backend: "ollama" (local) or "dashscope" (cloud)
VLM_BACKEND=ollama  # Default: local-first for DGX Spark

# ==============================================================================
# Embeddings - BGE-M3 (1024-dim, multilingual) via sentence-transformers
# ==============================================================================
# Sprint 35 Feature 35.8: High-performance GPU batch processing
# Throughput: ~500-1000 embeddings/sec (vs ~50-100 with Ollama HTTP)
# GPU Utilization: 90%+ (vs 30-50% with Ollama)
#
# Backend Selection:
EMBEDDING_BACKEND=sentence-transformers  # Use 'ollama' for HTTP API (backward compatible)

# Sentence-Transformers Settings (only used if EMBEDDING_BACKEND=sentence-transformers):
ST_MODEL_NAME=BAAI/bge-m3  # HuggingFace model (1024-dim, multilingual)
ST_DEVICE=auto             # Device: 'auto' (CUDA if available), 'cuda', 'cpu'
ST_BATCH_SIZE=64           # Batch size for GPU processing (64 optimal for most GPUs)

# Note: Model is automatically downloaded from HuggingFace on first use (~400MB)
# Cache location: ~/.cache/huggingface/hub/

# ==============================================================================
# Alternative: Qwen2.5-72B for Maximum Quality (Optional)
# ==============================================================================
# If you want the absolute best quality and have the memory:
#   docker exec aegis-ollama ollama pull qwen2.5:72b
#
# Memory: ~42GB (Q4_K_M) - leaves ~65GB for VLM + DBs
# Uncomment below to use:
# OLLAMA_MODEL_GENERATION=qwen2.5:72b
# OLLAMA_MODEL_EXTRACTION=qwen2.5:72b

# ==============================================================================
# Azure OpenAI (Optional - Production Fallback)
# ==============================================================================
USE_AZURE_LLM=false
# AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com
# AZURE_OPENAI_API_KEY=your-api-key-here

# ==============================================================================
# Alibaba Cloud DashScope (Optional - Cloud Backup)
# ==============================================================================
# Useful when local Ollama is busy or for comparison
# ALIBABA_CLOUD_API_KEY=sk-...
# ALIBABA_CLOUD_BASE_URL=https://dashscope-intl.aliyuncs.com/compatible-mode/v1
# MONTHLY_BUDGET_ALIBABA_CLOUD=10.0

# ==============================================================================
# Qdrant Vector Database
# ==============================================================================
QDRANT_HOST=localhost
QDRANT_PORT=6333
QDRANT_GRPC_PORT=6334
QDRANT_COLLECTION=documents_v1
QDRANT_USE_GRPC=true  # gRPC is faster, DGX Spark can handle it

# ==============================================================================
# Neo4j Graph Database
# ==============================================================================
NEO4J_URI=bolt://localhost:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=aegis-rag-neo4j-password  # CHANGE THIS!
NEO4J_DATABASE=neo4j

# ==============================================================================
# Redis Cache & Short-Term Memory
# ==============================================================================
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_DB=0
REDIS_TTL=3600

# ==============================================================================
# Docling Container (ARM64 Build)
# ==============================================================================
# Build first: docker build -f docker/Dockerfile.docling-spark -t aegis-docling-spark:latest .
DOCLING_BASE_URL=http://localhost:8080
DOCLING_TIMEOUT_SECONDS=900
DOCLING_MAX_RETRIES=3

# ==============================================================================
# LangSmith Observability (Optional)
# ==============================================================================
LANGSMITH_TRACING=false
# LANGSMITH_API_KEY=your-langsmith-api-key
# LANGSMITH_PROJECT=aegis-rag-dgx-spark

# ==============================================================================
# MCP Server
# ==============================================================================
MCP_SERVER_PORT=3000
MCP_AUTH_ENABLED=false

# ==============================================================================
# Performance Configuration (DGX Spark Optimized)
# ==============================================================================
MAX_CONCURRENT_REQUESTS=100  # DGX Spark: Higher concurrency
REQUEST_TIMEOUT=120  # Longer timeout for large models

# ==============================================================================
# Retrieval Configuration
# ==============================================================================
RETRIEVAL_TOP_K=10  # More results with better models
RETRIEVAL_SCORE_THRESHOLD=0.65  # Slightly lower threshold, rely on reranking

# ==============================================================================
# DGX Spark Quick Start Guide
# ==============================================================================
#
# 1. Build the Docling ARM64 image:
#    docker build -f docker/Dockerfile.docling-spark -t aegis-docling-spark:latest .
#
# 2. Start services:
#    docker compose -f docker-compose.dgx-spark.yml up -d
#
# 3. Pull Qwen3 models (takes ~10-15 minutes first time):
#    docker exec aegis-ollama ollama pull qwen3:32b
#    docker exec aegis-ollama ollama pull qwen3-vl:32b
#    # Note: Embeddings use BGE-M3 via sentence-transformers, not Ollama
#
# 4. Verify models are loaded:
#    docker exec aegis-ollama ollama list
#
# 5. Start Docling for document ingestion:
#    docker compose -f docker-compose.dgx-spark.yml --profile ingestion up -d docling
#
# 6. Access services:
#    - API: http://localhost:8000
#    - Qdrant UI: http://localhost:6333/dashboard
#    - Neo4j Browser: http://localhost:7474
#    - Grafana: http://localhost:3000
#    - Prometheus: http://localhost:9090
#
# ==============================================================================
# Memory Distribution Summary (128GB Unified Memory)
# ==============================================================================
#
# | Component          | Memory  | Notes                           |
# |--------------------|---------|----------------------------------|
# | Qwen3-32B          | ~20GB   | Generation + Entity Extraction   |
# | Qwen3-VL-32B       | ~21GB   | Vision Language Model            |
# | Docling            | ~8GB    | OCR + Layout (when running)      |
# | Qdrant             | ~8GB    | Vector DB                        |
# | Neo4j              | ~8GB    | Graph DB (4GB heap + pagecache)  |
# | Redis              | ~4GB    | Cache                            |
# | Prometheus/Grafana | ~1.5GB  | Monitoring                       |
# | System + Overhead  | ~10GB   | OS, Docker, etc.                 |
# |--------------------|---------|----------------------------------|
# | TOTAL USED         | ~80GB   | 63% of 128GB                     |
# | RESERVE            | ~48GB   | For concurrent model loading     |
#
# ==============================================================================
