# Sprint 13 Plan: Test Infrastructure & Performance Optimization

**Sprint Goal:** Achieve 70%+ E2E test pass rate and optimize backend performance
**Duration:** 1-2 Wochen
**Story Points:** 16 SP
**Start Date:** TBD
**Branch:** `sprint-13-dev`
**Theme:** Backend Excellence - Testing & Performance

---

## Executive Summary

Sprint 13 focuses **exclusively on test infrastructure completion and backend performance optimization**. With production deployment readiness achieved in Sprint 12, we now prioritize increasing test coverage from ~38% to 70%+ and implementing critical performance improvements.

**Why This Focus?**
- Test coverage is critical for production stability
- All features are backend/testing related (no context switching)
- Performance optimization naturally pairs with test improvements
- Prepares system for real production deployments
- React migration deferred to Sprint 14 (separate frontend focus)

### Sprint 13 Priorities

1. **E2E Test Fixes (8 SP):** Fix all 27 failing tests from Sprint 12
   - Memory agent event loop cleanup (4 errors)
   - Graphiti API compatibility (18 skipped tests)
   - LightRAG fixture connection stability (5 tests)
   - pytest-timeout integration

2. **CI/CD Pipeline Finalization (3 SP):** Test timeout, parallel execution, coverage reporting

3. **Performance Optimization (5 SP):** Community detection caching, LLM batching, cache invalidation

---

## Features Overview

| ID | Feature | SP | Priority | Dependencies | Status |
|----|---------|----|---------| -------------|--------|
| **CRITICAL TEST FIXES** |
| 13.1 | Fix Memory Agent Event Loop Errors | 2 | ðŸ”´ CRITICAL | None | ðŸ“‹ TODO |
| 13.2 | Fix Graphiti API Compatibility | 3 | ðŸ”´ CRITICAL | None | ðŸ“‹ TODO |
| 13.3 | Fix LightRAG Fixture Connection | 2 | ðŸ”´ CRITICAL | None | ðŸ“‹ TODO |
| **CI/CD ENHANCEMENTS** |
| 13.4 | Add pytest-timeout Plugin | 1 | ðŸŸ  HIGH | None | ðŸ“‹ TODO |
| 13.5 | CI/CD Pipeline Enhancements | 3 | ðŸŸ  HIGH | 13.4 | ðŸ“‹ TODO |
| **PERFORMANCE OPTIMIZATION** |
| 13.6 | Community Detection Caching | 2 | ðŸŸ  HIGH | None | ðŸ“‹ TODO |
| 13.7 | LLM Labeling Batching | 2 | ðŸŸ  HIGH | None | ðŸ“‹ TODO |
| 13.8 | Cache Invalidation Patterns | 1 | ðŸŸ¡ MEDIUM | None | ðŸ“‹ TODO |
| **TOTAL** | | **16** | | | |

---

## Critical Test Infrastructure Fixes (8 SP)

### Feature 13.1: Fix Memory Agent Event Loop Errors (2 SP)

**Priority:** ðŸ”´ CRITICAL
**Category:** Testing / AsyncIO
**Technical Debt:** TD-26 (NEW)

#### Current Issue

4 integration tests fail during teardown with event loop errors:

```
ERROR at teardown of test_memory_agent_process_with_coordinator_e2e
ERROR at teardown of test_memory_agent_state_management_e2e
ERROR at teardown of test_memory_agent_latency_target_e2e
ERROR at teardown of test_session_context_endpoint_e2e

RuntimeError: Event loop is closed
RuntimeError: Task <Task pending> got Future attached to a different loop
```

**Tests Affected:**
- `tests/integration/agents/test_memory_agent_e2e.py` (3 tests)
- `tests/integration/api/test_memory_api_e2e.py` (1 test)

#### Root Cause Analysis

**Hypothesis 1:** Async fixture cleanup timing
- Memory agent fixtures may be using different event loops
- Cleanup happens after pytest event loop is closed

**Hypothesis 2:** Graphiti async cleanup incomplete
- Similar to Redis async cleanup (TD-25, resolved in Sprint 12)
- Graphiti wrapper needs proper aclose() method

#### Solution

1. **Add aclose() to GraphitiWrapper:**
   ```python
   # src/components/memory/graphiti_wrapper.py
   class GraphitiWrapper:
       async def aclose(self) -> None:
           """Close Graphiti client and Neo4j connections."""
           if self.client:
               await self.client.close()  # If Graphiti has async close
               self.client = None
   ```

2. **Update conftest.py fixture:**
   ```python
   # tests/conftest.py
   @pytest.fixture
   async def graphiti_wrapper(neo4j_driver):
       wrapper = GraphitiWrapper()
       yield wrapper
       await wrapper.aclose()  # Proper cleanup
   ```

3. **Ensure event loop consistency:**
   ```python
   # tests/conftest.py
   @pytest.fixture(scope="function")
   def event_loop():
       """Create event loop for each test."""
       loop = asyncio.new_event_loop()
       yield loop
       loop.close()
   ```

#### Acceptance Criteria
- [ ] All 4 memory agent tests pass without teardown errors
- [ ] No "Event loop is closed" errors
- [ ] No "Task attached to different loop" errors
- [ ] GraphitiWrapper has proper async cleanup

#### Verification
```bash
poetry run pytest tests/integration/agents/test_memory_agent_e2e.py -v
poetry run pytest tests/integration/api/test_memory_api_e2e.py -v
# Expected: 4 tests PASSED, 0 errors
```

---

### Feature 13.2: Fix Graphiti API Compatibility (3 SP)

**Priority:** ðŸ”´ CRITICAL
**Category:** Integration / Dependencies
**Technical Debt:** TD-27 (NEW)

#### Current Issue

18 Graphiti tests skipped due to constructor signature change:

```
SKIPPED: Graphiti not available: Failed to initialize Graphiti:
Graphiti.__init__() got an unexpected keyword argument 'neo4j_uri'
```

**Tests Affected:**
- `tests/integration/memory/test_graphiti_e2e.py` (18 tests)

#### Root Cause Analysis

**Graphiti Library Breaking Change:**
- Sprint 12 Feature 12.2 fixed `generate_response()` â†’ `_generate_response()`
- However, constructor signature also changed
- Old: `Graphiti(neo4j_uri="...", neo4j_user="...", neo4j_password="...")`
- New: Unknown constructor signature (need to check Graphiti docs)

#### Investigation Steps

1. **Check Graphiti library version:**
   ```bash
   poetry show graphiti-core
   ```

2. **Review Graphiti documentation:**
   - Check official docs for current constructor API
   - Check GitHub releases for breaking changes

3. **Inspect current GraphitiWrapper implementation:**
   ```bash
   grep -A 20 "class GraphitiWrapper" src/components/memory/graphiti_wrapper.py
   ```

#### Solution (Placeholder - Needs Investigation)

**Option A: Update Constructor Signature**
```python
# src/components/memory/graphiti_wrapper.py
class GraphitiWrapper:
    def __init__(self):
        # OLD (Sprint 7):
        # self.client = Graphiti(
        #     neo4j_uri=settings.neo4j_uri,
        #     neo4j_user=settings.neo4j_user,
        #     neo4j_password=settings.neo4j_password,
        # )

        # NEW (Sprint 13 - needs verification):
        self.client = Graphiti(
            neo4j_config={
                "uri": settings.neo4j_uri,
                "user": settings.neo4j_user,
                "password": settings.neo4j_password,
            },
            # Other required params?
        )
```

**Option B: Pin Graphiti Version**
```toml
# pyproject.toml
[tool.poetry.dependencies]
graphiti-core = "0.2.0"  # Pin to working version
```

**Option C: Use Graphiti Builder Pattern**
```python
self.client = GraphitiBuilder()
    .with_neo4j(uri, user, password)
    .with_llm(ollama_client)
    .build()
```

#### Acceptance Criteria
- [ ] GraphitiWrapper initializes without errors
- [ ] 18 skipped tests now run
- [ ] Graphiti integration tests pass
- [ ] Memory system functional with Graphiti

#### Verification
```bash
poetry run pytest tests/integration/memory/test_graphiti_e2e.py -v
# Expected: 18 tests PASSED (or at least attempted, not skipped)
```

---

### Feature 13.3: Fix LightRAG Fixture Connection (2 SP)

**Priority:** ðŸ”´ CRITICAL
**Category:** Testing / Integration
**Technical Debt:** TD-28 (NEW)

#### Current Issue

5 Sprint 5 E2E tests updated to use `lightrag_instance` fixture (Feature 12.1) but tests fail at setup:

```
ERROR: fixture 'lightrag_instance' not found
```

**Tests Affected:**
- `tests/integration/test_sprint5_critical_e2e.py::test_graph_construction_full_pipeline_e2e`
- `tests/integration/test_sprint5_critical_e2e.py::test_local_search_entity_level_e2e`
- `tests/integration/test_sprint5_critical_e2e.py::test_global_search_topic_level_e2e`
- `tests/integration/test_sprint5_critical_e2e.py::test_hybrid_search_local_global_e2e`
- `tests/integration/test_sprint5_critical_e2e.py::test_incremental_graph_updates_e2e`

#### Root Cause Analysis

**Sprint 12 Feature 12.1 Documentation states:**
> "Added lightrag_instance fixture parameter to all 5 tests"

**However:**
- SPRINT_12_E2E_TEST_REPORT.md confirms: `fixture 'lightrag_instance' not found`
- Fixture exists in `tests/conftest.py:41-85` (Sprint 11 implementation)
- **Possible causes:**
  1. Fixture exists but Neo4j connection timing issues
  2. Fixture name mismatch (lightrag_instance vs lightrag_wrapper)
  3. Fixture scope issue (session vs function)

#### Investigation Steps

1. **Check if fixture exists:**
   ```bash
   pytest --fixtures | grep lightrag
   ```

2. **Review fixture implementation:**
   ```bash
   grep -A 30 "def lightrag_instance" tests/conftest.py
   ```

3. **Check test signatures:**
   ```bash
   grep -B 2 "async def test.*e2e.*lightrag" tests/integration/test_sprint5_critical_e2e.py
   ```

#### Solution

**Step 1: Verify/Create lightrag_instance Fixture**
```python
# tests/conftest.py
@pytest.fixture(scope="session")
async def lightrag_instance(neo4j_driver, ollama_client_real):
    """LightRAG singleton with Neo4j cleanup.

    Sprint 11: Uses singleton LightRAG instance (avoids re-initialization)
    but cleans Neo4j database before each test for isolation.
    """
    from src.components.graph_rag.lightrag_wrapper import LightRAGWrapper, reset_singleton

    # Clean Neo4j before tests
    async with neo4j_driver.session() as session:
        await session.run("MATCH (n) DETACH DELETE n")

    # Reset singleton to force re-initialization with clean DB
    reset_singleton()

    # Get singleton instance
    wrapper = await get_lightrag_wrapper_async()

    yield wrapper

    # Cleanup after session
    reset_singleton()
```

**Step 2: Ensure Test Uses Fixture**
```python
# tests/integration/test_sprint5_critical_e2e.py
@pytest.mark.asyncio
async def test_graph_construction_full_pipeline_e2e(lightrag_instance):
    """Test full LightRAG pipeline."""
    wrapper = lightrag_instance  # Use fixture

    # Test continues...
```

**Step 3: Handle Async Initialization**
```python
# If fixture needs to be function-scoped for isolation:
@pytest.fixture(scope="function")  # Changed from session
async def lightrag_instance(neo4j_driver):
    # Cleanup, init, yield, cleanup
    ...
```

#### Acceptance Criteria
- [ ] Fixture is discoverable via `pytest --fixtures`
- [ ] 5 tests run without "fixture not found" error
- [ ] Tests can access lightrag_instance
- [ ] Neo4j database is properly cleaned between tests
- [ ] No pickle errors occur

#### Verification
```bash
pytest --fixtures | grep lightrag  # Fixture exists
poetry run pytest tests/integration/test_sprint5_critical_e2e.py::test_graph_construction_full_pipeline_e2e -v
# Expected: PASSED (not ERROR at setup)
```

---

## CI/CD Pipeline Enhancements (4 SP)

### Feature 13.4: Add pytest-timeout Plugin (1 SP)

**Priority:** ðŸŸ  HIGH
**Category:** Testing / DevOps
**Technical Debt:** TD-29 (NEW)

#### Current Issue

```bash
poetry run pytest tests/integration/ --timeout=300
ERROR: unrecognized arguments: --timeout=300
```

**Impact:**
- Cannot enforce test timeouts locally
- Long-running tests can hang development workflow
- CI/CD has timeout enforcement, but local dev doesn't

#### Solution

**Step 1: Add to pyproject.toml**
```toml
# pyproject.toml
[tool.poetry.group.dev.dependencies]
pytest-timeout = "^2.2.0"
```

**Step 2: Install**
```bash
poetry install --with dev
```

**Step 3: Configure in pytest.ini**
```ini
# pytest.ini
[pytest]
# Timeout for tests (prevent hanging)
timeout = 300  # 5 minutes default
timeout_method = thread  # or signal (Unix only)
```

**Step 4: Use in Tests**
```python
# For specific test with custom timeout
@pytest.mark.timeout(600)  # 10 minutes
async def test_large_document_processing_e2e():
    ...
```

#### Acceptance Criteria
- [ ] pytest-timeout installed in dev dependencies
- [ ] --timeout flag recognized
- [ ] Default 300s timeout configured
- [ ] Tests fail after timeout (not hang)

#### Verification
```bash
poetry show pytest-timeout  # Installed
poetry run pytest --version  # Shows timeout plugin
poetry run pytest tests/integration/ --timeout=10  # Should timeout some tests
```

---

### Feature 13.5: CI/CD Pipeline Enhancements (3 SP)

**Priority:** ðŸŸ  HIGH
**Category:** DevOps / CI/CD
**Dependencies:** Feature 13.4

#### Current Status

Sprint 12 CI/CD improvements:
- âœ… Ollama service added
- âœ… 20min timeout for integration tests
- âœ… Docker cache configured
- âœ… Model pulling automated

**Remaining Improvements Needed:**
1. Test timeout enforcement in CI
2. Parallel test execution
3. Coverage reporting integration
4. Test result artifacts

#### Solution

**1. Enable pytest-timeout in CI:**
```yaml
# .github/workflows/ci.yml
- name: Run Integration Tests
  timeout-minutes: 20
  run: |
    poetry run pytest tests/integration/ \
      --cov=src \
      --cov-report=xml \
      --timeout=300 \      # NEW: Per-test timeout
      --timeout-method=thread \
      -v
```

**2. Parallel Test Execution:**
```yaml
# Install pytest-xdist
[tool.poetry.group.dev.dependencies]
pytest-xdist = "^3.5.0"

# Run tests in parallel
poetry run pytest tests/integration/ -n auto  # Use all CPU cores
```

**3. Coverage Reporting:**
```yaml
# .github/workflows/ci.yml
- name: Upload Coverage to Codecov
  uses: codecov/codecov-action@v4
  with:
    file: ./coverage.xml
    flags: integration
    fail_ci_if_error: true  # NEW: Fail if coverage upload fails
```

**4. Test Result Artifacts:**
```yaml
# .github/workflows/ci.yml
- name: Upload Test Results
  if: always()
  uses: actions/upload-artifact@v4
  with:
    name: test-results
    path: |
      test_*.txt
      pytest_report.html
      coverage.xml
```

#### Acceptance Criteria
- [ ] pytest-timeout enabled in CI
- [ ] Parallel test execution (pytest-xdist)
- [ ] Coverage reports uploaded to Codecov
- [ ] Test results available as artifacts
- [ ] CI fails fast on first test failure (optional: --maxfail=1)

#### Verification
```bash
# Local parallel execution test
poetry install --with dev
poetry run pytest tests/unit/ -n auto -v
# Expected: Tests run in parallel, faster completion
```

---

## Performance Optimization (5 SP)

### Feature 13.6: Community Detection Caching (2 SP)

**Priority:** ðŸŸ  HIGH
**Category:** Performance
**Technical Debt:** TD-09

#### Current Issue

Community detection is slower than target (30s vs 5s for 1000 nodes) due to NetworkX fallback.

**Impact:**
- â±ï¸ Medium - acceptable for batch, not real-time
- âœ… Auto-detects and uses GDS when available
- âš ï¸ Repeated calls re-compute from scratch

#### Solution

**1. Add Redis Cache Layer:**
```python
# src/components/graph_rag/community_detector.py
from redis import Redis
import hashlib
import pickle

class CommunityDetector:
    def __init__(self, redis_client: Redis):
        self.redis = redis_client
        self.cache_ttl = 3600  # 1 hour

    async def detect_communities(
        self,
        graph_id: str,
        algorithm: str = "louvain"
    ) -> list[dict]:
        """Detect communities with Redis caching."""

        # Generate cache key
        cache_key = f"communities:{graph_id}:{algorithm}"

        # Check cache
        cached = await self.redis.get(cache_key)
        if cached:
            logger.info("community_detection_cache_hit", graph_id=graph_id)
            return pickle.loads(cached)

        # Cache miss - compute
        logger.info("community_detection_cache_miss", graph_id=graph_id)
        communities = await self._compute_communities(graph_id, algorithm)

        # Store in cache
        await self.redis.setex(
            cache_key,
            self.cache_ttl,
            pickle.dumps(communities)
        )

        return communities
```

**2. Cache Invalidation on Graph Updates:**
```python
async def invalidate_community_cache(self, graph_id: str):
    """Invalidate community detection cache when graph changes."""
    pattern = f"communities:{graph_id}:*"
    keys = await self.redis.keys(pattern)
    if keys:
        await self.redis.delete(*keys)
        logger.info("community_cache_invalidated", graph_id=graph_id, keys=len(keys))
```

#### Acceptance Criteria
- [ ] Community detection results cached in Redis
- [ ] Cache hit reduces latency from 30s â†’ <100ms
- [ ] Cache invalidated on graph updates
- [ ] TTL configured (1 hour default)
- [ ] Cache hit/miss metrics logged

#### Verification
```bash
# First call (cache miss)
time poetry run python -c "from src.components.graph_rag.community_detector import detect_communities; detect_communities('test_graph')"
# Expected: ~30s

# Second call (cache hit)
time poetry run python -c "from src.components.graph_rag.community_detector import detect_communities; detect_communities('test_graph')"
# Expected: <1s
```

---

### Feature 13.7: LLM Labeling Batching (2 SP)

**Priority:** ðŸŸ  HIGH
**Category:** Performance
**Technical Debt:** TD-15

#### Current Issue

LLM community labeling takes ~5s per community (sequential processing).

**Impact:**
- â±ï¸ Medium for batch processing
- âœ… Acceptable for current scale
- âš ï¸ Doesn't scale to 100+ communities

#### Solution

**1. Batch Multiple Communities in Single Prompt:**
```python
# src/components/graph_rag/community_labeler.py
class CommunityLabeler:
    def __init__(self, batch_size: int = 5):
        self.batch_size = batch_size

    async def label_communities_batch(
        self,
        communities: list[dict]
    ) -> dict[int, str]:
        """Label communities in batches using single LLM call."""

        labels = {}

        # Process in batches
        for i in range(0, len(communities), self.batch_size):
            batch = communities[i:i + self.batch_size]

            # Create batch prompt
            prompt = self._create_batch_prompt(batch)

            # Single LLM call for entire batch
            response = await self.llm.ainvoke(prompt)

            # Parse batch response
            batch_labels = self._parse_batch_response(response.content, batch)
            labels.update(batch_labels)

        return labels

    def _create_batch_prompt(self, communities: list[dict]) -> str:
        """Create prompt for batch labeling."""
        return f"""
Label the following {len(communities)} communities based on their entities and relationships.

For each community, provide a concise 2-3 word label that describes the main theme.

Communities:
{self._format_communities(communities)}

Respond in JSON format:
{{
  "community_1": "label",
  "community_2": "label",
  ...
}}
"""
```

**2. Parallel Batch Processing:**
```python
import asyncio

async def label_communities_parallel(
    self,
    communities: list[dict],
    max_concurrency: int = 3
) -> dict[int, str]:
    """Label communities with parallel batch processing."""

    # Split into batches
    batches = [communities[i:i + self.batch_size]
               for i in range(0, len(communities), self.batch_size)]

    # Process batches in parallel (with concurrency limit)
    semaphore = asyncio.Semaphore(max_concurrency)

    async def process_batch(batch):
        async with semaphore:
            return await self.label_communities_batch(batch)

    # Execute parallel
    results = await asyncio.gather(*[process_batch(b) for b in batches])

    # Merge results
    return {k: v for result in results for k, v in result.items()}
```

#### Acceptance Criteria
- [ ] Batches 5 communities per LLM call (configurable)
- [ ] Reduces total labeling time by 4-5x
- [ ] Parallel batch processing with concurrency limit
- [ ] JSON parsing handles batch responses
- [ ] Falls back to sequential if batch fails

#### Verification
```bash
# Test with 20 communities
# Sequential: 20 communities Ã— 5s = 100s
# Batch (5 per call): 4 calls Ã— 6s = 24s (4x faster)
# Parallel batch (3 concurrent): 2 rounds Ã— 6s = 12s (8x faster)
```

---

### Feature 13.8: Cache Invalidation Patterns (1 SP)

**Priority:** ðŸŸ¡ MEDIUM
**Category:** Performance
**Technical Debt:** TD-11

#### Current Issue

Cache invalidation uses simple string matching, not regex patterns.

**Impact:**
- âš ï¸ Low - invalidation is conservative (invalidates all on writes)
- âœ… Works correctly for current use cases

#### Solution

**Upgrade to Pattern-Based Invalidation:**
```python
# src/core/cache.py
import re

class CacheManager:
    async def invalidate_pattern(self, pattern: str) -> int:
        """Invalidate cache entries matching regex pattern.

        Args:
            pattern: Regex pattern (e.g., "query:.*:user_123")

        Returns:
            Number of keys invalidated
        """
        all_keys = await self.redis.keys("*")
        matched_keys = [
            key for key in all_keys
            if re.match(pattern, key.decode())
        ]

        if matched_keys:
            await self.redis.delete(*matched_keys)
            logger.info(
                "cache_pattern_invalidated",
                pattern=pattern,
                keys_deleted=len(matched_keys)
            )

        return len(matched_keys)

# Usage examples:
await cache.invalidate_pattern(r"query:.*:user_123")  # All queries for user
await cache.invalidate_pattern(r"communities:graph_\d+:.*")  # All community caches
await cache.invalidate_pattern(r".*:2025-10-22.*")  # All caches from specific date
```

#### Acceptance Criteria
- [ ] Regex pattern matching implemented
- [ ] Can invalidate by user, date, graph, etc.
- [ ] Number of keys deleted returned
- [ ] Logging shows pattern and count
- [ ] Backward compatible with simple string invalidation

#### Verification
```python
# Test pattern invalidation
await cache.set("query:simple:user_1", "...")
await cache.set("query:complex:user_1", "...")
await cache.set("query:simple:user_2", "...")

count = await cache.invalidate_pattern(r"query:.*:user_1")
assert count == 2  # Only user_1 queries invalidated
```

---

## Sprint 13 Success Criteria

### Must Have (Critical Path)
- [ ] E2E test pass rate â‰¥70% (currently ~38%)
- [ ] All 4 memory agent event loop errors resolved
- [ ] All 18 Graphiti API compatibility tests passing
- [ ] All 5 LightRAG fixture connection tests passing
- [ ] pytest-timeout integrated
- [ ] CI/CD parallel test execution working

### Should Have (High Value)
- [ ] Community detection caching operational
- [ ] LLM labeling batching implemented
- [ ] Cache invalidation patterns functional
- [ ] Coverage reporting to Codecov

### Could Have (If Time Permits)
- [ ] Additional performance benchmarks
- [ ] Test timeout fine-tuning per test type
- [ ] Cache warming strategies

---

## Known Risks & Mitigation

### Risk 1: Graphiti API Investigation Time
**Risk:** Graphiti constructor signature unknown, may take time to investigate
**Mitigation:** Allocate 3 SP (includes investigation), consider pinning version as fallback

### Risk 2: Event Loop Complexity
**Risk:** Async fixture timing issues may be deeper than expected
**Mitigation:** 2 SP allocated, can defer complex cases to Sprint 14 if needed

### Risk 3: Performance Gains Uncertain
**Risk:** Caching may not provide expected speedup in all scenarios
**Mitigation:** Implement with metrics, validate before/after benchmarks

---

## Sprint 13 Velocity Planning

**Planned:** 16 SP (focused sprint)
**Breakdown:**
- Critical Test Fixes: 8 SP (50%)
- CI/CD Enhancements: 4 SP (25%)
- Performance Optimization: 5 SP (31%) - rounded up from 4.96

**Timeline:**
- Week 1: Test Fixes + CI/CD (12 SP)
- Week 2: Performance Optimization (4-5 SP, buffer time)

**Buffer:** Built-in buffer with 1-2 week duration flexibility

---

## Next Sprint Preview

### Sprint 14: React Frontend Migration - Phase 1 (15 SP, 2 weeks)
- React + Next.js project setup
- Basic chat UI with streaming responses
- NextAuth.js authentication
- Tailwind CSS styling
- Document upload UI
- Feature parity with Gradio MVP

**Deferred from original Sprint 13:** All frontend features moved to dedicated Sprint 14

---

## Sprint Retrospective Template

At end of Sprint 13, evaluate:

1. **Test Coverage Achievement:**
   - Did we reach 70%+ pass rate?
   - Which test fixes were harder than expected?

2. **Performance Improvements:**
   - What speedup did caching provide?
   - Is batching effective for LLM labeling?

3. **CI/CD Enhancements:**
   - Is parallel execution stable?
   - Are test timeouts properly configured?

4. **Technical Debt:**
   - How many TD items resolved? (Target: 4)
   - Any new TD introduced?

5. **Lessons for Sprint 14:**
   - What frontend preparation is needed?
   - Team readiness for React migration?

---

**Sprint 13 Start Date:** TBD
**Sprint 13 End Date:** TBD (1-2 weeks from start)
**Previous Sprint:** Sprint 12 (COMPLETE - 31/32 SP, 97%)
**Next Sprint:** Sprint 14 (React Frontend Migration Phase 1)

ðŸ¤– Generated with [Claude Code](https://claude.com/claude-code)

Co-Authored-By: Claude <noreply@anthropic.com>
