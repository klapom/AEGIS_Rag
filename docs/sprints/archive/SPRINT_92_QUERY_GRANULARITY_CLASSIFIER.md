# Query Granularity Classifier for Recursive LLM

**Status:** üìù Design Proposal
**Created:** 2026-01-14
**Context:** User question on intent routing for BGE-M3 vs LLM scoring

---

## Problem Statement

**Current Gap:**

Sprint 91 hat einen Intent Router (C-LARA SetFit) implementiert, aber der klassifiziert auf **HIGH-LEVEL**:
- FACTUAL, PROCEDURAL, COMPARISON, RECOMMENDATION, NAVIGATION (C-LARA)
- ‚Üí VECTOR, GRAPH, HYBRID, MEMORY, RESEARCH (Skill Routing)

**Was fehlt:** Eine **LOW-LEVEL** Klassifikation f√ºr die Wahl der Scoring-Methode im Recursive LLM:

| Query Type | Beispiel | Beste Methode | Warum |
|------------|----------|---------------|-------|
| **Fine-grained** | "What is the p-value for BGE-M3?" | BGE-M3 Multi-Vector | Token-level keyword matching |
| **Holistic** | "Summarize the main argument" | LLM Scoring | Versteht Argumentation & Kontext |
| **Reasoning** | "Why did the authors choose this approach?" | LLM Scoring | Braucht logisches Reasoning |

**Problem:** Ohne Query Granularity Detection nutzen wir immer die gleiche Methode f√ºr alle Queries ‚Üí suboptimal!

---

## Option 1: Heuristik-basierte Detection (Einfach, schnell) ‚úÖ EMPFOHLEN

### Heuristik-Regeln

**Fine-grained Indicators (‚Üí Multi-Vector):**
```python
fine_grained_patterns = [
    # Specific value queries
    r"\b(what|which) (is|are) the\b",
    r"\b(p-value|score|metric|number|count|percentage)\b",
    r"\bTable \d+\b",
    r"\bFigure \d+\b",
    r"\bEquation \d+\b",
    r"\b(formula|equation|definition) (for|of)\b",
    r"\b(exact|specific|precise) (value|number)\b",

    # Named entities
    r"\b[A-Z]{2,}-[A-Z]\d+\b",  # BGE-M3, GPT-4, etc.
    r"\b\d+(\.\d+)?%\b",  # Percentages
    r"\b\d{4}\b",  # Years

    # Structured content
    r"\bin (Table|Figure|Section|Chapter|Appendix)\b",
    r"\b(row|column|entry) \d+\b",
]

holistic_patterns = [
    # Summary/overview queries
    r"\b(summarize|overview|explain|describe)\b",
    r"\b(main|key|primary) (idea|argument|point|finding)\b",
    r"\b(how does|how do|how can)\b",

    # Reasoning queries
    r"\b(why|reason|rationale|motivation)\b",
    r"\b(advantage|benefit|drawback|limitation)\b",
    r"\b(compare|contrast|difference)\b",
    r"\b(implication|consequence|impact)\b",

    # High-level understanding
    r"\b(methodology|approach|strategy)\b",
    r"\b(overall|general|broad)\b",
]
```

### Implementation

```python
class QueryGranularityClassifier:
    """Classify query granularity for optimal scoring method selection.

    Sprint 92 Enhancement: Chooses between BGE-M3 Multi-Vector (fine-grained)
    and LLM Scoring (holistic/reasoning) for Level 2+ recursive processing.
    """

    def __init__(self):
        self.fine_grained_patterns = [
            re.compile(pattern, re.IGNORECASE)
            for pattern in fine_grained_patterns
        ]
        self.holistic_patterns = [
            re.compile(pattern, re.IGNORECASE)
            for pattern in holistic_patterns
        ]

    def classify(self, query: str) -> Literal["fine-grained", "holistic"]:
        """Classify query granularity using pattern matching.

        Args:
            query: User query string

        Returns:
            "fine-grained" ‚Üí Use BGE-M3 multi-vector (token-level)
            "holistic" ‚Üí Use LLM scoring (document-level reasoning)

        Example:
            >>> classifier = QueryGranularityClassifier()
            >>> classifier.classify("What is the p-value for BGE-M3?")
            'fine-grained'
            >>> classifier.classify("Summarize the main argument")
            'holistic'
        """
        fine_grained_score = sum(
            1 for pattern in self.fine_grained_patterns
            if pattern.search(query)
        )

        holistic_score = sum(
            1 for pattern in self.holistic_patterns
            if pattern.search(query)
        )

        logger.debug(
            "query_granularity_classification",
            query=query[:100],
            fine_grained_score=fine_grained_score,
            holistic_score=holistic_score,
        )

        # Tie-breaker: Default to fine-grained (faster & cheaper)
        if fine_grained_score >= holistic_score:
            return "fine-grained"
        else:
            return "holistic"
```

### Integration in Recursive LLM

```python
# src/agents/context/recursive_llm.py

class RecursiveLLMProcessor:
    def __init__(
        self,
        llm: BaseChatModel,
        skill_registry: SkillRegistry,
        settings: Optional[RecursiveLLMSettings] = None,
    ):
        # ... existing code ...
        self.granularity_classifier = QueryGranularityClassifier()

    async def _score_relevance(
        self,
        segments: list[DocumentSegment],
        query: str,
        skill: Optional[LoadedSkill],
        level: int,
    ) -> list[DocumentSegment]:
        """Score segments using query-adaptive method."""

        level_config = self.settings.levels[min(level, len(self.settings.levels) - 1)]

        # Check if level config forces a specific method
        if level_config.scoring_method == "llm":
            return await self._score_relevance_llm(segments, query, skill)
        elif level_config.scoring_method == "dense+sparse":
            return await self._score_relevance_dense_sparse(segments, query)
        elif level_config.scoring_method == "multi-vector":
            return await self._score_relevance_multi_vector(segments, query)
        elif level_config.scoring_method == "adaptive":
            # NEW: Query-adaptive selection
            granularity = self.granularity_classifier.classify(query)

            if granularity == "fine-grained":
                logger.info(
                    "query_adaptive_scoring",
                    level=level,
                    method="multi-vector",
                    reason="fine-grained query detected"
                )
                return await self._score_relevance_multi_vector(segments, query)
            else:
                logger.info(
                    "query_adaptive_scoring",
                    level=level,
                    method="llm",
                    reason="holistic/reasoning query detected"
                )
                return await self._score_relevance_llm(segments, query, skill)
```

### Configuration

```python
class RecursiveLevelConfig(BaseModel):
    """Configuration for a specific recursion level."""

    scoring_method: Literal[
        "dense+sparse",    # Fast document-level (BGE-M3)
        "multi-vector",    # Token-level precision (BGE-M3 ColBERT)
        "llm",             # Deep reasoning (LLM calls)
        "adaptive",        # Query-adaptive (NEW!)
    ] = "adaptive"
```

**Environment Configuration:**

```bash
# Level 0-1: Fast dense+sparse (no adaptation needed)
RECURSIVE_LLM_LEVEL_0_SCORING_METHOD=dense+sparse
RECURSIVE_LLM_LEVEL_1_SCORING_METHOD=dense+sparse

# Level 2+: Adaptive (heuristic chooses multi-vector vs LLM)
RECURSIVE_LLM_LEVEL_2_SCORING_METHOD=adaptive
RECURSIVE_LLM_LEVEL_3_SCORING_METHOD=adaptive
```

### Performance

| Query Type | Method Chosen | Latency | Accuracy |
|------------|---------------|---------|----------|
| "What is the p-value?" | Multi-Vector | 200ms | 95% |
| "Summarize the main argument" | LLM | 500ms-1s | 85% |
| "Why did authors choose X?" | LLM | 500ms-1s | 90% |

**Benefit:** Best of both worlds without manual user selection!

---

## Option 2: C-LARA Extension (ML-based, genauer) üî¨ LATER

### Extend C-LARA with Granularity Dimension

**Current C-LARA (5 classes):**
- FACTUAL, PROCEDURAL, COMPARISON, RECOMMENDATION, NAVIGATION

**Extended C-LARA (5 √ó 2 = 10 classes):**
- FACTUAL_FINE (e.g., "What is the p-value?")
- FACTUAL_HOLISTIC (e.g., "Summarize the key findings")
- PROCEDURAL_FINE (e.g., "What is step 3 in Algorithm 1?")
- PROCEDURAL_HOLISTIC (e.g., "How does the algorithm work?")
- ...

### Training Data Requirements

```python
training_examples = [
    # Fine-grained FACTUAL
    ("What is the accuracy of BGE-M3 on BEIR?", "factual_fine"),
    ("What is the p-value in Table 3?", "factual_fine"),
    ("How many parameters does GPT-4 have?", "factual_fine"),

    # Holistic FACTUAL
    ("Summarize the experimental results", "factual_holistic"),
    ("What are the main findings?", "factual_holistic"),

    # Fine-grained PROCEDURAL
    ("What is the formula for RRF?", "procedural_fine"),
    ("What is step 2 in the training process?", "procedural_fine"),

    # Holistic PROCEDURAL
    ("How does the training process work?", "procedural_holistic"),
    ("Explain the methodology", "procedural_holistic"),

    # Reasoning (always holistic)
    ("Why did the authors choose BGE-M3?", "factual_holistic"),
    ("What are the advantages of late interaction?", "comparison_holistic"),
]
```

### Implementation

```python
# Extend existing C-LARA model
class ExtendedCLARAIntent(str, Enum):
    """Extended C-LARA with granularity dimension."""
    FACTUAL_FINE = "factual_fine"
    FACTUAL_HOLISTIC = "factual_holistic"
    PROCEDURAL_FINE = "procedural_fine"
    PROCEDURAL_HOLISTIC = "procedural_holistic"
    # ... (10 classes total)

# Mapping to scoring method
GRANULARITY_MAP = {
    # Fine-grained ‚Üí Multi-Vector
    "factual_fine": "multi-vector",
    "procedural_fine": "multi-vector",
    "comparison_fine": "multi-vector",

    # Holistic ‚Üí LLM
    "factual_holistic": "llm",
    "procedural_holistic": "llm",
    "comparison_holistic": "llm",
    "recommendation_holistic": "llm",
    "navigation_holistic": "llm",
}
```

### Training Effort

- **Training Data:** ~500 examples (50 per class)
- **Training Time:** 1-2 hours (SetFit few-shot)
- **Accuracy Target:** 85-90%
- **Story Points:** 8 SP (data collection + training + validation)

**Trade-off:** H√∂here Accuracy aber mehr Aufwand. Erst sp√§ter wenn Heuristik nicht ausreicht.

---

## Option 3: Hybrid (Heuristik + ML Fallback) üéØ BEST LONG-TERM

### Architecture

```
Query ‚Üí Heuristic Confidence Check
         ‚îÇ
         ‚îú‚îÄ Confidence ‚â• 0.8 ‚Üí Use Heuristic Decision (fast path)
         ‚îÇ
         ‚îî‚îÄ Confidence < 0.8 ‚Üí Use ML Classifier (slow path)
```

### Implementation

```python
class HybridGranularityClassifier:
    """Hybrid classifier using heuristic + ML fallback."""

    def __init__(self):
        self.heuristic = QueryGranularityClassifier()
        self.ml_model = None  # Lazy load ExtendedCLARA if needed

    def classify(self, query: str) -> tuple[str, float]:
        """Classify with confidence score.

        Returns:
            (granularity, confidence)
        """
        # Try heuristic first
        fine_grained_score = sum(...)
        holistic_score = sum(...)

        # Compute confidence
        total_score = fine_grained_score + holistic_score

        if total_score == 0:
            confidence = 0.0  # No pattern matched
        else:
            max_score = max(fine_grained_score, holistic_score)
            confidence = max_score / total_score

        if confidence >= 0.8:
            # High confidence ‚Üí use heuristic
            decision = "fine-grained" if fine_grained_score >= holistic_score else "holistic"
            logger.debug(
                "heuristic_decision",
                decision=decision,
                confidence=confidence
            )
            return decision, confidence
        else:
            # Low confidence ‚Üí fallback to ML
            if self.ml_model is None:
                self._load_ml_model()

            ml_intent = self.ml_model.predict([query])[0]
            ml_decision = GRANULARITY_MAP[ml_intent]

            logger.info(
                "ml_fallback",
                decision=ml_decision,
                ml_intent=ml_intent,
                heuristic_confidence=confidence
            )
            return ml_decision, 0.95  # ML confidence

    def _load_ml_model(self):
        """Lazy load ML model (only if needed)."""
        from setfit import SetFitModel
        self.ml_model = SetFitModel.from_pretrained("models/clara-extended")
```

### Performance

| Scenario | Path | Latency | Accuracy |
|----------|------|---------|----------|
| Clear fine-grained query | Heuristic | ~0ms | 90% |
| Clear holistic query | Heuristic | ~0ms | 85% |
| Ambiguous query | ML Fallback | ~40ms | 90% |

**Benefit:** Fast path f√ºr 80% der Queries, ML nur f√ºr ambige F√§lle.

---

## Recommendation: Start with Heuristic, Add ML Later

### Phase 1: Heuristik (Sprint 92.5) ‚úÖ PRIORITY 1

**Rationale:**
- ‚úÖ 0ms latency overhead
- ‚úÖ 80-90% accuracy f√ºr klare Queries
- ‚úÖ 0 SP training effort
- ‚úÖ Sofort implementierbar
- ‚úÖ Einfach zu debuggen (Regex patterns)

**Story Points:** 2 SP
- 1 SP: Implement `QueryGranularityClassifier`
- 1 SP: Integration in `recursive_llm.py`

**Files:**
- `src/agents/context/query_granularity.py` (new, 150 LOC)
- `src/agents/context/recursive_llm.py` (update `_score_relevance`)

---

### Phase 2: ML Extension (Sprint 95+) üî¨ LATER

**When:** After collecting real-world query logs + accuracy analysis

**Trigger:**
- Heuristic accuracy drops below 75%
- >20% ambiguous queries (fine_grained_score ‚âà holistic_score)

**Story Points:** 8 SP
- 3 SP: Collect + annotate training data (500 examples)
- 3 SP: Train Extended C-LARA model
- 2 SP: Integrate hybrid classifier

---

## Integration with Sprint 91 Intent Router

### Two-Stage Classification

```
Query
  ‚Üì
[Stage 1: High-Level Intent Router (Sprint 91)]
  ‚îú‚îÄ C-LARA SetFit: FACTUAL/PROCEDURAL/COMPARISON/RECOMMENDATION/NAVIGATION
  ‚îî‚îÄ Skill Routing: VECTOR/GRAPH/HYBRID/MEMORY/RESEARCH
  ‚Üì
[Stage 2: Granularity Classifier (Sprint 92.5 - NEW!)]
  ‚îú‚îÄ Query Granularity: FINE-GRAINED / HOLISTIC
  ‚îî‚îÄ Scoring Method: Multi-Vector / LLM
  ‚Üì
Recursive LLM Processing
```

**Orthogonal Dimensions:**

| Dimension | Sprint | Purpose | Classes |
|-----------|--------|---------|---------|
| **Intent** | 91 | Skill Routing + Retrieval Strategy | FACTUAL, PROCEDURAL, etc. |
| **Granularity** | 92.5 | Scoring Method Selection | FINE-GRAINED, HOLISTIC |

**Example:**
- Query: "What is the p-value for BGE-M3 in Table 3?"
- **Stage 1 (Sprint 91):** Intent = FACTUAL ‚Üí Skills = [retrieval, reflection]
- **Stage 2 (Sprint 92.5):** Granularity = FINE-GRAINED ‚Üí Scoring = Multi-Vector
- Result: Fast token-level matching finds "p-value", "BGE-M3", "Table 3"

---

## Performance Impact

### Without Granularity Classifier (Always Multi-Vector)

| Query | Method | Latency | Accuracy |
|-------|--------|---------|----------|
| "What is the p-value?" | Multi-Vector | 200ms | ‚úÖ 95% |
| "Summarize the main argument" | Multi-Vector | 200ms | ‚ùå 60% (no reasoning!) |

**Problem:** Multi-Vector schlecht f√ºr holistische Queries (matcht nur Keywords, versteht Argumentation nicht)

---

### With Granularity Classifier (Adaptive)

| Query | Method Chosen | Latency | Accuracy |
|-------|---------------|---------|----------|
| "What is the p-value?" | Multi-Vector | 200ms | ‚úÖ 95% |
| "Summarize the main argument" | LLM | 500ms | ‚úÖ 85% |
| "Why did authors choose X?" | LLM | 500ms | ‚úÖ 90% |

**Benefit:** +25-30% accuracy f√ºr holistische Queries, ~3x latency aber nur wenn n√∂tig!

---

## Decision Matrix

| Scenario | Fine-grained % | Holistic % | Heuristic Sufficient? | ML Needed? |
|----------|----------------|------------|-----------------------|------------|
| Research Papers | 60% | 40% | ‚úÖ Yes (clear patterns) | ‚ö†Ô∏è Later |
| Technical Docs | 70% | 30% | ‚úÖ Yes | ‚ùå No |
| Conversational QA | 40% | 60% | ‚ö†Ô∏è Maybe | ‚úÖ Yes |
| Mixed Corpus | 50% | 50% | ‚ö†Ô∏è Maybe | ‚úÖ Yes |

**AegisRAG Use Case:** Research Papers + Technical Docs ‚Üí **Heuristic ausreichend**

---

## Implementation Priority

### Phase 1: Heuristic Classifier (2 SP) ‚úÖ START HERE

**Files:**
- `src/agents/context/query_granularity.py` (new, 150 LOC)
- `src/agents/context/recursive_llm.py` (update)
- `tests/unit/agents/context/test_query_granularity.py` (new, 20 tests)

**Config:**
```python
levels = [
    {"level": 0, "scoring_method": "dense+sparse"},  # Fast
    {"level": 1, "scoring_method": "dense+sparse"},  # Fast
    {"level": 2, "scoring_method": "adaptive"},      # Heuristic chooses
]
```

---

### Phase 2: ML Classifier (8 SP) üî¨ LATER (Sprint 95+)

**Trigger:** After 1-2 months of usage + query log analysis

**Requirements:**
- 500+ annotated queries
- Heuristic accuracy analysis
- Identified ambiguous query patterns

---

## Summary

‚úÖ **JA, wir brauchen einen Query Granularity Classifier!**

**Warum:**
- BGE-M3 Multi-Vector ist super f√ºr fine-grained Queries (95% accuracy)
- LLM Scoring ist besser f√ºr holistische/reasoning Queries (85-90% accuracy)
- Ohne Classifier nutzen wir immer die gleiche Methode ‚Üí suboptimal

**Empfehlung:**
1. ‚úÖ **Start:** Heuristik-basiert (2 SP, 0ms latency, 80-90% accuracy)
2. üî¨ **Later:** ML-basiert wenn Heuristik nicht ausreicht (8 SP, Sprint 95+)
3. üéØ **Long-term:** Hybrid (Heuristik + ML Fallback)

**Integration:** Orthogonal zu Sprint 91 Intent Router:
- Sprint 91: High-level intent (FACTUAL/PROCEDURAL/...) ‚Üí Skill Routing
- Sprint 92.5: Query granularity (FINE-GRAINED/HOLISTIC) ‚Üí Scoring Method

**Impact:**
- +25-30% accuracy f√ºr holistische Queries
- 0ms latency overhead (Heuristik)
- ~3x latency nur f√ºr holistische Queries (aber bessere Accuracy)
