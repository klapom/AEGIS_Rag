# Sprint 25: Production Readiness & Architecture Consolidation

**Status:** üìã PLANNED ‚Üí üöß IN PROGRESS
**Goal:** Production monitoring, integration tests, deferred Sprint 24 features, Sprint 22 refactoring cleanup, AND LLM architecture consolidation
**Duration:** 8 days (2025-11-15 to 2025-11-24)
**Prerequisites:** Sprint 24 complete (Dependency Optimization & CI Performance)
**Story Points:** 45 SP (22 SP deferred from Sprint 24 + 10 SP refactoring + 8 SP new work + 5 SP LLM migration)

---

## üéØ Sprint Objectives

### **Primary Goals:**
1. Implement production monitoring (Prometheus metrics, Grafana dashboards)
2. Complete integration tests for LangGraph ingestion pipeline
3. Improve token tracking accuracy (no 50/50 estimation)
4. Refactor async/sync bridge for cleaner architecture
5. Complete MyPy strict mode and documentation updates

### **Success Criteria:**
- ‚úÖ Prometheus /metrics endpoint operational
- ‚úÖ Grafana dashboard displaying LLM cost, latency, and token metrics
- ‚úÖ Integration tests for LangGraph pipeline (>80% coverage)
- ‚úÖ Token tracking accuracy improved (accurate input/output split)
- ‚úÖ Async/sync bridge refactored (no ThreadPoolExecutor)
- ‚úÖ MyPy strict mode passing
- ‚úÖ Architecture documentation updated

---

## üì¶ Sprint Features

### Category 1: Production Monitoring (High Priority)

#### Feature 25.1: Prometheus Metrics Implementation (5 SP) ‚≠ê
**Priority:** P1 (High) - **Production Blocker**
**Duration:** 1 day
**Source:** Deferred from Sprint 24 (Feature 24.1)

**Problem:**
Cost tracking logs structured metrics but doesn't export to Prometheus. No /metrics endpoint for monitoring dashboards.

**Solution:**
Implement `src/core/metrics.py` with prometheus_client and export metrics endpoint.

**Deliverables:**
- [ ] `src/core/metrics.py` with LLM-specific metrics
- [ ] FastAPI /metrics endpoint registration
- [ ] Grafana dashboard JSON (LLM cost, latency, requests)
- [ ] Documentation in `docs/guides/MONITORING.md`
- [ ] Integration with existing Prometheus client

**Metrics to Implement:**
```python
from prometheus_client import Counter, Gauge, Histogram

# LLM Metrics
llm_requests_total = Counter(
    "llm_requests_total",
    "Total LLM requests",
    ["provider", "model", "task_type"]
)

llm_latency_seconds = Histogram(
    "llm_latency_seconds",
    "LLM request latency",
    ["provider", "model"],
    buckets=[0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 30.0]
)

llm_cost_usd = Counter(
    "llm_cost_usd",
    "Total LLM cost in USD",
    ["provider", "model"]
)

llm_tokens_used = Counter(
    "llm_tokens_used",
    "Total tokens used",
    ["provider", "model", "token_type"]  # token_type: input, output
)

llm_errors_total = Counter(
    "llm_errors_total",
    "Total LLM errors",
    ["provider", "error_type"]
)

# Budget Metrics
monthly_budget_remaining = Gauge(
    "monthly_budget_remaining_usd",
    "Monthly budget remaining",
    ["provider"]
)

monthly_spend = Gauge(
    "monthly_spend_usd",
    "Monthly spend so far",
    ["provider"]
)

# System Metrics
qdrant_points_count = Gauge(
    "qdrant_points_count",
    "Total points in Qdrant",
    ["collection"]
)

neo4j_entities_count = Gauge(
    "neo4j_entities_count",
    "Total entities in Neo4j"
)

neo4j_relations_count = Gauge(
    "neo4j_relations_count",
    "Total relationships in Neo4j"
)
```

**Integration Points:**
```python
# src/components/llm_proxy/aegis_llm_proxy.py
from src.core.metrics import (
    llm_requests_total,
    llm_latency_seconds,
    llm_cost_usd,
    llm_tokens_used,
)

async def generate(self, task: LLMTask) -> LLMResponse:
    start_time = time.time()

    try:
        result = await self._execute_with_any_llm(...)

        # Record metrics
        latency = time.time() - start_time
        llm_requests_total.labels(
            provider=provider,
            model=model,
            task_type=task.task_type
        ).inc()

        llm_latency_seconds.labels(
            provider=provider,
            model=model
        ).observe(latency)

        llm_cost_usd.labels(
            provider=provider,
            model=model
        ).inc(result.cost_usd)

        llm_tokens_used.labels(
            provider=provider,
            model=model,
            token_type="input"
        ).inc(result.tokens_input)

        llm_tokens_used.labels(
            provider=provider,
            model=model,
            token_type="output"
        ).inc(result.tokens_output)

        return result

    except Exception as e:
        llm_errors_total.labels(
            provider=provider,
            error_type=type(e).__name__
        ).inc()
        raise
```

**Grafana Dashboard:**
```json
{
  "dashboard": {
    "title": "AEGIS RAG - LLM Monitoring",
    "panels": [
      {
        "title": "LLM Request Rate",
        "targets": [{"expr": "rate(llm_requests_total[5m])"}]
      },
      {
        "title": "LLM Latency (p95)",
        "targets": [{"expr": "histogram_quantile(0.95, llm_latency_seconds)"}]
      },
      {
        "title": "LLM Cost (Hourly)",
        "targets": [{"expr": "rate(llm_cost_usd[1h])"}]
      },
      {
        "title": "Token Usage",
        "targets": [{"expr": "rate(llm_tokens_used[5m])"}]
      },
      {
        "title": "Budget Remaining",
        "targets": [{"expr": "monthly_budget_remaining_usd"}]
      }
    ]
  }
}
```

**Acceptance Criteria:**
- ‚úÖ /metrics endpoint returns Prometheus-formatted metrics
- ‚úÖ Metrics update in real-time during LLM requests
- ‚úÖ Grafana dashboard displays cost and latency trends
- ‚úÖ Budget alerts trigger when >80% spent
- ‚úÖ Integration tests validate metric collection
- ‚úÖ Documentation in MONITORING.md

**Files to Create/Modify:**
- `src/core/metrics.py` (NEW)
- `src/api/main.py` (add /metrics endpoint)
- `src/components/llm_proxy/aegis_llm_proxy.py` (add metric recording)
- `src/components/llm_proxy/cost_tracker.py` (add metric export)
- `dashboards/grafana/llm_monitoring.json` (NEW)
- `docs/guides/MONITORING.md` (NEW)
- `tests/integration/test_prometheus_metrics.py` (NEW)

---

### Category 2: Testing & Quality Assurance

#### Feature 25.2: LangGraph Integration Tests (5 SP) ‚≠ê
**Priority:** P1 (High) - **Test Coverage Gap**
**Duration:** 1 day
**Source:** Deferred from Sprint 24 (Feature 24.7)

**Problem:**
LangGraph ingestion pipeline (Sprint 21-23) missing comprehensive integration tests. Only unit tests exist for individual nodes.

**Solution:**
Create integration tests for full LangGraph ingestion pipeline (6 nodes).

**Deliverables:**
- [ ] Integration tests for LangGraph nodes (6 tests)
- [ ] E2E test for full ingestion pipeline
- [ ] Test fixtures for sample documents
- [ ] Test coverage report (target: >80%)
- [ ] CI pipeline validation

**Test Coverage:**
```python
# tests/integration/components/ingestion/test_langgraph_pipeline.py

import pytest
from pathlib import Path
from src.components.ingestion.langgraph_pipeline import IngestionPipeline
from src.components.ingestion.ingestion_state import IngestionState

@pytest.fixture
def sample_pdf():
    """Fixture for sample PDF document."""
    return Path("tests/fixtures/sample_document.pdf")

@pytest.fixture
def ingestion_pipeline():
    """Fixture for ingestion pipeline."""
    return IngestionPipeline()

# Node-level integration tests
async def test_memory_check_node_success():
    """Test memory check node with sufficient memory."""
    state = create_initial_state(document_path="test.pdf")
    result = await memory_check_node(state)

    assert result["memory_check_passed"] is True
    assert result["current_memory_mb"] > 0
    assert result["current_vram_mb"] >= 0

async def test_docling_extraction_node(sample_pdf):
    """Test Docling container integration."""
    state = create_initial_state(document_path=sample_pdf)
    state["memory_check_passed"] = True

    result = await docling_extraction_node(state)

    assert result["docling_status"] == "completed"
    assert len(result["parsed_content"]) > 0
    assert len(result["page_dimensions"]) > 0

async def test_image_enrichment_node(sample_pdf):
    """Test VLM image enrichment."""
    # Setup: Run Docling first
    state = await docling_extraction_node(...)

    result = await image_enrichment_node(state)

    assert result["enrichment_status"] == "completed"
    assert len(result["vlm_metadata"]) >= 0

async def test_chunking_node():
    """Test chunking with HybridChunker."""
    state = ...  # Setup with parsed document

    result = await chunking_node(state)

    assert result["chunking_status"] == "completed"
    assert len(result["chunks"]) > 0
    assert all("chunk" in c and "image_bboxes" in c for c in result["chunks"])

async def test_embedding_node():
    """Test Qdrant embedding and upsert."""
    state = ...  # Setup with chunks

    result = await embedding_node(state)

    assert result["embedding_status"] == "completed"
    assert len(result["embedded_chunk_ids"]) == len(state["chunks"])

async def test_graph_extraction_node():
    """Test LightRAG entity/relation extraction."""
    state = ...  # Setup with chunks

    result = await graph_extraction_node(state)

    assert result["graph_status"] == "completed"
    assert result["entities"] is not None
    assert result["relations"] is not None

# End-to-end pipeline test
@pytest.mark.slow
async def test_end_to_end_pipeline(sample_pdf, ingestion_pipeline):
    """Test complete pipeline: PDF ‚Üí Qdrant + Neo4j."""
    # Run full pipeline
    result = await ingestion_pipeline.run(sample_pdf)

    # Validate all stages completed
    assert result["memory_check_passed"] is True
    assert result["docling_status"] == "completed"
    assert result["chunking_status"] == "completed"
    assert result["embedding_status"] == "completed"
    assert result["graph_status"] == "completed"

    # Validate data in Qdrant
    qdrant = get_qdrant_client()
    points = await qdrant.scroll(
        collection_name="documents_v1",
        limit=10
    )
    assert len(points[0]) > 0

    # Validate data in Neo4j
    lightrag = await get_lightrag_wrapper_async()
    stats = await lightrag.get_stats()
    assert stats["entity_count"] > 0
    assert stats["relationship_count"] > 0

# Error handling tests
async def test_pipeline_graceful_degradation():
    """Test pipeline continues when optional steps fail."""
    # Setup: Mock VLM to fail
    with patch('src.components.ingestion.image_processor.ImageProcessor.process_image', side_effect=Exception("VLM failed")):
        result = await ingestion_pipeline.run(sample_pdf)

    # Pipeline should complete despite VLM failure
    assert result["enrichment_status"] == "failed"
    assert result["embedding_status"] == "completed"  # Should still succeed

# Performance tests
@pytest.mark.slow
async def test_pipeline_performance(sample_pdf):
    """Test pipeline completes within acceptable time."""
    import time

    start = time.time()
    result = await ingestion_pipeline.run(sample_pdf)
    duration = time.time() - start

    # Should complete within 5 minutes for small document
    assert duration < 300
    assert result["overall_progress"] == 100.0
```

**Test Fixtures:**
```python
# tests/fixtures/sample_documents.py

@pytest.fixture
def sample_pdf():
    """2-page PDF with text, image, and table."""
    return Path("tests/fixtures/sample_document.pdf")

@pytest.fixture
def sample_pptx():
    """5-slide PowerPoint presentation."""
    return Path("tests/fixtures/sample_presentation.pptx")

@pytest.fixture
def sample_docx():
    """3-page Word document."""
    return Path("tests/fixtures/sample_document.docx")
```

**Acceptance Criteria:**
- ‚úÖ 6+ integration tests created for LangGraph nodes
- ‚úÖ 1 E2E test for full pipeline
- ‚úÖ All tests passing
- ‚úÖ Coverage >80% for ingestion module
- ‚úÖ CI pipeline includes new tests
- ‚úÖ Performance baseline established (<5 min for small docs)

**Files to Create:**
- `tests/integration/components/ingestion/test_langgraph_pipeline.py` (NEW)
- `tests/fixtures/sample_document.pdf` (NEW)
- `tests/fixtures/sample_presentation.pptx` (NEW)
- `tests/fixtures/conftest.py` (update with fixtures)

---

### Category 3: LLM Proxy Improvements

#### Feature 25.3: Token Tracking Accuracy Fix (3 SP)
**Priority:** P2 (Medium)
**Duration:** 0.5 day
**Source:** Deferred from Sprint 24 (Feature 24.2)

**Problem:**
Token split estimation uses 50/50 split for input/output tokens because ANY-LLM response doesn't provide detailed breakdown. Alibaba Cloud charges different rates for input vs output.

**Current Code:**
```python
# src/components/llm_proxy/aegis_llm_proxy.py:495-497
# Estimate 50/50 split for input/output tokens if not available
tokens_input = result.tokens_used // 2
tokens_output = result.tokens_used - tokens_input
```

**Solution:**
Extract detailed token usage from API responses. Alibaba Cloud returns `prompt_tokens` and `completion_tokens`.

**Implementation:**
```python
# Parse response.usage properly
if hasattr(result, 'usage') and result.usage:
    tokens_input = result.usage.get('prompt_tokens', 0)
    tokens_output = result.usage.get('completion_tokens', 0)
    total_tokens = tokens_input + tokens_output
else:
    # Fallback to total tokens with 50/50 estimate
    total_tokens = getattr(result, 'tokens_used', 0)
    if total_tokens > 0:
        tokens_input = total_tokens // 2
        tokens_output = total_tokens - tokens_input
    else:
        tokens_input = 0
        tokens_output = 0

logger.info(
    "token_usage_parsed",
    provider=provider,
    model=model,
    tokens_input=tokens_input,
    tokens_output=tokens_output,
    total_tokens=total_tokens,
    estimation_used=not (hasattr(result, 'usage') and result.usage)
)
```

**Deliverables:**
- [ ] Update `aegis_llm_proxy.py` to parse `usage` field
- [ ] Add unit tests for token parsing (with/without usage field)
- [ ] Update cost tracker with accurate input/output split
- [ ] Verify with DashScope API response
- [ ] Add logging for estimation fallback

**Acceptance Criteria:**
- ‚úÖ Token input/output split accurate when `usage` field available
- ‚úÖ Fallback to 50/50 estimation when `usage` missing (with warning log)
- ‚úÖ Cost calculations use correct rates (input vs output)
- ‚úÖ Unit tests cover edge cases (missing usage field, zero tokens)
- ‚úÖ SQLite database shows accurate token breakdown
- ‚úÖ Prometheus metrics show input/output token split

**Files to Modify:**
- `src/components/llm_proxy/aegis_llm_proxy.py`
- `src/components/llm_proxy/cost_tracker.py`
- `tests/unit/components/llm_proxy/test_aegis_llm_proxy.py`

---

#### Feature 25.4: Async/Sync Bridge Refactoring (5 SP)
**Priority:** P2 (Medium)
**Duration:** 1 day
**Source:** Deferred from Sprint 24 (Feature 24.3)

**Problem:**
`ImageProcessor.process_image()` is synchronous but calls async `generate_vlm_description_with_dashscope()`. We use `asyncio.run()` with ThreadPoolExecutor to bridge sync/async, adding complexity.

**Current Workaround:**
```python
# src/components/ingestion/image_processor.py:414-434
try:
    asyncio.get_running_loop()
    # Already in event loop - use ThreadPoolExecutor
    with concurrent.futures.ThreadPoolExecutor() as executor:
        future = executor.submit(
            asyncio.run,
            generate_vlm_description_with_dashscope(...)
        )
        description = future.result()
except RuntimeError:
    # Not in event loop - use asyncio.run
    description = asyncio.run(generate_vlm_description_with_dashscope(...))
```

**Solution:**
Make entire ingestion pipeline async. Refactor `ImageProcessor.process_image()` to be async.

**Implementation:**
```python
# src/components/ingestion/image_processor.py

class ImageProcessor:
    async def process_image(
        self,
        image: PILImage,
        picture_index: int,
        use_proxy: bool = True,
    ) -> str | None:
        """Process image and generate VLM description (ASYNC)."""
        # ... existing filter logic ...

        # Direct async call (no ThreadPoolExecutor needed!)
        if use_proxy and DASHSCOPE_VLM_AVAILABLE:
            description = await generate_vlm_description_with_dashscope(
                image_path=temp_path,
                vl_high_resolution_images=False,
            )
        else:
            # Fallback to local Ollama VLM (also async)
            description = await generate_vlm_description_with_ollama(
                image_path=temp_path
            )

        return description
```

**Callers Update:**
```python
# src/components/ingestion/langgraph_nodes.py:576

async def image_enrichment_node(state: IngestionState) -> IngestionState:
    processor = ImageProcessor()

    for idx, picture_item in enumerate(doc.pictures):
        pil_image = picture_item.get_image()

        # Now properly async!
        description = await processor.process_image(
            image=pil_image,
            picture_index=idx,
        )
```

**Deliverables:**
- [ ] Refactor `ImageProcessor.process_image()` to async
- [ ] Update all callers to use `await`
- [ ] Remove ThreadPoolExecutor complexity
- [ ] Verify LangGraph pipeline compatibility
- [ ] Update tests to use async fixtures

**Acceptance Criteria:**
- ‚úÖ `ImageProcessor.process_image()` is async
- ‚úÖ No ThreadPoolExecutor usage
- ‚úÖ All callers updated (langgraph_nodes.py)
- ‚úÖ Tests passing with async fixtures
- ‚úÖ Performance unchanged or improved
- ‚úÖ Code complexity reduced (simpler logic)

**Files to Modify:**
- `src/components/ingestion/image_processor.py`
- `src/components/ingestion/langgraph_nodes.py`
- `tests/unit/components/ingestion/test_image_processor.py`
- `tests/integration/components/ingestion/test_langgraph_pipeline.py`

---

### Category 4: Code Quality & Documentation

#### Feature 25.5: MyPy Strict Mode (2 SP)
**Priority:** P3 (Low)
**Duration:** 0.5 day
**Source:** Deferred from Sprint 24 (Feature 24.4 partial)

**Problem:**
MyPy type checking not enforced in CI. Potential type safety issues undetected.

**Solution:**
Enable MyPy strict mode and fix all type errors.

**Implementation:**
```bash
# Run MyPy on src/
mypy src/ --strict

# Fix errors iteratively
# Common fixes:
# 1. Add return type hints: def foo() -> int:
# 2. Add parameter type hints: def bar(x: str, y: int) -> None:
# 3. Fix Optional types: x: str | None
# 4. Add generic types: List[str], Dict[str, int]
```

**Deliverables:**
- [ ] Run MyPy strict mode on `src/`
- [ ] Fix all type errors
- [ ] Add MyPy to CI pipeline (code-quality job)
- [ ] Update `pyproject.toml` with strict MyPy config
- [ ] Document type hints best practices

**Acceptance Criteria:**
- ‚úÖ MyPy strict mode passing (0 errors)
- ‚úÖ CI includes MyPy check
- ‚úÖ All public functions have type hints
- ‚úÖ Generic types used correctly
- ‚úÖ No `# type: ignore` comments (fix issues properly)

**Files to Modify:**
- `.github/workflows/ci.yml` (add MyPy step)
- `pyproject.toml` (enable strict mode)
- Various `src/` files (type hint fixes)

---

#### Feature 25.6: Architecture Documentation Update (2 SP)
**Priority:** P3 (Low)
**Duration:** 0.5 day
**Source:** Deferred from Sprint 24 (Feature 24.8 partial)

**Problem:**
`docs/architecture/CURRENT_ARCHITECTURE.md` and `docs/guides/PRODUCTION_DEPLOYMENT.md` not updated with Sprint 21-24 changes.

**Solution:**
Comprehensive documentation update reflecting current architecture.

**Deliverables:**
- [ ] Update `docs/architecture/CURRENT_ARCHITECTURE.md` with Sprint 21-24 changes
- [ ] Update `docs/guides/PRODUCTION_DEPLOYMENT.md` with new deployment steps
- [ ] Add Prometheus monitoring section to deployment guide
- [ ] Update dependency installation instructions
- [ ] Document lazy import pattern for optional dependencies

**CURRENT_ARCHITECTURE.md Updates:**
```markdown
# Current Architecture (Post-Sprint 24)

## System Overview
- Docling CUDA Container for ingestion (Sprint 21, ADR-027)
- LlamaIndex deprecated to optional dependency (Sprint 21, ADR-028)
- Multi-cloud LLM routing with ANY-LLM (Sprint 23, ADR-033)
- Lazy imports for optional dependencies (Sprint 24)
- Poetry dependency groups for modular installation (Sprint 24)

## Dependency Architecture
### Core Dependencies (~600MB)
- FastAPI, Qdrant, Neo4j, Redis
- Ollama client (local LLM)
- ANY-LLM SDK (multi-cloud routing)
- BGE-M3 embeddings

### Optional Dependencies (5 groups)
1. **ingestion** (~500MB): llama-index, spacy, docling
2. **reranking** (~400MB): sentence-transformers
3. **evaluation** (~600MB): ragas, datasets
4. **graph-analysis** (~150MB): graspologic
5. **ui** (~200MB): gradio

## Installation Patterns
```bash
# Minimal (core only)
poetry install

# With ingestion
poetry install --with ingestion

# Full development
poetry install --with dev,ingestion,reranking

# Production (all features)
poetry install --all-extras
```

## LLM Routing (Sprint 23)
- Tier 1 (Local): Ollama (70% of tasks, free)
- Tier 2 (Cloud): Alibaba Cloud DashScope (high quality, $10/month)
- Tier 3 (Premium): OpenAI GPT-4 (critical tasks, $20/month)

## Monitoring (Sprint 25)
- Prometheus /metrics endpoint
- Grafana dashboards for LLM cost, latency, tokens
- Budget tracking and alerts
```

**PRODUCTION_DEPLOYMENT.md Updates:**
```markdown
# Production Deployment Guide

## Prerequisites
- Docker + NVIDIA Container Toolkit (for Docling CUDA)
- Kubernetes cluster (optional, for scale)
- Prometheus + Grafana (for monitoring)

## Installation Steps
1. Clone repository
2. Install dependencies: `poetry install --all-extras`
3. Configure .env (API keys, database URLs)
4. Start services: `docker-compose up -d`
5. Run migrations
6. Verify /metrics endpoint: `curl http://localhost:8000/metrics`

## Monitoring Setup
1. Configure Prometheus to scrape /metrics
2. Import Grafana dashboard: `dashboards/grafana/llm_monitoring.json`
3. Set up budget alerts (>80% monthly spend)

## Performance Tuning
- Poetry cache for faster CI (85% speedup)
- Lazy imports for optional dependencies
- Minimal dependency installation for specific workloads
```

**Acceptance Criteria:**
- ‚úÖ CURRENT_ARCHITECTURE.md reflects Sprint 21-24 changes
- ‚úÖ PRODUCTION_DEPLOYMENT.md includes monitoring setup
- ‚úÖ Dependency installation patterns documented
- ‚úÖ Lazy import pattern explained
- ‚úÖ All docs reviewed and accurate

**Files to Create/Modify:**
- `docs/architecture/CURRENT_ARCHITECTURE.md`
- `docs/guides/PRODUCTION_DEPLOYMENT.md`
- `docs/guides/MONITORING.md` (NEW)
- `docs/guides/DEPENDENCY_MANAGEMENT.md` (NEW)

---

### Category 5: Refactoring Cleanup (Sprint 22 Debt)

#### Feature 25.7: Remove Deprecated Code (5 SP) ‚≠ê
**Priority:** P1 (High) - **ADR Compliance**
**Duration:** 2.5 days
**Source:** REFACTORING_ROADMAP.md (TD-REF-01, TD-REF-02, TD-REF-03)

**Problem:**
Three deprecated files violate architecture decisions (ADR-026, ADR-027, ADR-028) but still exist in codebase.

**Files to Remove:**

**1. unified_ingestion.py (TD-REF-01)**
- **File:** `src/components/shared/unified_ingestion.py` (275 lines)
- **Deprecated:** Sprint 21 (ADR-027)
- **Replacement:** `src/components/ingestion/langgraph_pipeline.py`
- **Reason:** Parallel execution (asyncio.gather) violates memory constraints
- **Impact:** Removes 275 lines, cleaner architecture

**2. three_phase_extractor.py (TD-REF-02)**
- **File:** `src/components/graph_rag/three_phase_extractor.py` (350 lines)
- **Deprecated:** Sprint 21 (ADR-026)
- **Replacement:** Pure LLM extraction (default in config)
- **Reason:** Three-phase (SpaCy NER + Dedup + Gemma) lower quality than pure LLM
- **Impact:** Removes 350 lines, simplifies extraction pipeline

**3. LlamaIndex load_documents() (TD-REF-03)**
- **Method:** `DocumentIngestionPipeline.load_documents()` (~100 lines)
- **Deprecated:** Sprint 21 (ADR-028)
- **Replacement:** Docling container client
- **Reason:** LlamaIndex lacks OCR, table extraction, GPU acceleration
- **Impact:** Removes ~100 lines, enforces ADR-028

**Implementation Plan:**

**Step 1: Archive Files**
```bash
# Create archive directory
mkdir -p archive/deprecated/sprint-21/

# Move files to archive
mv src/components/shared/unified_ingestion.py \
   archive/deprecated/sprint-21/unified_ingestion_deprecated.py

mv src/components/graph_rag/three_phase_extractor.py \
   archive/deprecated/sprint-21/three_phase_extractor_deprecated.py
```

**Step 2: Update Imports**
```python
# Before (unified_ingestion.py callers):
from src.components.shared.unified_ingestion import UnifiedIngestionPipeline
pipeline = UnifiedIngestionPipeline()

# After (LangGraph):
from src.components.ingestion.langgraph_pipeline import create_ingestion_graph
pipeline = create_ingestion_graph()
```

**Step 3: Remove load_documents() Method**
```python
# src/components/vector_search/ingestion.py
# DELETE lines 137-237 (load_documents method)
# Keep rest of file (chunk_documents, create_points, etc.)
```

**Step 4: Update Config**
```python
# src/core/config.py
# Before:
extraction_pipeline: Literal["three_phase", "llm_extraction"] = "llm_extraction"

# After:
extraction_pipeline: Literal["llm_extraction"] = "llm_extraction"
```

**Deliverables:**
- [ ] unified_ingestion.py archived
- [ ] three_phase_extractor.py archived
- [ ] load_documents() method removed
- [ ] All imports updated to new implementations
- [ ] Tests migrated
- [ ] Config updated (remove "three_phase" option)
- [ ] 725+ lines of deprecated code removed

**Acceptance Criteria:**
- ‚úÖ All deprecated files archived to `archive/deprecated/sprint-21/`
- ‚úÖ No imports reference deprecated code
- ‚úÖ Tests passing (100%)
- ‚úÖ ADR-026, ADR-027, ADR-028 compliance verified
- ‚úÖ 725+ lines removed from src/
- ‚úÖ Admin endpoint uses LangGraph pipeline
- ‚úÖ extraction_factory.py updated (no three_phase branch)

**Files to Modify:**
- `src/components/shared/unified_ingestion.py` (DELETE)
- `src/components/graph_rag/three_phase_extractor.py` (DELETE)
- `src/components/vector_search/ingestion.py` (remove load_documents)
- `src/api/v1/admin.py` (update to LangGraph)
- `src/components/graph_rag/extraction_factory.py` (remove three_phase)
- `src/core/config.py` (update Literal type)
- `tests/` (migrate tests to LangGraph)

---

#### Feature 25.8: Consolidate Duplicate Code (3 SP)
**Priority:** P2 (Medium)
**Duration:** 1.5 days
**Source:** REFACTORING_ROADMAP.md (TD-REF-04, TD-REF-05)

**Problem:**
Two critical code duplications exist (base agent, embedding services).

**Duplications to Fix:**

**1. Duplicate Base Agent (TD-REF-04)**
- **Files:** `src/agents/base.py` (155 lines) vs `src/agents/base_agent.py` (155 lines)
- **Status:** IDENTICAL classes (100% duplication)
- **Decision:** Keep `base_agent.py` (more explicit naming), delete `base.py`

**2. Duplicate Embedding Services (TD-REF-05)**
- **Files:**
  - `src/components/shared/embedding_service.py` (UnifiedEmbeddingService - 269 lines - core)
  - `src/components/vector_search/embeddings.py` (EmbeddingService - 160 lines - wrapper)
- **Status:** EmbeddingService is 100% wrapper (all methods delegate to UnifiedEmbeddingService)
- **Decision:** Remove wrapper, migrate all imports to UnifiedEmbeddingService

**Implementation:**

**Part 1: Remove base.py**
```python
# Update all imports:
# Before:
from src.agents.base import BaseAgent

# After:
from src.agents.base_agent import BaseAgent
```

**Part 2: Remove EmbeddingService Wrapper**
```python
# Update all imports:
# Before:
from src.components.vector_search.embeddings import EmbeddingService

# After:
from src.components.shared.embedding_service import UnifiedEmbeddingService

# Usage:
# Before:
embedder = EmbeddingService()

# After:
embedder = UnifiedEmbeddingService()
```

**Deliverables:**
- [ ] `src/agents/base.py` deleted
- [ ] All agent imports updated to `base_agent.py`
- [ ] `src/components/vector_search/embeddings.py` (EmbeddingService) removed
- [ ] All embedding imports updated to `UnifiedEmbeddingService`
- [ ] Tests migrated
- [ ] 315 lines of duplicate code removed (155 + 160)

**Acceptance Criteria:**
- ‚úÖ base.py deleted
- ‚úÖ EmbeddingService wrapper deleted
- ‚úÖ All imports updated
- ‚úÖ Tests passing (100%)
- ‚úÖ 315 lines of duplicate code removed
- ‚úÖ No functionality lost

**Files to Modify:**
- `src/agents/base.py` (DELETE)
- `src/components/vector_search/embeddings.py` (DELETE EmbeddingService class)
- All files importing `BaseAgent` from `base.py` (update imports)
- All files importing `EmbeddingService` (update to UnifiedEmbeddingService)
- Tests (update imports)

---

#### Feature 25.9: Standardize Client Naming (2 SP)
**Priority:** P2 (Medium)
**Duration:** 1 day
**Source:** REFACTORING_ROADMAP.md (TD-REF-06)

**Problem:**
Inconsistent naming for client wrapper classes (`Wrapper` suffix vs `Client` suffix).

**Current Naming Inconsistencies:**
- ‚ùå `QdrantClientWrapper` (wrapper suffix - redundant)
- ‚ùå `GraphitiWrapper` (wrapper suffix)
- ‚ùå `LightRAGWrapper` (wrapper suffix)
- ‚ùå `DoclingContainerClient` (container + client - verbose)
- ‚úÖ `Neo4jClient` (client suffix - correct)
- ‚úÖ `MCPClient` (client suffix - correct)

**Target Naming (Standardized):**
- ‚úÖ `QdrantClient` (rename from QdrantClientWrapper)
- ‚úÖ `GraphitiClient` (rename from GraphitiWrapper)
- ‚úÖ `LightRAGClient` (rename from LightRAGWrapper)
- ‚úÖ `DoclingClient` (rename from DoclingContainerClient)
- ‚úÖ `Neo4jClient` (already correct)
- ‚úÖ `MCPClient` (already correct)

**Implementation:**

**Step 1: Rename Classes**
```python
# src/components/vector_search/qdrant_client.py
# Before:
class QdrantClientWrapper:
    ...

# After:
class QdrantClient:
    ...

# Backward compatibility alias (deprecation period: Sprint 25-26)
QdrantClientWrapper = QdrantClient  # DEPRECATED: Use QdrantClient
```

**Step 2: Update Imports**
```python
# Before:
from src.components.vector_search.qdrant_client import QdrantClientWrapper

# After:
from src.components.vector_search.qdrant_client import QdrantClient
```

**Deliverables:**
- [ ] QdrantClientWrapper ‚Üí QdrantClient
- [ ] GraphitiWrapper ‚Üí GraphitiClient
- [ ] LightRAGWrapper ‚Üí LightRAGClient
- [ ] DoclingContainerClient ‚Üí DoclingClient
- [ ] Backward compatibility aliases created (deprecation warnings)
- [ ] All imports updated
- [ ] Tests updated
- [ ] Documentation updated

**Acceptance Criteria:**
- ‚úÖ All client classes renamed to `<Service>Client` pattern
- ‚úÖ Backward compatibility aliases in place
- ‚úÖ All imports updated
- ‚úÖ Tests passing (100%)
- ‚úÖ No breaking changes (aliases provide compatibility)
- ‚úÖ Documentation reflects new naming

**Files to Modify:**
- `src/components/vector_search/qdrant_client.py` (rename class)
- `src/components/memory/graphiti_wrapper.py` (rename class)
- `src/components/graph_rag/lightrag_wrapper.py` (rename class)
- `src/components/ingestion/docling_client.py` (rename class)
- All files importing these clients (update imports)
- Tests (update imports)
- Documentation (update references)

---

### Category 6: LLM Architecture Consolidation (Sprint 23 Debt)

#### Feature 25.10: Migrate All LLM Calls to AegisLLMProxy (5 SP) ‚≠ê‚≠ê‚≠ê
**Priority:** P1 (High) - **Architecture Consistency & Cost Tracking**
**Duration:** 2.5 days
**Source:** Comprehensive LLM Call Analysis (Sprint 25 Discovery)

**Problem:**
7 files make direct Ollama API calls, bypassing AegisLLMProxy routing layer. Estimated **$11,750/year** in untracked LLM costs.

**Current State:**
- **router.py:** Intent classification (EVERY query bypasses proxy!)
- **extraction_service.py:** Entity/relation extraction (EVERY document ingestion bypasses proxy!)
- **graphiti_wrapper.py:** Memory operations (EVERY memory op bypasses proxy!)
- **community_labeler.py:** Community labeling
- **dual_level_search.py:** Answer generation
- **custom_metrics.py:** Evaluation metrics
- **image_processor.py:** VLM fallback

**Impact:**
- ‚ùå **No cost tracking** (~$11,750/year hidden costs)
- ‚ùå **No Prometheus metrics** (request count, latency, tokens)
- ‚ùå **No multi-cloud routing** (stuck on local Ollama)
- ‚ùå **No budget limits** (unlimited spending possible)
- ‚ùå **Architecture inconsistency** (violates Sprint 23 design)

**Solution:**
Refactor all 7 files to use `AegisLLMProxy.generate()` instead of direct `ollama.AsyncClient` calls.

**Implementation Pattern:**

**Before (router.py example):**
```python
from ollama import AsyncClient

class IntentClassifier:
    def __init__(self, ...):
        self.client = AsyncClient(host=self.base_url)

    async def classify_intent(self, query: str) -> QueryIntent:
        response = await self.client.generate(
            model=self.model_name,
            prompt=prompt,
            options={"temperature": self.temperature, ...}
        )
        llm_response = response.get("response", "")
        intent = self._parse_intent(llm_response)
        return intent
```

**After (with AegisLLMProxy):**
```python
from src.components.llm_proxy.aegis_llm_proxy import (
    AegisLLMProxy, LLMTask, TaskType, QualityRequirement, Complexity
)

class IntentClassifier:
    def __init__(self, ...):
        self.llm_proxy = AegisLLMProxy()

    async def classify_intent(self, query: str) -> QueryIntent:
        task = LLMTask(
            task_type=TaskType.QUERY_CLASSIFICATION,
            prompt=CLASSIFICATION_PROMPT.format(query=query),
            quality_requirement=QualityRequirement.MEDIUM,
            complexity=Complexity.LOW,
            max_tokens=self.max_tokens,
            temperature=self.temperature,
        )

        result = await self.llm_proxy.generate(task)
        intent = self._parse_intent(result.content)
        return intent
```

**Files to Migrate (Priority Order):**

**Week 2 (Day 8): Feature 25.10 - LLM Migration** (5 SP)

**Morning (2 SP):**
1. **src/agents/router.py** (Intent Classification) - 0.5 day
   - Most critical: Every query passes through this
   - Task type: `TaskType.QUERY_CLASSIFICATION`
   - Quality: `QualityRequirement.MEDIUM`, Complexity: `Complexity.LOW`

2. **src/components/graph_rag/extraction_service.py** (Entity Extraction) - 0.5 day
   - Critical: Every document ingestion uses this
   - Task type: `TaskType.ENTITY_EXTRACTION`
   - Quality: `QualityRequirement.HIGH`, Complexity: `Complexity.MEDIUM`

**Afternoon (1.5 SP):**
3. **src/components/memory/graphiti_wrapper.py** (Memory Operations) - 0.5 day
   - Custom `OllamaLLMClient` class for Graphiti
   - Implements Graphiti's LLMClient interface
   - Task type: `TaskType.MEMORY_CONSOLIDATION`
   - Quality: `QualityRequirement.HIGH`, Complexity: `Complexity.MEDIUM`

4. **src/components/graph_rag/community_labeler.py** (Community Labeling) - 0.25 day
   - Task type: `TaskType.SUMMARIZATION`
   - Quality: `QualityRequirement.MEDIUM`, Complexity: `Complexity.LOW`

5. **src/components/graph_rag/dual_level_search.py** (Answer Generation) - 0.25 day
   - Task type: `TaskType.ANSWER_GENERATION`
   - Quality: `QualityRequirement.HIGH`, Complexity: `Complexity.MEDIUM`

**Evening (1.5 SP):**
6. **src/evaluation/custom_metrics.py** (Evaluation Metrics) - 0.25 day
   - Task type: `TaskType.EVALUATION`
   - Quality: `QualityRequirement.HIGH`, Complexity: `Complexity.LOW`

7. **src/components/ingestion/image_processor.py** (VLM Fallback) - 0.25 day
   - Lowest priority: Only fallback when DashScope unavailable
   - Task type: `TaskType.VLM_DESCRIPTION`
   - Note: Keep as fallback, DashScope VLM is primary

**Deliverables:**
- [ ] router.py migrated to AegisLLMProxy
- [ ] extraction_service.py migrated to AegisLLMProxy
- [ ] graphiti_wrapper.py migrated to AegisLLMProxy
- [ ] community_labeler.py migrated to AegisLLMProxy
- [ ] dual_level_search.py migrated to AegisLLMProxy
- [ ] custom_metrics.py migrated to AegisLLMProxy
- [ ] image_processor.py migrated to AegisLLMProxy (fallback)
- [ ] All tests updated
- [ ] Cost tracking validates all LLM calls
- [ ] Prometheus metrics show all request types

**Acceptance Criteria:**
- ‚úÖ All 7 files use AegisLLMProxy for text generation
- ‚úÖ Cost tracking includes all LLM requests (SQLite database)
- ‚úÖ Prometheus metrics show all request types (if Feature 25.1 done)
- ‚úÖ Multi-cloud routing available (Local ‚Üí Alibaba ‚Üí OpenAI)
- ‚úÖ Budget limits enforced per provider
- ‚úÖ Tests passing (100%)
- ‚úÖ No regressions in functionality
- ‚úÖ **$11,750/year cost visibility** achieved!

**Testing Strategy:**
```python
# tests/integration/components/llm_proxy/test_llm_migration.py

async def test_intent_classification_uses_proxy():
    """Verify intent classification uses AegisLLMProxy."""
    classifier = IntentClassifier()

    # Mock proxy to verify it's called
    with patch('src.components.llm_proxy.aegis_llm_proxy.AegisLLMProxy.generate') as mock_generate:
        mock_generate.return_value = LLMResult(content="VECTOR", ...)

        intent = await classifier.classify_intent("What is RAG?")

        assert mock_generate.called
        assert intent == QueryIntent.VECTOR

async def test_extraction_uses_proxy():
    """Verify entity extraction uses AegisLLMProxy."""
    service = ExtractionService()

    with patch('src.components.llm_proxy.aegis_llm_proxy.AegisLLMProxy.generate') as mock_generate:
        mock_generate.return_value = LLMResult(content='[{"name": "RAG", ...}]', ...)

        entities = await service.extract_entities("RAG is a technique...")

        assert mock_generate.called
        assert len(entities) > 0
```

**Cost Tracking Validation:**
```python
# After migration, verify all LLM calls tracked in SQLite
from src.components.llm_proxy.cost_tracker import CostTracker

tracker = CostTracker()
requests = await tracker.get_requests_by_provider("local_ollama")

# Should include:
assert any(r.task_type == "query_classification" for r in requests)
assert any(r.task_type == "entity_extraction" for r in requests)
assert any(r.task_type == "memory_consolidation" for r in requests)
# ... etc
```

**Files to Modify:**
- `src/agents/router.py`
- `src/components/graph_rag/extraction_service.py`
- `src/components/memory/graphiti_wrapper.py`
- `src/components/graph_rag/community_labeler.py`
- `src/components/graph_rag/dual_level_search.py`
- `src/evaluation/custom_metrics.py`
- `src/components/ingestion/image_processor.py`
- `tests/unit/agents/test_router.py`
- `tests/unit/components/graph_rag/test_extraction_service.py`
- `tests/unit/components/memory/test_graphiti_wrapper.py`
- `tests/integration/components/llm_proxy/test_llm_migration.py` (NEW)

**Prometheus Metrics (if Feature 25.1 completed):**
After migration, `/metrics` endpoint will show:
```prometheus
# HELP llm_requests_total Total LLM requests
# TYPE llm_requests_total counter
llm_requests_total{provider="local_ollama",model="llama3.2:3b",task_type="query_classification"} 1250
llm_requests_total{provider="local_ollama",model="llama3.2:8b",task_type="entity_extraction"} 450
llm_requests_total{provider="local_ollama",model="llama3.2:8b",task_type="memory_consolidation"} 320
# ... etc

# HELP llm_cost_usd Total LLM cost in USD
# TYPE llm_cost_usd counter
llm_cost_usd{provider="local_ollama",model="llama3.2:3b"} 0.00  # Local is free!
llm_cost_usd{provider="alibaba_cloud",model="qwen-turbo"} 2.35
llm_cost_usd{provider="openai",model="gpt-4o-mini"} 5.12
```

---

## üîß Technical Debt Resolution Summary

### Deferred from Sprint 24

| Feature | SP | Priority | Sprint 25 Status |
|---------|----|----|---------|
| 24.1: Prometheus Metrics | 5 | P1 | ‚û°Ô∏è **Feature 25.1** |
| 24.2: Token Tracking | 3 | P2 | ‚û°Ô∏è **Feature 25.3** |
| 24.3: Async/Sync Bridge | 5 | P2 | ‚û°Ô∏è **Feature 25.4** |
| 24.4: MyPy Strict (partial) | 2 | P3 | ‚û°Ô∏è **Feature 25.5** |
| 24.7: Integration Tests | 5 | P1 | ‚û°Ô∏è **Feature 25.2** |
| 24.8: Documentation (partial) | 2 | P3 | ‚û°Ô∏è **Feature 25.6** |

**Total Deferred:** 22 SP

### Still Deferred (Future Sprints)

| ID | Description | Priority | Effort | Deferred To |
|----|-------------|----------|--------|-------------|
| TD-23.1 | ANY-LLM Full Integration | P2 | 8 SP | Sprint 26+ |
| TD-23.2 | DashScope VLM Routing | P3 | 5 SP | Sprint 26+ |
| TD-G.1 | Cross-platform Dev Env | P2 | 1 week | Sprint 27+ |

---

## üìä Sprint 25 Timeline

### Week 1 (Days 1-3): Production Monitoring & Tests

**Day 1: Feature 25.1 - Prometheus Metrics** (5 SP)
- Morning: Implement `src/core/metrics.py`
- Afternoon: Integrate with LLM proxy + cost tracker
- Evening: Create Grafana dashboard

**Day 2: Feature 25.2 - Integration Tests** (5 SP)
- Morning: Write 6 node-level integration tests
- Afternoon: Write E2E pipeline test
- Evening: Fix test failures, validate coverage

**Day 3: Feature 25.3 - Token Tracking** (3 SP)
- Morning: Parse `usage` field from API responses
- Afternoon: Update cost tracker + Prometheus metrics
- Evening: Write unit tests

### Week 1 (Days 4-5): Code Quality & Async Refactoring

**Day 4: Feature 25.4 - Async/Sync Bridge** (5 SP)
- Morning: Refactor `ImageProcessor` to async
- Afternoon: Update callers (langgraph_nodes.py)
- Evening: Update tests, verify no regressions

**Day 5: Features 25.5 + 25.6** (4 SP)
- Morning: MyPy strict mode fixes
- Afternoon: Architecture documentation updates
- Evening: Final review, commit all changes

### Week 2 (Days 6-7): Refactoring Cleanup

**Day 6: Feature 25.7 - Remove Deprecated Code (Part 1)** (2.5 SP)
- Morning: Archive unified_ingestion.py
- Afternoon: Archive three_phase_extractor.py
- Evening: Remove load_documents() method
- Evening: Update imports, tests

**Day 7: Features 25.7-25.9 - Refactoring Cleanup (Part 2)** (7.5 SP)
- Morning: Feature 25.7 completion (verify ADR compliance)
- Afternoon: Feature 25.8 - Consolidate duplicates (base agent, embedding service)
- Evening: Feature 25.9 - Standardize client naming (aliases + imports)

### Week 2 (Day 8): LLM Architecture Consolidation

**Day 8: Feature 25.10 - Migrate All LLM Calls to AegisLLMProxy** (5 SP)
- Morning: Migrate router.py + extraction_service.py (2 SP)
- Afternoon: Migrate graphiti_wrapper.py + community_labeler.py + dual_level_search.py (1.5 SP)
- Evening: Migrate custom_metrics.py + image_processor.py + tests (1.5 SP)

---

## üéØ Sprint 25 Success Metrics

| Metric | Target | Measurement |
|--------|--------|-------------|
| Prometheus Metrics | /metrics endpoint live | curl test |
| Grafana Dashboard | Operational | Manual inspection |
| Integration Test Coverage | >80% | pytest --cov |
| Token Tracking Accuracy | 100% (no 50/50 split) | Database validation |
| Async/Sync Bridge | Removed | Code review |
| MyPy Strict Mode | 0 errors | mypy src/ --strict |
| Documentation Completeness | 100% | Manual review |

---

## üìã Priority Matrix

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ               ‚îÇ  Low Effort (<1 day)   ‚îÇ High Effort (>1 day)   ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ High Impact   ‚îÇ Feature 25.3 (Token)   ‚îÇ Feature 25.1 (Metrics) ‚îÇ
‚îÇ               ‚îÇ Feature 25.6 (Docs)    ‚îÇ Feature 25.2 (Tests)   ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Medium Impact ‚îÇ Feature 25.5 (MyPy)    ‚îÇ Feature 25.4 (Async)   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üîÑ Next Steps After Sprint 25

**Sprint 26 Candidates:**
1. ANY-LLM Full Integration (TD-23.1, 8 SP) - If ANY-LLM adds VLM support
2. DashScope VLM Routing Unification (TD-23.2, 5 SP) - Unified routing
3. Advanced Features:
   - Multi-hop query optimization
   - Profiling module completion
   - Enhanced semantic deduplication

**Sprint 27 Candidates:**
1. Cross-platform Development (TD-G.1, 1 week) - Linux CI/CD
2. Kubernetes Production Deployment
3. Multi-tenant Cost Tracking

---

## ‚úÖ Sprint 25 Completion Criteria

### Production Readiness (Features 25.1-25.6)
- ‚úÖ Prometheus /metrics endpoint operational with LLM-specific metrics
- ‚úÖ Grafana dashboard displaying cost, latency, and token metrics
- ‚úÖ Integration tests for LangGraph pipeline (>80% coverage)
- ‚úÖ Token tracking accuracy improved (accurate input/output split)
- ‚úÖ Async/sync bridge refactored (no ThreadPoolExecutor)
- ‚úÖ MyPy strict mode passing (0 errors)
- ‚úÖ Architecture documentation updated (CURRENT_ARCHITECTURE.md, PRODUCTION_DEPLOYMENT.md)
- ‚úÖ All deferred Sprint 24 features completed

### Refactoring Cleanup (Features 25.7-25.9)
- ‚úÖ Deprecated files archived (unified_ingestion.py, three_phase_extractor.py)
- ‚úÖ load_documents() method removed (ADR-028 compliance)
- ‚úÖ Duplicate base agent removed (base.py deleted)
- ‚úÖ Duplicate embedding service removed (EmbeddingService wrapper deleted)
- ‚úÖ Client naming standardized (<Service>Client pattern)
- ‚úÖ ADR-026, ADR-027, ADR-028 compliance verified
- ‚úÖ 1040+ lines of deprecated/duplicate code removed
- ‚úÖ Backward compatibility aliases in place (deprecation warnings)

### Quality Assurance
- ‚úÖ Test coverage >80% maintained
- ‚úÖ No regressions introduced
- ‚úÖ All imports updated (no broken references)
- ‚úÖ CI passing (100% tests)

---

## üìä Sprint 25 Impact Summary

| Category | Metric | Target | Impact |
|----------|--------|--------|--------|
| **Monitoring** | Prometheus metrics | /metrics endpoint live | Production monitoring |
| **Testing** | Integration tests | >80% coverage | LangGraph pipeline validated |
| **Accuracy** | Token tracking | 100% accurate split | Correct cost calculations |
| **Code Quality** | Deprecated code removed | 725 lines | ADR compliance |
| **Code Quality** | Duplicate code removed | 315 lines | Cleaner codebase |
| **Architecture** | Client naming | Standardized | Consistent API |
| **Total LOC Removed** | Deprecated + duplicates | 1040+ lines | 10% codebase reduction |

---

**Sprint 25 Objectives:** Production monitoring, integration tests, deferred Sprint 24 features, AND Sprint 22 refactoring cleanup ‚úÖ
**Next Sprint:** Sprint 26 - ANY-LLM Full Integration & Advanced Features

**Last Updated:** 2025-11-14
**Author:** Claude Code (Backend Agent)
**Prerequisites:** Sprint 24 complete (Dependency Optimization & CI Performance)
**Story Points:** 40 SP (22 SP deferred from Sprint 24 + 10 SP refactoring + 8 SP new work)
