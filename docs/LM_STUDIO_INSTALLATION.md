# LM Studio Installation & Setup Guide

**Version:** 1.0
**Date:** 2025-10-30
**Purpose:** Sprint 20 Feature 20.2 (LM Studio Parameter Evaluation)

---

## üìã Overview

LM Studio ist ein Desktop-Tool f√ºr lokale LLM-Inferenz mit fortgeschrittenen Parameter-Tuning-Optionen. Es nutzt dieselbe Backend-Technologie wie Ollama (llama.cpp), bietet aber eine grafische Oberfl√§che und mehr Konfigurationsm√∂glichkeiten.

### Warum LM Studio zus√§tzlich zu Ollama?

| Feature | Ollama | LM Studio |
|---------|--------|-----------|
| Backend | llama.cpp | llama.cpp (identisch) |
| API | REST API | OpenAI-compatible API |
| GUI | ‚ùå CLI only | ‚úÖ Grafische Oberfl√§che |
| Parameter Tuning | Basis-Parameter | Erweiterte Parameter |
| Monitoring | Logs only | Live-Monitoring (VRAM, tokens/sec) |
| Model Management | CLI | Drag & Drop |
| Development | ‚≠ê Optimal | ‚≠ê‚≠ê Sehr gut (visuelle Analysen) |
| Production | ‚≠ê‚≠ê Optimal (headless) | ‚ö†Ô∏è Nicht empfohlen (GUI-App) |

**Empfehlung aus LMSTUDIO_VS_OLLAMA_ANALYSIS.md:**
- **Development:** LM Studio f√ºr Parameter-Tuning und visuelle Analyse
- **Production:** Ollama f√ºr Docker-Deployment und CI/CD

---

## üöÄ Installation (Windows 11)

### Schritt 1: Download

1. Besuche: https://lmstudio.ai/
2. Klicke auf **"Download LM Studio"**
3. W√§hle **Windows (x64)** Version
4. Download: `LMStudio-0.2.x-Setup.exe` (~100 MB)

**System Requirements:**
- Windows 10/11 (64-bit)
- 16 GB RAM (32 GB empfohlen)
- NVIDIA GPU mit 8+ GB VRAM (f√ºr Gemma 3 4B Q8)
- ~20 GB freier Speicherplatz

### Schritt 2: Installation

1. F√ºhre `LMStudio-0.2.x-Setup.exe` aus
2. W√§hle Installationspfad (Standard: `C:\Users\<USER>\AppData\Local\Programs\LM Studio`)
3. Akzeptiere Lizenz (MIT License)
4. Warte auf Installation (~2 Minuten)
5. **Wichtig:** Lasse "Start LM Studio" aktiviert

### Schritt 3: Erster Start

Beim ersten Start:
1. LM Studio √∂ffnet sich mit Welcome Screen
2. Akzeptiere Telemetry-Einstellungen (optional)
3. Du siehst das Hauptfenster mit 4 Tabs:
   - **Chat:** Interaktive Chat-Oberfl√§che
   - **Models:** Model Management (Download, L√∂schen)
   - **Local Server:** API-Server (OpenAI-compatible)
   - **Settings:** Konfiguration

---

## üì¶ Model Download

### Gemma 3 4B Instruct Q8 herunterladen

F√ºr Sprint 20 Feature 20.2 ben√∂tigen wir das gleiche Modell wie in Ollama:

1. **Wechsle zum "Models" Tab**
2. **Suche nach "gemma"** in der Suchleiste
3. **Finde:** `google/gemma-2-2b-it-GGUF` oder √§hnliche Varianten
4. **Wichtig:** W√§hle **Q8_0 Quantisierung** (h√∂chste Qualit√§t)
   - `gemma-2-2b-it.Q8_0.gguf` (~4 GB)

   **Alternative (falls Gemma 3 4B nicht verf√ºgbar):**
   - Verwende `gemma-2-2b-it` (2B Parameter, kleiner)
   - Oder lade GGUF-File manuell von Hugging Face

5. **Download starten:** Klicke auf Download-Button
6. **Fortschritt:** Wird unten im Fenster angezeigt
7. **Warte:** Download dauert 5-15 Minuten (abh√§ngig von Internet-Geschwindigkeit)

### Manueller GGUF-Download (falls n√∂tig)

Falls Gemma 3 4B nicht in LM Studio verf√ºgbar:

1. Besuche: https://huggingface.co/models
2. Suche nach: `gemma-3-4b-it GGUF`
3. Finde Repository mit `.gguf` Files
4. Download: `gemma-3-4b-it.Q8_0.gguf` (~5 GB)
5. **Verschiebe File nach:**
   ```
   C:\Users\<USER>\.cache\lm-studio\models\
   ```
6. Starte LM Studio neu ‚Üí Modell erscheint in "Models" Tab

---

## ‚öôÔ∏è Konfiguration f√ºr Sprint 20

### Local Server einrichten

1. **Wechsle zum "Local Server" Tab**
2. **W√§hle Modell:** `gemma-3-4b-it-Q8_0` aus Dropdown
3. **Konfiguriere Server-Settings:**

   ```yaml
   Server Configuration:
     Port: 1234 (Standard)
     CORS: Enabled
     API Format: OpenAI Compatible
   ```

4. **GPU Settings:**
   ```yaml
   GPU Offload: 100% (alle Layers auf GPU)
   Context Length: 8192 tokens
   Thread Count: 8 (match WSL2 CPU cores)
   Batch Size: 512
   ```

5. **Starte Server:** Klicke "Start Server"
6. **Verifiziere:** Gr√ºner Status "Server Running" erscheint

### API-Endpoint testen

√ñffne PowerShell und teste:

```powershell
# Test health endpoint
Invoke-RestMethod -Uri "http://localhost:1234/v1/models" -Method Get

# Expected output:
# {
#   "data": [
#     {
#       "id": "gemma-3-4b-it-Q8_0",
#       "object": "model",
#       "owned_by": "local"
#     }
#   ]
# }
```

Falls Fehler: √úberpr√ºfe, ob Server l√§uft (gr√ºner Status in LM Studio)

---

## üîß Advanced Parameter Configuration

### Parameter-Kategorien in LM Studio

LM Studio bietet Parameter, die in Ollama nicht verf√ºgbar sind:

#### 1. Sampling Parameters (Qualit√§t)

**In LM Studio UI verf√ºgbar:**

```yaml
Temperature: 0.7
  ‚Üí Kreativit√§t (0.0 = deterministisch, 1.0 = sehr kreativ)

Top P: 0.9
  ‚Üí Nucleus Sampling (0.9 = nur top 90% Tokens)

Top K: 40
  ‚Üí Anzahl betrachteter Tokens (40 = top 40 Kandidaten)

Min P: 0.05 ‚≠ê NEU (nicht in Ollama)
  ‚Üí Minimum Probability Threshold

Typical P: 1.0 ‚≠ê NEU (nicht in Ollama)
  ‚Üí Typical Sampling (Alternative zu Top-P)

Mirostat: Off / 1.0 / 2.0 ‚≠ê NEU (nicht in Ollama)
  ‚Üí Adaptive Sampling f√ºr konsistente Perplexity
  ‚Üí Mirostat Tau: 5.0 (Target Entropy)
  ‚Üí Mirostat Eta: 0.1 (Learning Rate)
```

#### 2. Context Management (Speicher)

```yaml
Context Length: 8192
  ‚Üí Max. Tokens im Kontext-Fenster

RoPE Frequency Base: 10000 ‚≠ê NEU
  ‚Üí Rotary Position Embeddings Basis

RoPE Frequency Scale: 1.0 ‚≠ê NEU
  ‚Üí RoPE Skalierung f√ºr l√§ngere Kontexte

Cache Type: f16 / q8_0 / q4_0
  ‚Üí KV-Cache Quantisierung f√ºr VRAM-Optimierung
```

#### 3. Performance Tuning (Geschwindigkeit)

```yaml
Threads: 8
  ‚Üí CPU-Threads (match WSL2: 8 cores)

Batch Size: 512
  ‚Üí Prompt Processing Batch Size

GPU Layers: -1 (all)
  ‚Üí Anzahl Layers auf GPU (-1 = alle)

Use mlock: false ‚≠ê NEU
  ‚Üí Lock model in RAM (verhindert Swapping)

Use mmap: true ‚≠ê NEU
  ‚Üí Memory-map model file (schnelleres Laden)
```

### Wo Parameter in LM Studio setzen?

**Option 1: Local Server Tab (f√ºr API-Nutzung)**
1. Wechsle zu "Local Server" Tab
2. Klicke "Server Options" (‚öôÔ∏è Icon)
3. Setze Parameter in GUI
4. Klicke "Apply" ‚Üí Server neu starten

**Option 2: Chat Tab (f√ºr interaktive Tests)**
1. Wechsle zu "Chat" Tab
2. Lade Modell oben
3. Klicke "Model Settings" (‚öôÔ∏è Icon rechts)
4. Setze Parameter
5. Teste direkt im Chat

**Option 3: API-Request (f√ºr Benchmarking)**
```python
import httpx

response = httpx.post(
    "http://localhost:1234/v1/chat/completions",
    json={
        "model": "gemma-3-4b-it-Q8_0",
        "messages": [{"role": "user", "content": "Hello"}],
        # Advanced parameters (NEU in LM Studio)
        "temperature": 0.7,
        "top_p": 0.9,
        "top_k": 40,
        "min_p": 0.05,           # ‚≠ê NEU
        "typical_p": 0.9,        # ‚≠ê NEU
        "mirostat_mode": 2,      # ‚≠ê NEU (0=off, 1=v1, 2=v2)
        "mirostat_tau": 5.0,     # ‚≠ê NEU
        "mirostat_eta": 0.1,     # ‚≠ê NEU
    }
)
```

---

## üß™ Verwendung f√ºr Sprint 20 Feature 20.2

### Evaluation Script ausf√ºhren

Nach Installation von LM Studio:

1. **Starte LM Studio**
2. **Wechsle zu "Local Server" Tab**
3. **Lade Modell:** `gemma-3-4b-it-Q8_0`
4. **Starte Server:** Port 1234
5. **Lasse LM Studio laufen** (im Hintergrund)

6. **√ñffne neues Terminal (WSL2 oder PowerShell)**
7. **Navigiere zu Projekt:**
   ```bash
   cd AEGIS_Rag
   ```

8. **F√ºhre Evaluation Script aus:**
   ```bash
   poetry run python scripts/evaluate_lm_studio_params.py
   ```

Das Script wird:
- 4 verschiedene Sampling-Configs testen
- Tokens/sec, TTFT, Antwort-Qualit√§t messen
- Ergebnisse in `docs/sprints/SPRINT_20_LM_STUDIO_PARAMS.json` speichern

### Monitoring w√§hrend der Evaluation

**In LM Studio:**
- Wechsle zu "Local Server" Tab
- Beobachte Live-Statistiken:
  - Tokens/sec (Echtzeit)
  - VRAM Usage (MB)
  - Requests Count
  - Active Connections

**Beispiel-Output:**
```
Server Running: gemma-3-4b-it-Q8_0
VRAM: 4821 MB / 8192 MB (58%)
Tokens/sec: 23.4 t/s
Requests: 12 total, 1 active
```

---

## üîç Vergleich: LM Studio vs Ollama API

### Ollama (bisherig)

```bash
# Modell laden
ollama pull gemma-3-4b-it-Q8_0

# API Request
curl http://localhost:11434/api/generate \
  -d '{
    "model": "gemma-3-4b-it-Q8_0",
    "prompt": "Hello",
    "options": {
      "temperature": 0.7,
      "top_p": 0.9,
      "top_k": 40
    }
  }'
```

**Verf√ºgbare Parameter:**
- ‚úÖ temperature
- ‚úÖ top_p
- ‚úÖ top_k
- ‚úÖ repeat_penalty
- ‚ùå min_p (nicht verf√ºgbar)
- ‚ùå typical_p (nicht verf√ºgbar)
- ‚ùå mirostat (nicht verf√ºgbar)

### LM Studio (neu)

```python
import httpx

response = httpx.post(
    "http://localhost:1234/v1/chat/completions",
    json={
        "model": "gemma-3-4b-it-Q8_0",
        "messages": [{"role": "user", "content": "Hello"}],
        "temperature": 0.7,
        "top_p": 0.9,
        "top_k": 40,
        "min_p": 0.05,           # ‚≠ê ZUS√ÑTZLICH
        "typical_p": 0.9,        # ‚≠ê ZUS√ÑTZLICH
        "mirostat_mode": 2,      # ‚≠ê ZUS√ÑTZLICH
        "mirostat_tau": 5.0,     # ‚≠ê ZUS√ÑTZLICH
        "mirostat_eta": 0.1,     # ‚≠ê ZUS√ÑTZLICH
    }
)
```

**Alle Parameter verf√ºgbar:**
- ‚úÖ temperature
- ‚úÖ top_p
- ‚úÖ top_k
- ‚úÖ repeat_penalty
- ‚úÖ min_p ‚≠ê
- ‚úÖ typical_p ‚≠ê
- ‚úÖ mirostat (mode, tau, eta) ‚≠ê

---

## üêõ Troubleshooting

### Problem: Server startet nicht

**Symptom:** "Failed to start server" Fehler

**L√∂sungen:**
1. Pr√ºfe, ob Port 1234 bereits belegt:
   ```powershell
   netstat -ano | findstr :1234
   ```
2. W√§hle anderen Port in LM Studio Settings
3. Oder stoppe andere Anwendung auf Port 1234

### Problem: Modell l√§dt nicht auf GPU

**Symptom:** "Using CPU for inference" Warnung

**L√∂sungen:**
1. Pr√ºfe NVIDIA Treiber:
   ```powershell
   nvidia-smi
   ```
2. In LM Studio: Settings ‚Üí GPU Offload ‚Üí 100%
3. Verringere Context Length (8192 ‚Üí 4096) falls VRAM voll

### Problem: Sehr langsame Inferenz (<5 t/s)

**Symptom:** Tokens/sec unter 5 t/s

**L√∂sungen:**
1. Erh√∂he Thread Count auf 8 (match CPU cores)
2. Verringere Batch Size (512 ‚Üí 256) falls RAM-Problem
3. Pr√ºfe, ob Antivirus LM Studio blockiert
4. Nutze Q4 statt Q8 Quantisierung (kleiner, schneller)

### Problem: "Model not found" API-Fehler

**Symptom:** 404 Not Found bei API-Request

**L√∂sungen:**
1. Pr√ºfe, ob Server l√§uft (gr√ºner Status)
2. Pr√ºfe Modell-Name in API-Request:
   ```python
   # Falsch:
   "model": "gemma-3-4b-it"

   # Richtig:
   "model": "gemma-3-4b-it-Q8_0"
   ```
3. Liste verf√ºgbare Modelle:
   ```bash
   curl http://localhost:1234/v1/models
   ```

---

## üìä Performance Expectations

### Gemma 3 4B Q8 auf RTX 4070 (12 GB VRAM)

**Erwartete Performance:**
- **VRAM Usage:** ~5 GB (bei 8192 Context)
- **Tokens/sec:** 20-30 t/s (CUDA)
- **Time to First Token:** <500ms (warm start)
- **Cold Start:** ~5-10s (Modell-Loading)

**Vergleich zu Ollama:**
- **Identische Geschwindigkeit** (gleicher Backend)
- **+GUI Overhead:** ~100 MB RAM mehr
- **+Monitoring:** Live-Statistiken
- **+Parameter:** Mehr Tuning-Optionen

---

## üéØ N√§chste Schritte nach Installation

### Sprint 20 Feature 20.2 Workflow

1. ‚úÖ **LM Studio installiert** (diese Anleitung)
2. ‚úÖ **Gemma 3 4B Q8 heruntergeladen**
3. ‚úÖ **Local Server l√§uft auf Port 1234**

4. **F√ºhre Parameter-Evaluation aus:**
   ```bash
   poetry run python scripts/evaluate_lm_studio_params.py
   ```

5. **Analysiere Ergebnisse:**
   - √ñffne `docs/sprints/SPRINT_20_LM_STUDIO_PARAMS.json`
   - Vergleiche Sampling-Configs (mirostat vs typical_p vs baseline)
   - Identifiziere beste Config f√ºr Qualit√§t + Geschwindigkeit

6. **Human Evaluation:**
   - Lies Antworten in JSON-File
   - Bewerte mit Rubric aus `test_questions.yaml`
   - Dokumentiere in `SPRINT_20_LM_STUDIO_EVALUATION.md`

7. **A/B Test:**
   - Beste LM Studio Config vs Ollama Baseline
   - Gleiche 10 Fragen aus Feature 20.1
   - Vergleiche Qualit√§t + Performance

8. **Finale Empfehlung:**
   - Update `docs/LMSTUDIO_VS_OLLAMA_ANALYSIS.md`
   - Entscheidung: LM Studio-Parameter √ºbernehmen oder bei Ollama-Defaults bleiben

---

## üìö Weitere Ressourcen

- **LM Studio Dokumentation:** https://lmstudio.ai/docs
- **llama.cpp Parameter:** https://github.com/ggerganov/llama.cpp/blob/master/examples/main/README.md
- **Mirostat Paper:** https://arxiv.org/abs/2007.14966
- **RoPE Scaling:** https://arxiv.org/abs/2104.09864
- **AEGIS_Rag Sprint 20 Plan:** `docs/sprints/SPRINT_20_PLAN.md`

---

## ‚úÖ Installation Complete Checklist

- [ ] LM Studio heruntergeladen und installiert
- [ ] Gemma 3 4B Q8 Modell heruntergeladen (~5 GB)
- [ ] Local Server konfiguriert (Port 1234)
- [ ] Server gestartet (gr√ºner Status)
- [ ] API-Endpoint getestet (`/v1/models` funktioniert)
- [ ] Live-Monitoring funktioniert (VRAM, tokens/sec sichtbar)
- [ ] Bereit f√ºr `evaluate_lm_studio_params.py` Script

**Wenn alle Punkte ‚úÖ:** Sprint 20 Feature 20.2 kann beginnen!

---

**Version History:**
- v1.0 (2025-10-30): Initial version f√ºr Sprint 20 Feature 20.2
