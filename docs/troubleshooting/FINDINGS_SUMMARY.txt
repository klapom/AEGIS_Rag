LightRAG Repository Search - Key Findings Summary
==================================================

Search Completed: October 22, 2025
Repository: https://github.com/HKUDS/LightRAG

CRITICAL FINDINGS
=================

1. EMPTY QUERY RESULTS ROOT CAUSE
Location: /lightrag/operate.py and /lightrag/lightrag.py
Commit: 29bac49f (October 15, 2025)
Title: "Handle empty query results by returning None instead of fail responses"

When no context is found:
{
  "status": "failure",
  "message": "Query returned no results",
  "metadata": {"failure_reason": "no_results", "mode": "..."}
}

This occurs when:
- No entities extracted from knowledge graph
- Vector search returns no matches
- Mixed mode returns empty context

2. CONTEXT WINDOW MISMATCH - CRITICAL
Default settings exceed llama3.2:3b capabilities:

Setting                    Default    llama3.2:3b Limit    Status
MAX_TOTAL_TOKENS          30,000     8,192                EXCEEDS 3.7x!
SUMMARY_CONTEXT_SIZE      12,000     8,192                EXCEEDS
SUMMARY_MAX_TOKENS        1,200      8,192                High but OK
MAX_ENTITY_TOKENS         6,000      8,192                Risky
MAX_RELATION_TOKENS       8,000      8,192                Risky

FIX: Reduce all to fit within 8k budget

3. SMALL MODEL ENTITY EXTRACTION ISSUES
Sources: Commits f69c5dfd, 92f8fc6f, 8a3e2c03

Small models struggle with:
- Complex extraction prompts (2,500+ tokens)
- Strict delimiter rules (<|#|>, <|COMPLETE|>)
- JSON format requirements
- Multiple examples in prompt

Symptom: Empty entities/relationships after insertion

4. EMBEDDING DIMENSION COMPATIBLE
BGE-M3:latest = 1024 dimensions
LightRAG default EMBEDDING_DIM = 1024
No mismatch issue

5. VECTOR SEARCH THRESHOLD TOO STRICT
Default COSINE_THRESHOLD = 0.2
For small models with less precise embeddings, this filters out valid results
Recommendation: Lower to 0.05-0.1

RECENT RELATED FIXES
====================

Commit 29bac49f (Oct 15, 2025)
- Handle empty query results gracefully
- Return None when no context found

Commit 4ce823b4 (Sep 11, 2025)
- Handle empty context in mix mode
- Better logging for vector search

Commit 6cc9411c (Jul 23, 2025)
- Handle empty tasks list in merge
- Prevents ValueError in asyncio.wait()

KEY FILES EXAMINED
==================

Configuration & Defaults:
- /lightrag/constants.py - All DEFAULT_* constants
- /env.example - Query configuration documentation
- /env.ollama-binding-options.example - Ollama options

Core Implementation:
- /lightrag/llm/ollama.py - Ollama integration (180 lines)
- /lightrag/base.py - QueryParam dataclass
- /lightrag/prompt.py - All prompt templates

Examples:
- /examples/lightrag_ollama_demo.py - Working Ollama example

Documentation:
- /docs/OfflineDeployment.md - Offline setup
- /AGENTS.md - Repository guidelines

RECOMMENDED CONFIGURATION FOR llama3.2:3b
=========================================

.env Settings:
TOP_K=15
CHUNK_TOP_K=10
MAX_ENTITY_TOKENS=2500
MAX_RELATION_TOKENS=2500
MAX_TOTAL_TOKENS=7000
CHUNK_SIZE=600
SUMMARY_MAX_TOKENS=600
SUMMARY_LENGTH_RECOMMENDED=400
SUMMARY_CONTEXT_SIZE=4000
COSINE_THRESHOLD=0.05
RERANK_BINDING=null
ENABLE_LLM_CACHE=true

Python Initialization:
llm_model_kwargs={
    "host": "http://localhost:11434",
    "options": {
        "num_ctx": 8192,
        "num_predict": 256,
        "temperature": 0.7,
    },
    "timeout": 300,
}

TROUBLESHOOTING STEPS
====================

1. Enable debug logging:
   export VERBOSE_DEBUG=true
   export LOG_LEVEL=DEBUG

2. Verify entity extraction:
   - Check vdb_entities.json and vdb_relationships.json
   - If empty, extraction failed

3. Test models individually:
   - llama3.2:3b responds to simple prompts
   - bge-m3:latest produces 1024-dim embeddings

4. Reduce token budgets:
   - Set MAX_TOTAL_TOKENS < 8192

5. Lower similarity threshold:
   - Change COSINE_THRESHOLD to 0.05

6. Try "naive" query mode first:
   - Vector search only, no KG

WHEN TO UPGRADE MODEL
====================

llama3.2:3b limitations:
- Only 8k context window
- Poor instruction following
- Weak entity recognition
- No structured output training

Better alternatives:
- llama3.2:7b (2x larger)
- qwen2.5-coder:7b (strong at extraction)
- mistral:7b (good balance)
- neural-chat:7b (conversation optimized)

CONCLUSION
==========

Your issue is caused by:
1. Context window exceeded (30k default vs 8k available)
2. Vector search threshold too strict
3. Possible entity extraction failure

Quick Fix:
- Set MAX_TOTAL_TOKENS to 7000
- Set COSINE_THRESHOLD to 0.05
- Reduce all related token limits

Or upgrade to 7b+ parameter model for full feature support.

