# Quick Start Guide
## AegisRAG - Agentisches RAG-System mit Claude Code

Dieser Guide fÃ¼hrt dich durch das initiale Setup des Projekts in Claude Code.

---

## ğŸ“‹ Voraussetzungen

### Lokale Umgebung
- **Python 3.12.7** installiert (3.11+ kompatibel)
- **Docker & Docker Compose** installiert
- **NVIDIA Container Toolkit** (fÃ¼r GPU-Beschleunigung, siehe unten)
- **Git** konfiguriert
- **Claude Code CLI** installiert

### GPU-Beschleunigung (Sprint 21+)

**NVIDIA Container Toolkit** (erforderlich fÃ¼r Docling CUDA Container):

```powershell
# Windows: NVIDIA Container Toolkit Installation
# Voraussetzung: NVIDIA GPU (z.B. RTX 3060 6GB), CUDA 12.4+

# 1. Docker Desktop mit WSL2 Backend installieren
winget install Docker.DockerDesktop

# 2. NVIDIA Driver installieren (falls nicht vorhanden)
# Download: https://www.nvidia.com/Download/index.aspx

# 3. CUDA Toolkit 12.4+ installieren
# Download: https://developer.nvidia.com/cuda-downloads

# 4. NVIDIA Container Toolkit in WSL2 installieren
wsl
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -
curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list
sudo apt-get update && sudo apt-get install -y nvidia-docker2
sudo systemctl restart docker
exit

# 5. GPU-Zugriff testen
docker run --rm --gpus all nvidia/cuda:12.4.0-base-ubuntu22.04 nvidia-smi
```

**Hinweis:** Ohne GPU lÃ¤uft Docling im CPU-Modus (langsamer, siehe ADR-027).

### Ollama (Local LLM - PrimÃ¤r)

- **Ollama installiert** (https://ollama.ai)
- **Empfohlene Models (Sprint 21)**:
  ```bash
  # Core Models
  ollama pull llama3.2:3b                       # Schnelle Queries (2GB RAM)
  ollama pull llama3.2:8b                       # QualitÃ¤ts-Generierung (4.7GB RAM)

  # Embeddings (Sprint 16: BGE-M3 Migration, ADR-024)
  ollama pull bge-m3                            # Multilingual Embeddings (1024-dim, 2.2GB)

  # Entity/Relation Extraction (Sprint 13, ADR-018)
  ollama pull hf.co/MaziyarPanahi/gemma-3-4b-it-GGUF:Q8_0  # Gemma 3 4B (4.5GB RAM)

  # Vision (Sprint 21 Feature 21.6)
  ollama pull llava:7b-v1.6-mistral-q2_K        # Vision-Language Model (4.3GB VRAM)
  ```

**Total Download:** ~18GB, **VRAM Bedarf:** 6GB (RTX 3060 empfohlen)

### API Keys (Optional fÃ¼r Production)
- Azure OpenAI API Key (optional, falls benÃ¶tigt)
- Optional: LangSmith API Key (fÃ¼r Monitoring)

---

## ğŸš€ Projekt-Setup in Claude Code

### Schritt 1: Strategische Dokumentation Ã¼bertragen

Erstelle ein neues Verzeichnis und kopiere die Strategiedokumente:

```bash
mkdir aegis-rag && cd aegis-rag
mkdir docs

# Kopiere die strategischen Dokumente in das Projekt
cp /home/claude/SPRINT_PLAN.md ./docs/
cp /home/claude/CLAUDE.md ./
cp /home/claude/NAMING_CONVENTIONS.md ./docs/
cp /home/claude/ADR_INDEX.md ./docs/ADR/
cp /home/claude/SUBAGENTS.md ./docs/
cp /home/claude/TECH_STACK.md ./docs/
```

### Schritt 2: Claude Code initialisieren

```bash
# Starte Claude Code im Projektverzeichnis
claude-code .
```

### Schritt 3: Initiales Setup mit Claude Code

Im Claude Code Chat:

```
Hey Claude! Ich mÃ¶chte das AegisRAG-Projekt starten basierend auf der Dokumentation in ./docs/ und CLAUDE.md.

Bitte fÃ¼hre folgende Schritte aus:

**Sprint 1 - Foundation Setup:**
1. Repository-Struktur erstellen (siehe CLAUDE.md)
2. pyproject.toml mit allen Dependencies (siehe TECH_STACK.md)
3. Docker Compose fÃ¼r Qdrant, Neo4j, Redis
4. Basic FastAPI Health-Check Endpoint
5. Pre-commit Hooks Setup
6. GitHub Actions CI Pipeline (lint, test)
7. .env.template mit allen erforderlichen Secrets

**Nutze die Subagenten optimal:**
- Infrastructure Agent: Docker & CI/CD
- Backend Agent: pyproject.toml & core structure
- API Agent: FastAPI setup
- Testing Agent: pytest configuration
- Documentation Agent: README.md

**Wichtig:**
- Folge NAMING_CONVENTIONS.md
- BerÃ¼cksichtige alle ADRs
- Python 3.11+, alle Versionen aus TECH_STACK.md
```

---

## ğŸ“ Erwartete Projekt-Struktur nach Sprint 1

```
aegis-rag/
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â”œâ”€â”€ ci.yml
â”‚       â””â”€â”€ security.yml
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ Dockerfile.api
â”‚   â””â”€â”€ docker-compose.yml
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ ADR/
â”‚   â”‚   â””â”€â”€ ADR_INDEX.md
â”‚   â”œâ”€â”€ SPRINT_PLAN.md
â”‚   â”œâ”€â”€ NAMING_CONVENTIONS.md
â”‚   â”œâ”€â”€ SUBAGENTS.md
â”‚   â””â”€â”€ TECH_STACK.md
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ vector_search/
â”‚   â”‚   â”œâ”€â”€ graph_rag/
â”‚   â”‚   â”œâ”€â”€ memory/
â”‚   â”‚   â””â”€â”€ mcp/
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ config.py
â”‚   â”‚   â”œâ”€â”€ logging.py
â”‚   â”‚   â”œâ”€â”€ models.py
â”‚   â”‚   â””â”€â”€ exceptions.py
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ main.py
â”‚   â”‚   â”œâ”€â”€ health.py
â”‚   â”‚   â””â”€â”€ v1/
â”‚   â””â”€â”€ utils/
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ conftest.py
â”‚   â”œâ”€â”€ unit/
â”‚   â”œâ”€â”€ integration/
â”‚   â””â”€â”€ e2e/
â”œâ”€â”€ scripts/
â”œâ”€â”€ .env.template
â”œâ”€â”€ .gitignore
â”œâ”€â”€ .pre-commit-config.yaml
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ README.md
â””â”€â”€ CLAUDE.md
```

---

## ğŸ”§ Lokale Entwicklung starten

### 1. Environment Setup

```bash
# Virtual Environment erstellen
python -m venv .venv
source .venv/bin/activate  # Linux/Mac
# oder: .venv\Scripts\activate  # Windows

# Dependencies installieren
pip install poetry
poetry install

# Pre-commit Hooks installieren
pre-commit install
```

### 2. Secrets konfigurieren

```bash
# .env erstellen aus Template
cp .env.template .env

# .env editieren und API Keys eintragen
nano .env
```

**Minimale .env fÃ¼r Sprint 21+ (Ollama + Docling):**
```bash
# LLM Configuration (Ollama by default)
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL_GENERATION=llama3.2:8b
OLLAMA_MODEL_QUERY=llama3.2:3b
OLLAMA_MAX_LOADED_MODELS=1  # Sprint 19: Sequential model loading for 6GB VRAM

# Embeddings (Sprint 16: BGE-M3 Migration, ADR-024)
OLLAMA_MODEL_EMBEDDING=bge-m3  # 1024-dim multilingual embeddings

# Entity/Relation Extraction (Sprint 13, ADR-018)
EXTRACTION_LLM_MODEL=hf.co/MaziyarPanahi/gemma-3-4b-it-GGUF:Q8_0

# Extraction Pipeline (Sprint 21, ADR-026)
EXTRACTION_PIPELINE=llm_extraction  # Options: llm_extraction, three_phase, lightrag_default

# Docling Container (Sprint 21, ADR-027)
DOCLING_ENABLED=true
DOCLING_BASE_URL=http://localhost:8080
DOCLING_DEVICE=cuda  # Options: cuda, cpu
DOCLING_MAX_FILE_SIZE=100000000  # 100MB

# Optional: Azure OpenAI (nur falls benÃ¶tigt)
# USE_AZURE_LLM=false
# AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com
# AZURE_OPENAI_API_KEY=your-key
# AZURE_OPENAI_MODEL=gpt-4o

# Databases (Docker defaults)
QDRANT_HOST=localhost
QDRANT_PORT=6333

NEO4J_URI=bolt://localhost:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=changeme

REDIS_HOST=localhost
REDIS_PORT=6379

# Optional: LangSmith Monitoring
LANGSMITH_API_KEY=<optional>
LANGSMITH_PROJECT=aegis-rag
```

### 3. Docker Services starten

```bash
# Alle Services starten (inkl. Docling Container, Sprint 21)
docker compose up -d

# Services prÃ¼fen
docker compose ps

# Logs anschauen
docker compose logs -f

# Einzelne Services:
# Qdrant Dashboard: http://localhost:6333/dashboard
# Neo4j Browser: http://localhost:7474
# Redis: redis-cli -h localhost -p 6379
# Docling API: http://localhost:8080/health (Sprint 21+)

# GPU-Status prÃ¼fen (Docling Container)
docker exec aegis-docling nvidia-smi
```

**Sprint 21 Hinweis - Docling Container:**
- Startet automatisch mit `docker compose up -d`
- BenÃ¶tigt **6GB VRAM** (RTX 3060 oder besser)
- **Container-Lifecycle**: Wird von LangGraph Pipeline gestartet/gestoppt (ADR-027)
- **Healthcheck**: `curl http://localhost:8080/health`
- **GPU-Modus**: Wenn `DOCLING_DEVICE=cuda` in `.env`
- **CPU-Fallback**: Wenn GPU nicht verfÃ¼gbar, lÃ¤uft automatisch auf CPU (langsamer)

### 4. API Server starten

```bash
# Development Server mit Hot Reload
uvicorn src.api.main:app --reload --port 8000

# API testen
curl http://localhost:8000/health

# OpenAPI Docs
open http://localhost:8000/docs
```

### 5. Tests ausfÃ¼hren

```bash
# Alle Tests
pytest

# Mit Coverage
pytest --cov=src --cov-report=html

# Nur Unit Tests
pytest tests/unit/

# Spezifischer Test
pytest tests/unit/agents/test_coordinator.py -v
```

---

## ğŸ“Š Entwicklungs-Workflow

### Feature Development mit Claude Code

```
1. Sprint Planning: Checke SPRINT_PLAN.md fÃ¼r aktuelle Ziele
2. Task Assignment: Delegiere an Subagenten (siehe SUBAGENTS.md)
3. Development: Claude Code implementiert
4. Testing: Testing Agent schreibt Tests
5. Review: Code Quality Gates prÃ¼fen
6. Commit: Conventional Commits (siehe NAMING_CONVENTIONS.md)
7. Deploy: CI/CD Pipeline automatisch
```

### Beispiel: Sprint 2 Task

**Im Claude Code Chat:**
```
Wir starten jetzt Sprint 2: Component 1 - Vector Search Foundation.

**Backend Agent:** 
Implementiere den Qdrant Client Wrapper mit:
- Connection Pooling
- Async I/O
- Error Handling mit Retry Logic
- Health Check Method
Siehe: docs/TECH_STACK.md fÃ¼r Qdrant Version

**Infrastructure Agent:**
Erweitere docker-compose.yml mit:
- Qdrant Service (v1.11.0)
- Persistent Volume
- Health Check
- Resource Limits

**Testing Agent:**
Schreibe Unit Tests fÃ¼r Qdrant Client:
- Test Connection
- Test Search (mocked)
- Test Error Handling
- Coverage >80%

Referenz: CLAUDE.md Sektion "Qdrant Integration"
```

---

## ğŸ› Troubleshooting

### Docker Services starten nicht

```bash
# Ports prÃ¼fen
netstat -tulpn | grep -E '6333|6379|7474|7687|8080'

# Docker Logs prÃ¼fen
docker compose logs qdrant
docker compose logs neo4j
docker compose logs redis
docker compose logs docling  # Sprint 21+

# Services neustarten
docker compose down
docker compose up -d
```

### GPU/CUDA Probleme (Sprint 21+)

**Docling Container startet nicht:**

```powershell
# 1. GPU-VerfÃ¼gbarkeit prÃ¼fen
nvidia-smi

# 2. NVIDIA Container Toolkit prÃ¼fen (WSL2)
wsl
docker run --rm --gpus all nvidia/cuda:12.4.0-base-ubuntu22.04 nvidia-smi
exit

# 3. Docling Container Logs prÃ¼fen
docker compose logs docling

# 4. CPU-Fallback aktivieren (falls GPU-Probleme)
# .env Ã¤ndern:
# DOCLING_DEVICE=cpu
docker compose restart docling

# 5. Container-Berechtigungen prÃ¼fen
docker compose exec docling ls -la /workspace
```

**VRAM-Probleme (Out of Memory):**

```powershell
# 1. VRAM-Nutzung prÃ¼fen
nvidia-smi

# 2. Ollama-Models reduzieren (nur 1 Model gleichzeitig laden)
# .env anpassen:
# OLLAMA_MAX_LOADED_MODELS=1

# 3. Docling Container stoppen wenn nicht benÃ¶tigt
docker compose stop docling

# 4. Kleinere Vision-Models verwenden
ollama pull llava:7b-v1.6-mistral-q2_K  # Statt grÃ¶ÃŸerer Varianten
```

**Performance-Probleme (langsames OCR):**

```powershell
# 1. GPU-Auslastung prÃ¼fen
docker exec aegis-docling nvidia-smi

# 2. GPU-Mode verifizieren
curl http://localhost:8080/health | jq '.device'  # Sollte "cuda" sein

# 3. CUDA-Version prÃ¼fen
docker exec aegis-docling nvcc --version  # Sollte 12.4+ sein

# 4. Falls CPU-Mode: Batch-Processing verwenden
# GroÃŸe PDFs in kleinere Teile splitten
```

### Import Errors

```bash
# Python Path prÃ¼fen
python -c "import sys; print(sys.path)"

# Neu installieren
poetry install --no-cache

# Oder mit pip
pip install -e .
```

### Tests schlagen fehl

```bash
# Pytest Cache lÃ¶schen
pytest --cache-clear

# Nur failed Tests wiederholen
pytest --lf

# Verbose Output
pytest -vv -s
```

### Pre-commit Hooks schlagen fehl

```bash
# Manuell ausfÃ¼hren
pre-commit run --all-files

# Bestimmten Hook skippen (nur fÃ¼r NotfÃ¤lle)
SKIP=mypy git commit -m "message"

# Hooks aktualisieren
pre-commit autoupdate
```

---

## ğŸ“ˆ Progress Tracking

### Sprint 1 Checkliste

- [ ] Repository-Struktur erstellt
- [ ] pyproject.toml konfiguriert
- [ ] Docker Compose funktional (`docker compose up`)
- [ ] Health-Check Endpoint (`/health` returns 200)
- [ ] Pre-commit Hooks installiert
- [ ] CI Pipeline grÃ¼n
- [ ] .env.template erstellt
- [ ] README.md mit Setup Instructions
- [ ] Alle Team-Mitglieder kÃ¶nnen lokal entwickeln

### Definition of Done (Sprint 1)

```bash
# Alle Tests laufen durch
pytest
# âœ… Passed

# Linting erfolgreich
ruff check src/
# âœ… No errors

# Type Checking erfolgreich
mypy src/
# âœ… Success

# Docker Services healthy
docker compose ps
# âœ… All services healthy

# CI Pipeline grÃ¼n
# âœ… Check GitHub Actions
```

---

## ğŸ”„ NÃ¤chste Schritte nach Sprint 1

1. **Sprint 2 vorbereiten:**
   - Test-Dokumente fÃ¼r Vector Search sammeln (PDF, TXT, MD)
   - Ollama Models pullen (llama3.2:3b, llama3.2:8b, nomic-embed-text)
   - Qdrant Collection Schema designen

2. **Team Onboarding:**
   - Alle Entwickler durch Setup fÃ¼hren
   - CLAUDE.md durcharbeiten
   - Claude Code Training

3. **Sprint 2 starten:**
   - Vector Search Foundation implementieren
   - Siehe SPRINT_PLAN.md fÃ¼r Details

---

## ğŸ“š Wichtige Ressourcen

### Interne Dokumentation
- `CLAUDE.md` - Projekt Context fÃ¼r Claude Code
- `docs/SPRINT_PLAN.md` - 10-Sprint Roadmap
- `docs/NAMING_CONVENTIONS.md` - Code Standards
- `docs/ADR_INDEX.md` - Architektur-Entscheidungen
- `docs/SUBAGENTS.md` - Subagent Delegation
- `docs/TECH_STACK.md` - Technology Choices

### Externe Ressourcen
- [LangGraph Docs](https://langchain-ai.github.io/langgraph/)
- [LlamaIndex Docs](https://docs.llamaindex.ai/)
- [Qdrant Docs](https://qdrant.tech/documentation/)
- [Neo4j Cypher](https://neo4j.com/docs/cypher-manual/)
- [FastAPI Docs](https://fastapi.tiangolo.com/)

### Community
- LangChain Discord
- LlamaIndex Discord
- AegisRAG GitHub Discussions (create after repo setup)

---

## ğŸ’¡ Pro Tips fÃ¼r Claude Code

### Effektive Delegation

**Gut:**
```
Backend Agent: Implementiere Hybrid Search mit den Requirements aus TECH_STACK.md.
Nutze Reciprocal Rank Fusion (siehe ADR-008).
Target: <200ms Latency, >80% Test Coverage.
```

**Schlecht:**
```
Mach mal Hybrid Search fertig.
```

### Kontext bereitstellen

Claude Code arbeitet besser mit explizitem Kontext:
```
Kontext: Sprint 2, Tag 3
Ziel: Hybrid Search MVP
AbhÃ¤ngigkeiten: Qdrant Client (bereits implementiert)
Referenz: docs/TECH_STACK.md, Section "Qdrant"
```

### Iteratives Vorgehen

Statt:
```
Implementiere das gesamte RAG-System
```

Besser:
```
1. Qdrant Client Wrapper (heute)
2. Document Ingestion (morgen)
3. Hybrid Search (Ã¼bermorgen)
```

### Feedback geben

Nach jedem Task:
```
âœ… Gut: Fehlerbehandlung, Type Hints, Tests
âš ï¸ Improve: Docstrings fehlen noch
ğŸ“ Next: API Endpoint fÃ¼r Search
```

---

## âœ… Sprint 1 Completion Criteria

**Vor dem Merge zu `main`:**
- [ ] Alle Tests grÃ¼n
- [ ] CI Pipeline erfolgreich
- [ ] Code Review abgeschlossen
- [ ] Documentation vollstÃ¤ndig
- [ ] Demo erfolgreich (`docker compose up` â†’ `/health` â†’ 200 OK)
- [ ] Team Sign-Off

**Dann:**
```bash
git checkout main
git merge develop
git tag v0.1.0
git push origin main --tags
```

---

Viel Erfolg beim Projekt! ğŸš€

Bei Fragen: Checke CLAUDE.md oder frage Claude Code nach spezifischen Implementierungs-Details.
