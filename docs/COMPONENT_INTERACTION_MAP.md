# COMPONENT INTERACTION MAP
**Project:** AEGIS RAG (Agentic Enterprise Graph Intelligence System)
**Purpose:** Complete data flow documentation - how components communicate
**Last Updated:** 2025-12-08 (Sprint 37 - Streaming Pipeline Architecture)

---

## ðŸ“‹ TABLE OF CONTENTS

1. [System Overview](#system-overview)
2. [Request Flow Scenarios](#request-flow-scenarios)
3. [Component Details](#component-details)
4. [API Contracts](#api-contracts)
5. [Data Flow Diagrams](#data-flow-diagrams)

---

## ðŸŽ¯ SYSTEM OVERVIEW

### High-Level Architecture
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         AEGIS RAG System                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚   React Frontend (Port 5173)                        â”‚          â”‚
â”‚  â”‚   (Sprint 15 SSE, Sprint 28 Perplexity UX)          â”‚          â”‚
â”‚  â”‚   Components:                                        â”‚          â”‚
â”‚  â”‚   - SearchResultsPage (SSE streaming)               â”‚          â”‚
â”‚  â”‚   - StreamingAnswer (custom ReactMarkdown)          â”‚          â”‚
â”‚  â”‚   - Citation (inline [1][2][3] with tooltips)       â”‚          â”‚
â”‚  â”‚   - FollowUpQuestions (grid layout, responsive)     â”‚          â”‚
â”‚  â”‚   - Settings (tabbed UI, localStorage)              â”‚          â”‚
â”‚  â”‚   - SettingsContext (React Context API)             â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                 â”‚ HTTP POST /api/v1/chat (SSE)                     â”‚
â”‚                 â–¼                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚              FastAPI Backend (Port 8000)                      â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚  â”‚
â”‚  â”‚  â”‚ Health API â”‚  â”‚ Retrieval APIâ”‚  â”‚ Graph Viz APIâ”‚         â”‚  â”‚
â”‚  â”‚  â”‚ (Sprint 2) â”‚  â”‚  (Sprint 2)  â”‚  â”‚ (Sprint 12)  â”‚         â”‚  â”‚
â”‚  â”‚  â”‚            â”‚  â”‚              â”‚  â”‚ Frontend:    â”‚         â”‚  â”‚
â”‚  â”‚  â”‚            â”‚  â”‚              â”‚  â”‚ Sprint 29 ðŸš§ â”‚         â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                            â”‚                                         â”‚
â”‚                            â–¼                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚          LangGraph Multi-Agent Orchestration                  â”‚  â”‚
â”‚  â”‚                     (Sprint 4-9)                              â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚  â”‚
â”‚  â”‚  â”‚ Router  â”‚â†’ â”‚ Vector  â”‚  â”‚  Graph  â”‚  â”‚ Memory  â”‚         â”‚  â”‚
â”‚  â”‚  â”‚  Agent  â”‚  â”‚  Agent  â”‚  â”‚  Agent  â”‚  â”‚  Agent  â”‚         â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜         â”‚  â”‚
â”‚  â”‚                    â”‚            â”‚            â”‚               â”‚  â”‚
â”‚  â”‚               â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”          â”‚  â”‚
â”‚  â”‚               â”‚     Aggregator Node              â”‚          â”‚  â”‚
â”‚  â”‚               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                 â”‚                                  â”‚
â”‚                                 â–¼                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                   Storage Layer                               â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚  â”‚
â”‚  â”‚  â”‚  Redis  â”‚  â”‚ Qdrant  â”‚  â”‚ Neo4j   â”‚  â”‚AegisLLMProxyâ”‚     â”‚  â”‚
â”‚  â”‚  â”‚(Memory) â”‚  â”‚(Vector) â”‚  â”‚ (Graph) â”‚  â”‚ Multi-Cloud â”‚     â”‚  â”‚
â”‚  â”‚  â”‚Port 6379â”‚  â”‚Port 6333â”‚  â”‚Port 7687â”‚  â”‚ LLM Routing â”‚     â”‚  â”‚
â”‚  â”‚  â”‚         â”‚  â”‚ BGE-M3  â”‚  â”‚         â”‚  â”‚ (ADR-033)   â”‚     â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Component Communication Patterns

| Source | Target | Protocol | Data Format | Purpose |
|--------|--------|----------|-------------|---------|
| Gradio UI | FastAPI | HTTP/REST | JSON | User queries, document upload |
| FastAPI | LangGraph | Python Call | Pydantic | Agent orchestration |
| LangGraph | Redis | Redis Protocol | Pickled State | State persistence |
| Vector Agent | Qdrant | gRPC/HTTP | Protobuf/JSON | Vector search |
| Vector Agent | BM25 | Python Call | Python objects | Keyword search |
| Graph Agent | LightRAG | Python Call | Python objects | Entity extraction |
| Graph Agent | Neo4j | Bolt Protocol | Cypher queries | Graph traversal, RELATES_TO queries |
| Memory Agent | Redis | Redis Protocol | JSON | Short-term memory |
| Memory Agent | Qdrant | gRPC/HTTP | Protobuf/JSON | Semantic memory |
| Memory Agent | Graphiti | Python Call | Python objects | Episodic memory |
| RelationExtractor | AegisLLMProxy | Python Call | Pydantic | RELATES_TO extraction via Qwen3-32B (Sprint 34) |
| RelationExtractor | Neo4j | Bolt Protocol | Cypher queries | Store RELATES_TO relationships |
| GraphViewer | Neo4j API | HTTP/REST | JSON | Edge filtering, relationship queries |
| All Agents | AegisLLMProxy | Python Call | Pydantic | Multi-cloud LLM routing (Sprint 25) |
| AegisLLMProxy | Ollama/Cloud | HTTP | JSON | LLM inference (local â†’ Alibaba â†’ OpenAI) |
| StreamingPipelineOrchestrator | TypedQueue[ChunkQueueItem] | AsyncIO Queue | Pydantic | Inter-stage chunk communication (Sprint 37) |
| StreamingPipelineOrchestrator | TypedQueue[EmbeddedChunkItem] | AsyncIO Queue | Pydantic | Embedding stage output queue (Sprint 37) |
| Admin UI | FastAPI SSE Endpoint | SSE | JSON | Real-time pipeline progress updates (Sprint 37) |
| Worker Pool | StreamingPipelineOrchestrator | Python Call | Pydantic | Dynamic worker configuration (Sprint 37) |

---

## ðŸ”„ REQUEST FLOW SCENARIOS

### Scenario 1: Simple Vector Search Query

**User Query:** "What is RAG?"

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Flow: Simple Vector Search (Hybrid Mode)                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  1. User Input                                                      â”‚
â”‚     â””â”€> Gradio UI: Chatbot.submit("What is RAG?")                 â”‚
â”‚                                                                     â”‚
â”‚  2. HTTP Request                                                    â”‚
â”‚     â””â”€> POST http://localhost:8000/api/v1/chat                    â”‚
â”‚         Body: {                                                     â”‚
â”‚           "query": "What is RAG?",                                 â”‚
â”‚           "session_id": "abc123",                                  â”‚
â”‚           "rag_mode": "hybrid"                                     â”‚
â”‚         }                                                          â”‚
â”‚                                                                     â”‚
â”‚  3. FastAPI Handler                                                 â”‚
â”‚     â””â”€> def chat_endpoint(request: ChatRequest)                   â”‚
â”‚         - Validate request (Pydantic)                              â”‚
â”‚         - Extract query, session_id, rag_mode                      â”‚
â”‚                                                                     â”‚
â”‚  4. LangGraph Invocation                                           â”‚
â”‚     â””â”€> graph = create_agent_graph()                              â”‚
â”‚         initial_state = {                                          â”‚
â”‚           "query": "What is RAG?",                                 â”‚
â”‚           "session_id": "abc123",                                  â”‚
â”‚           "rag_mode": "hybrid"                                     â”‚
â”‚         }                                                          â”‚
â”‚         result = await graph.ainvoke(initial_state)                â”‚
â”‚                                                                     â”‚
â”‚  5. Router Agent (Node: route_query)                               â”‚
â”‚     â””â”€> Classify query type                                       â”‚
â”‚         - Input: "What is RAG?"                                    â”‚
â”‚         - LLM Call: AegisLLMProxy (Sprint 25 Feature 25.10)       â”‚
â”‚           proxy = get_aegis_llm_proxy()                            â”‚
â”‚           response = await proxy.complete(                         â”‚
â”‚             prompt="Classify: What is RAG?",                       â”‚
â”‚             quality=QualityRequirement.LOW,  # Router task         â”‚
â”‚             task_type=TaskType.QUERY_UNDERSTANDING                 â”‚
â”‚           )                                                        â”‚
â”‚           # Multi-cloud routing: Local Ollama â†’ Alibaba â†’ OpenAI  â”‚
â”‚         - Response: {"type": "SIMPLE", "intent": "definition"}     â”‚
â”‚         - Decision: Route to Vector Agent (rag_mode=hybrid)        â”‚
â”‚                                                                     â”‚
â”‚  6. Vector Agent (Node: vector_search)                             â”‚
â”‚     â””â”€> Parallel Execution: Vector + BM25                         â”‚
â”‚                                                                     â”‚
â”‚         A. Embedding Generation                                    â”‚
â”‚            â””â”€> UnifiedEmbeddingService.embed("What is RAG?")      â”‚
â”‚                - LRU Cache Check: MISS                             â”‚
â”‚                - Ollama Call: bge-m3 (Sprint 16)                   â”‚
â”‚                  POST http://localhost:11434/api/embeddings        â”‚
â”‚                  Body: {                                           â”‚
â”‚                    "model": "bge-m3",                              â”‚
â”‚                    "prompt": "What is RAG?"                        â”‚
â”‚                  }                                                 â”‚
â”‚                - Response: [0.123, -0.456, ..., 0.789] (1024d)     â”‚
â”‚                - Cache Store: LRU[hash("What is RAG?")] = vector   â”‚
â”‚                                                                     â”‚
â”‚         B. Vector Search (Qdrant)                                  â”‚
â”‚            â””â”€> QdrantClient.search(                               â”‚
â”‚                  collection="aegis-rag-documents",                 â”‚
â”‚                  query_vector=[0.123, -0.456, ..., 0.789],        â”‚
â”‚                  limit=10                                          â”‚
â”‚                )                                                   â”‚
â”‚                gRPC Call: localhost:6334 (or HTTP :6333)           â”‚
â”‚                Response: [                                         â”‚
â”‚                  {                                                 â”‚
â”‚                    "id": "doc1",                                   â”‚
â”‚                    "score": 0.92,                                  â”‚
â”‚                    "payload": {                                    â”‚
â”‚                      "text": "RAG is Retrieval-Augmented...",      â”‚
â”‚                      "source": "rag_overview.md"                   â”‚
â”‚                    }                                               â”‚
â”‚                  },                                                â”‚
â”‚                  ...                                               â”‚
â”‚                ]                                                   â”‚
â”‚                                                                     â”‚
â”‚         C. BM25 Search (Local)                                     â”‚
â”‚            â””â”€> BM25Search.search("What is RAG?", top_k=10)        â”‚
â”‚                - Load index: pickle.load("bm25_index.pkl")         â”‚
â”‚                - Tokenize query: ["what", "rag"]                   â”‚
â”‚                - Compute BM25 scores                               â”‚
â”‚                - Response: [                                       â”‚
â”‚                    {                                               â”‚
â”‚                      "doc_id": "doc2",                             â”‚
â”‚                      "score": 8.5,                                 â”‚
â”‚                      "text": "Retrieval Augmented Generation..."   â”‚
â”‚                    },                                              â”‚
â”‚                    ...                                             â”‚
â”‚                  ]                                                 â”‚
â”‚                                                                     â”‚
â”‚         D. Reciprocal Rank Fusion                                  â”‚
â”‚            â””â”€> RRF.fuse(                                          â”‚
â”‚                  vector_results=[...],                             â”‚
â”‚                  bm25_results=[...],                               â”‚
â”‚                  k=60                                              â”‚
â”‚                )                                                   â”‚
â”‚                Algorithm:                                          â”‚
â”‚                  for each result r in all_results:                 â”‚
â”‚                    score(r) = sum(1 / (k + rank(r, source)))       â”‚
â”‚                Response: [                                         â”‚
â”‚                  {                                                 â”‚
â”‚                    "doc_id": "doc1",                               â”‚
â”‚                    "rrf_score": 0.035,                             â”‚
â”‚                    "text": "RAG is Retrieval-Augmented...",        â”‚
â”‚                    "sources": ["vector", "bm25"]                   â”‚
â”‚                  },                                                â”‚
â”‚                  ...                                               â”‚
â”‚                ]                                                   â”‚
â”‚                                                                     â”‚
â”‚         E. Reranking (Cross-Encoder)                               â”‚
â”‚            â””â”€> Reranker.rerank(                                   â”‚
â”‚                  query="What is RAG?",                             â”‚
â”‚                  candidates=[...]                                  â”‚
â”‚                )                                                   â”‚
â”‚                Model: sentence-transformers/ms-marco-MiniLM        â”‚
â”‚                For each candidate:                                 â”‚
â”‚                  score = cross_encoder(query, candidate.text)      â”‚
â”‚                Response: [                                         â”‚
â”‚                  {                                                 â”‚
â”‚                    "doc_id": "doc1",                               â”‚
â”‚                    "rerank_score": 0.95,                           â”‚
â”‚                    "text": "RAG is Retrieval-Augmented..."         â”‚
â”‚                  },                                                â”‚
â”‚                  ...                                               â”‚
â”‚                ]                                                   â”‚
â”‚                                                                     â”‚
â”‚  7. Aggregator Node (Node: aggregate_results)                      â”‚
â”‚     â””â”€> Synthesize final answer                                   â”‚
â”‚         - Input: Top 5 reranked documents                          â”‚
â”‚         - Context: "RAG is Retrieval-Augmented Generation..."      â”‚
â”‚         - LLM Call: AegisLLMProxy (Sprint 25)                      â”‚
â”‚           proxy = get_aegis_llm_proxy()                            â”‚
â”‚           response = await proxy.complete(                         â”‚
â”‚             prompt="Answer based on context:\n[context]\n\n        â”‚
â”‚                     Question: What is RAG?",                       â”‚
â”‚             quality=QualityRequirement.MEDIUM,  # Generation task  â”‚
â”‚             task_type=TaskType.GENERATION                          â”‚
â”‚           )                                                        â”‚
â”‚           # Routes to local Ollama (llama3.2:8b) if available      â”‚
â”‚         - Response: {                                              â”‚
â”‚             "answer": "RAG (Retrieval-Augmented Generation)...",   â”‚
â”‚             "sources": ["doc1", "doc2"],                           â”‚
â”‚             "metadata": {...}                                      â”‚
â”‚           }                                                        â”‚
â”‚                                                                     â”‚
â”‚  8. State Update (Redis Checkpointer)                              â”‚
â”‚     â””â”€> Save conversation state                                   â”‚
â”‚         Redis SET session:abc123:state <pickled_state>             â”‚
â”‚         TTL: 86400 seconds (24 hours)                              â”‚
â”‚                                                                     â”‚
â”‚  9. HTTP Response                                                   â”‚
â”‚     â””â”€> FastAPI returns:                                          â”‚
â”‚         {                                                          â”‚
â”‚           "answer": "RAG (Retrieval-Augmented Generation)...",     â”‚
â”‚           "sources": [                                             â”‚
â”‚             {"file": "rag_overview.md", "score": 0.95},            â”‚
â”‚             {"file": "llama_index.md", "score": 0.88}              â”‚
â”‚           ],                                                       â”‚
â”‚           "session_id": "abc123",                                  â”‚
â”‚           "metadata": {                                            â”‚
â”‚             "tokens": 150,                                         â”‚
â”‚             "latency_ms": 450,                                     â”‚
â”‚             "rag_mode": "hybrid"                                   â”‚
â”‚           }                                                        â”‚
â”‚         }                                                          â”‚
â”‚                                                                     â”‚
â”‚  10. UI Update                                                      â”‚
â”‚      â””â”€> Gradio displays answer + sources                         â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Total Latency: ~450ms (with GPU)
- Embedding: 50ms
- Vector Search: 30ms
- BM25 Search: 20ms (parallel)
- RRF Fusion: 10ms
- Reranking: 50ms
- LLM Generation: 250ms (25 tokens @ 105 tokens/s)
- Overhead: 40ms
```

---

### Scenario 2: Graph RAG Query (Relationship Query)

**User Query:** "How are transformers related to attention mechanisms?"

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Flow: Graph RAG Query (Entity Relationships)                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  1-4. [Same as Scenario 1: User Input â†’ FastAPI â†’ LangGraph]       â”‚
â”‚                                                                     â”‚
â”‚  5. Router Agent (Node: route_query)                               â”‚
â”‚     â””â”€> Classify query type                                       â”‚
â”‚         - Input: "How are transformers related to attention?"      â”‚
â”‚         - LLM Classification: "RELATIONSHIP_QUERY"                 â”‚
â”‚         - Decision: Route to Graph Agent                           â”‚
â”‚                                                                     â”‚
â”‚  6. Graph Agent (Node: graph_search)                               â”‚
â”‚     â””â”€> LightRAG Dual-Level Retrieval                             â”‚
â”‚                                                                     â”‚
â”‚         A. Entity Extraction from Query                            â”‚
â”‚            â””â”€> LLM Call: AegisLLMProxy (Sprint 25)                â”‚
â”‚                proxy = get_aegis_llm_proxy()                       â”‚
â”‚                response = await proxy.complete(                    â”‚
â”‚                  prompt="Extract entities:\n                       â”‚
â”‚                          'How are transformers related to          â”‚
â”‚                           attention mechanisms?'",                 â”‚
â”‚                  quality=QualityRequirement.MEDIUM,                â”‚
â”‚                  task_type=TaskType.ENTITY_EXTRACTION              â”‚
â”‚                )                                                   â”‚
â”‚                # Uses extraction-optimized model (gemma-3-4b-it)   â”‚
â”‚                Response: {                                         â”‚
â”‚                  "entities": [                                     â”‚
â”‚                    {"text": "transformers", "type": "MODEL"},      â”‚
â”‚                    {"text": "attention mechanisms", "type":        â”‚
â”‚                     "TECHNIQUE"}                                   â”‚
â”‚                  ]                                                 â”‚
â”‚                }                                                   â”‚
â”‚                                                                     â”‚
â”‚         B. Low-Level Retrieval (Entity Matching)                   â”‚
â”‚            â””â”€> Neo4j Cypher Query                                 â”‚
â”‚                Bolt Connection: localhost:7687                     â”‚
â”‚                Query:                                              â”‚
â”‚                  MATCH (e1:Entity {name: "transformers"})          â”‚
â”‚                  MATCH (e2:Entity {name: "attention mechanisms"})  â”‚
â”‚                  MATCH path = (e1)-[*1..3]-(e2)                    â”‚
â”‚                  RETURN path, relationships(path)                  â”‚
â”‚                  LIMIT 10                                          â”‚
â”‚                                                                     â”‚
â”‚                Response: [                                         â”‚
â”‚                  {                                                 â”‚
â”‚                    "path": [                                       â”‚
â”‚                      {entity: "transformers"},                     â”‚
â”‚                      {rel: "USES", weight: 0.9},                   â”‚
â”‚                      {entity: "attention mechanisms"}              â”‚
â”‚                    ]                                               â”‚
â”‚                  },                                                â”‚
â”‚                  {                                                 â”‚
â”‚                    "path": [                                       â”‚
â”‚                      {entity: "transformers"},                     â”‚
â”‚                      {rel: "COMPONENT_OF", weight: 0.8},           â”‚
â”‚                      {entity: "multi-head attention"},             â”‚
â”‚                      {rel: "IMPLEMENTS", weight: 0.95},            â”‚
â”‚                      {entity: "attention mechanisms"}              â”‚
â”‚                    ]                                               â”‚
â”‚                  }                                                 â”‚
â”‚                ]                                                   â”‚
â”‚                                                                     â”‚
â”‚         C. High-Level Retrieval (Topic/Community Matching)         â”‚
â”‚            â””â”€> Community Detection                                â”‚
â”‚                Neo4j Cypher Query:                                 â”‚
â”‚                  MATCH (e:Entity)-[:BELONGS_TO]->(c:Community)     â”‚
â”‚                  WHERE e.name IN ["transformers",                  â”‚
â”‚                                   "attention mechanisms"]          â”‚
â”‚                  RETURN c.topic, c.summary                         â”‚
â”‚                                                                     â”‚
â”‚                Response: [                                         â”‚
â”‚                  {                                                 â”‚
â”‚                    "topic": "Neural Network Architectures",        â”‚
â”‚                    "summary": "Transformers use attention          â”‚
â”‚                                mechanisms for sequence             â”‚
â”‚                                processing..."                      â”‚
â”‚                  }                                                 â”‚
â”‚                ]                                                   â”‚
â”‚                                                                     â”‚
â”‚         D. Combine Low-Level + High-Level Context                  â”‚
â”‚            â””â”€> Construct graph context:                           â”‚
â”‚                {                                                   â”‚
â”‚                  "entities": ["transformers", "attention"],        â”‚
â”‚                  "relationships": [                                â”‚
â”‚                    "transformers USES attention mechanisms",       â”‚
â”‚                    "transformers COMPONENT_OF multi-head           â”‚
â”‚                     attention"                                     â”‚
â”‚                  ],                                                â”‚
â”‚                  "community_summary": "Neural Network              â”‚
â”‚                    Architectures..."                               â”‚
â”‚                }                                                   â”‚
â”‚                                                                     â”‚
â”‚  7. Aggregator Node (Node: aggregate_results)                      â”‚
â”‚     â””â”€> Synthesize answer with graph context                      â”‚
â”‚         - LLM Call: AegisLLMProxy (Sprint 25)                      â”‚
â”‚         - Context: Graph relationships + community summary         â”‚
â”‚         - Response: "Transformers are fundamentally built on       â”‚
â”‚                      attention mechanisms..."                      â”‚
â”‚                                                                     â”‚
â”‚  8-10. [Same as Scenario 1: State Update â†’ Response â†’ UI]          â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Total Latency: ~800ms (with GPU)
- Entity Extraction: 150ms
- Neo4j Low-Level Query: 200ms
- Neo4j High-Level Query: 150ms
- Context Construction: 50ms
- LLM Generation: 250ms
```

---

### Scenario 3: Memory-Augmented Query

**User Query:** "What did we discuss about RAG earlier?" (continuation of Scenario 1)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Flow: Memory-Augmented Query (3-Layer Memory)                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  1-5. [Same as previous: User Input â†’ Router]                      â”‚
â”‚       Router classifies as "MEMORY_QUERY"                          â”‚
â”‚                                                                     â”‚
â”‚  6. Memory Agent (Node: memory_search)                             â”‚
â”‚     â””â”€> 3-Layer Memory Lookup (Parallel)                          â”‚
â”‚                                                                     â”‚
â”‚         A. Layer 1: Redis (Short-Term Memory)                      â”‚
â”‚            â””â”€> Redis GET session:abc123:messages                  â”‚
â”‚                Redis Connection: localhost:6379                     â”‚
â”‚                Response: [                                         â”‚
â”‚                  {                                                 â”‚
â”‚                    "role": "user",                                 â”‚
â”‚                    "content": "What is RAG?",                      â”‚
â”‚                    "timestamp": "2025-10-22T10:15:00Z"             â”‚
â”‚                  },                                                â”‚
â”‚                  {                                                 â”‚
â”‚                    "role": "assistant",                            â”‚
â”‚                    "content": "RAG is Retrieval-Augmented...",     â”‚
â”‚                    "timestamp": "2025-10-22T10:15:02Z"             â”‚
â”‚                  }                                                 â”‚
â”‚                ]                                                   â”‚
â”‚                Latency: 5ms                                        â”‚
â”‚                                                                     â”‚
â”‚         B. Layer 2: Qdrant (Semantic Long-Term Memory)             â”‚
â”‚            â””â”€> Embed query: "What did we discuss about RAG?"      â”‚
â”‚                UnifiedEmbeddingService â†’ Ollama bge-m3 (Sprint 16) â”‚
â”‚                Vector: [0.234, -0.567, ...] (1024d)                â”‚
â”‚                                                                     â”‚
â”‚                QdrantClient.search(                                â”‚
â”‚                  collection="conversation-history",                â”‚
â”‚                  query_vector=[0.234, -0.567, ...],                â”‚
â”‚                  limit=5,                                          â”‚
â”‚                  filter={                                          â”‚
â”‚                    "session_id": "abc123",                         â”‚
â”‚                    "timestamp": {"$gte": "2025-10-21T00:00:00Z"}   â”‚
â”‚                  }                                                 â”‚
â”‚                )                                                   â”‚
â”‚                                                                     â”‚
â”‚                Response: [                                         â”‚
â”‚                  {                                                 â”‚
â”‚                    "conversation_id": "conv1",                     â”‚
â”‚                    "score": 0.88,                                  â”‚
â”‚                    "summary": "Discussed RAG definition and        â”‚
â”‚                                use cases"                          â”‚
â”‚                  }                                                 â”‚
â”‚                ]                                                   â”‚
â”‚                Latency: 30ms                                       â”‚
â”‚                                                                     â”‚
â”‚         C. Layer 3: Graphiti (Episodic Temporal Memory)            â”‚
â”‚            â””â”€> Graphiti.search(                                   â”‚
â”‚                  query="RAG discussion",                           â”‚
â”‚                  session_id="abc123",                              â”‚
â”‚                  temporal_filter={                                 â”‚
â”‚                    "valid_time": "2025-10-22T10:00:00Z"            â”‚
â”‚                  }                                                 â”‚
â”‚                )                                                   â”‚
â”‚                                                                     â”‚
â”‚                Neo4j Query (via Graphiti):                         â”‚
â”‚                  MATCH (e:Episode {session_id: "abc123"})          â”‚
â”‚                  WHERE e.valid_from <= timestamp() AND             â”‚
â”‚                        e.valid_to >= timestamp()                   â”‚
â”‚                  MATCH (e)-[:CONTAINS]->(f:Fact)                   â”‚
â”‚                  WHERE f.text CONTAINS "RAG"                       â”‚
â”‚                  RETURN e, f                                       â”‚
â”‚                                                                     â”‚
â”‚                Response: [                                         â”‚
â”‚                  {                                                 â”‚
â”‚                    "episode_id": "ep1",                            â”‚
â”‚                    "facts": [                                      â”‚
â”‚                      "User asked about RAG definition",            â”‚
â”‚                      "Assistant explained RAG components"          â”‚
â”‚                    ],                                              â”‚
â”‚                    "timestamp": "2025-10-22T10:15:00Z"             â”‚
â”‚                  }                                                 â”‚
â”‚                ]                                                   â”‚
â”‚                Latency: 150ms                                      â”‚
â”‚                                                                     â”‚
â”‚         D. Memory Fusion                                           â”‚
â”‚            â””â”€> Combine all 3 layers:                              â”‚
â”‚                {                                                   â”‚
â”‚                  "recent_context": [Redis messages],               â”‚
â”‚                  "similar_conversations": [Qdrant results],        â”‚
â”‚                  "episodic_facts": [Graphiti facts]                â”‚
â”‚                }                                                   â”‚
â”‚                                                                     â”‚
â”‚  7. Aggregator Node                                                â”‚
â”‚     â””â”€> Synthesize answer using memory context                    â”‚
â”‚         - LLM sees recent conversation from Redis                  â”‚
â”‚         - LLM sees similar past conversations from Qdrant          â”‚
â”‚         - LLM sees extracted facts from Graphiti                   â”‚
â”‚         - Response: "Earlier we discussed that RAG is              â”‚
â”‚                      Retrieval-Augmented Generation, which..."     â”‚
â”‚                                                                     â”‚
â”‚  8-10. [State Update â†’ Response â†’ UI]                              â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Total Latency: ~350ms
- Redis Lookup: 5ms
- Qdrant Lookup: 30ms (parallel)
- Graphiti Lookup: 150ms (parallel)
- Fusion: 15ms
- LLM Generation: 150ms
```

---

### Scenario 4: Document Ingestion Flow

**User Action:** Upload PDF document "transformer_paper.pdf"

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Flow: Document Ingestion (Parallel Indexing)                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  1. User Upload                                                     â”‚
â”‚     â””â”€> Gradio UI: File.upload("transformer_paper.pdf")           â”‚
â”‚                                                                     â”‚
â”‚  2. HTTP Request                                                    â”‚
â”‚     â””â”€> POST http://localhost:8000/api/v1/documents/upload        â”‚
â”‚         Content-Type: multipart/form-data                          â”‚
â”‚         File: transformer_paper.pdf (5MB)                          â”‚
â”‚                                                                     â”‚
â”‚  3. FastAPI Handler                                                 â”‚
â”‚     â””â”€> async def upload_document(file: UploadFile)               â”‚
â”‚         - Save to temp: /tmp/transformer_paper.pdf                 â”‚
â”‚         - Validate: PDF, <10MB                                     â”‚
â”‚         - Security: Path traversal check                           â”‚
â”‚                                                                     â”‚
â”‚  4. LangGraph Ingestion Pipeline (Sprint 21-22)                    â”‚
â”‚     â””â”€> create_ingestion_graph().ainvoke(initial_state)          â”‚
â”‚                                                                     â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚         â”‚   Sequential Execution (LangGraph StateGraph)       â”‚   â”‚
â”‚         â”‚   (Memory-optimized: 4.4GB RAM, 6GB VRAM)           â”‚   â”‚
â”‚         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚
â”‚         â”‚                                                     â”‚   â”‚
â”‚         â”‚  Node 1: memory_check_node (5% progress)           â”‚   â”‚
â”‚         â”‚     â””â”€> Check document already indexed             â”‚   â”‚
â”‚         â”‚         Neo4j query: MATCH (d:Document {hash: ...}) â”‚   â”‚
â”‚         â”‚         Decision: Skip if duplicate                 â”‚   â”‚
â”‚         â”‚                                                     â”‚   â”‚
â”‚         â”‚  Node 2: format_routing_node (10% progress)        â”‚   â”‚
â”‚         â”‚     â””â”€> FormatRouter.route(document_path)          â”‚   â”‚
â”‚         â”‚         Decision: Docling vs LlamaIndex            â”‚   â”‚
â”‚         â”‚         30+ formats supported (Sprint 22.3)         â”‚   â”‚
â”‚         â”‚         - Docling: 14 formats (PDF, DOCX, PPTX...) â”‚   â”‚
â”‚         â”‚         - LlamaIndex: 9 formats (CSV, Markdown...) â”‚   â”‚
â”‚         â”‚         - Shared: 7 formats (fallback logic)       â”‚   â”‚
â”‚         â”‚                                                     â”‚   â”‚
â”‚         â”‚  Node 3a: docling_parse_node (30% progress)        â”‚   â”‚
â”‚         â”‚     â””â”€> DoclingContainerClient.parse(pdf)          â”‚   â”‚
â”‚         â”‚         GPU-accelerated OCR (EasyOCR)              â”‚   â”‚
â”‚         â”‚         Table structure preservation (92% accuracy) â”‚   â”‚
â”‚         â”‚         Performance: 420s â†’ 120s (3.5x speedup)    â”‚   â”‚
â”‚         â”‚         Raw text: "Attention Is All You Need..."    â”‚   â”‚
â”‚         â”‚                                                     â”‚   â”‚
â”‚         â”‚  Node 3b: llamaindex_parse_node (fallback)         â”‚   â”‚
â”‚         â”‚     â””â”€> LlamaIndexParser.parse(csv)                â”‚   â”‚
â”‚         â”‚         SimpleDirectoryReader.load_data()           â”‚   â”‚
â”‚         â”‚         Connector library (300+ sources)            â”‚   â”‚
â”‚         â”‚                                                     â”‚   â”‚
â”‚         â”‚  Node 4: chunking_node (45% progress)              â”‚   â”‚
â”‚         â”‚     â””â”€> ChunkingService.chunk(                    â”‚   â”‚
â”‚         â”‚           text=parsed_text,                        â”‚   â”‚
â”‚         â”‚           strategy="adaptive"  # Document-aware    â”‚   â”‚
â”‚         â”‚         )                                          â”‚   â”‚
â”‚         â”‚         Unified chunks with SHA-256 IDs (Sprint 16) â”‚   â”‚
â”‚         â”‚         Chunks: [Chunk(id="a3f2e1d9", text=...)]   â”‚   â”‚
â”‚         â”‚         # 45 chunks created                        â”‚   â”‚
â”‚         â”‚                                                     â”‚   â”‚
â”‚         â”‚  Node 5: embedding_node (75% progress)             â”‚   â”‚
â”‚         â”‚     â””â”€> UnifiedEmbeddingService.embed_batch([     â”‚   â”‚
â”‚         â”‚           "Abstract: The dominant...",             â”‚   â”‚
â”‚         â”‚           "Introduction: Recurrent...",            â”‚   â”‚
â”‚         â”‚           ...  # batch of 32 (Sprint 16)           â”‚   â”‚
â”‚         â”‚         ])                                         â”‚   â”‚
â”‚         â”‚                                                     â”‚   â”‚
â”‚         â”‚         Ollama API Call:                           â”‚   â”‚
â”‚         â”‚         POST http://localhost:11434/api/embeddings â”‚   â”‚
â”‚         â”‚         Body: {                                    â”‚   â”‚
â”‚         â”‚           "model": "bge-m3",  # 1024-dim (Sprint 16) â”‚   â”‚
â”‚         â”‚           "inputs": [...]  # batch of 32           â”‚   â”‚
â”‚         â”‚         }                                          â”‚   â”‚
â”‚         â”‚                                                     â”‚   â”‚
â”‚         â”‚         Response: [                                â”‚   â”‚
â”‚         â”‚           [0.123, -0.456, ...],  # 1024d           â”‚   â”‚
â”‚         â”‚           [0.234, -0.567, ...],                    â”‚   â”‚
â”‚         â”‚           ...                                      â”‚   â”‚
â”‚         â”‚         ]                                          â”‚   â”‚
â”‚         â”‚         Cache: Store in LRU cache                  â”‚   â”‚
â”‚         â”‚                                                     â”‚   â”‚
â”‚         â”‚         Upload to Qdrant + BM25:                   â”‚   â”‚
â”‚         â”‚         QdrantClient.upsert(                       â”‚   â”‚
â”‚         â”‚           collection="aegis-rag-documents",        â”‚   â”‚
â”‚         â”‚           points=[{id, vector, payload}, ...]      â”‚   â”‚
â”‚         â”‚         )                                          â”‚   â”‚
â”‚         â”‚         BM25Search.add_documents([...])            â”‚   â”‚
â”‚         â”‚         Latency: ~2s for 45 chunks                 â”‚   â”‚
â”‚         â”‚                                                     â”‚   â”‚
â”‚         â”‚  Node 6: graph_extraction_node (100% progress)     â”‚   â”‚
â”‚         â”‚     â””â”€> LightRAG Entity Extraction                â”‚   â”‚
â”‚         â”‚         For each chunk:                            â”‚   â”‚
â”‚         â”‚         AegisLLMProxy.complete(                    â”‚   â”‚
â”‚         â”‚           prompt="Extract entities from chunk...", â”‚   â”‚
â”‚         â”‚           quality=QualityRequirement.MEDIUM,       â”‚   â”‚
â”‚         â”‚           task_type=TaskType.ENTITY_EXTRACTION     â”‚   â”‚
â”‚         â”‚         )                                          â”‚   â”‚
â”‚         â”‚         # Uses gemma-3-4b-it (extraction-optimized) â”‚   â”‚
â”‚         â”‚                                                     â”‚   â”‚
â”‚         â”‚         Entities extracted: [                      â”‚   â”‚
â”‚         â”‚           {"text": "Transformer", "type": "MODEL"} â”‚   â”‚
â”‚         â”‚           {"text": "Attention Mechanism", ...}     â”‚   â”‚
â”‚         â”‚         ]                                          â”‚   â”‚
â”‚         â”‚                                                     â”‚   â”‚
â”‚         â”‚         Store in Neo4j:                            â”‚   â”‚
â”‚         â”‚         MERGE (e:Entity {name: "Transformer"})     â”‚   â”‚
â”‚         â”‚         MERGE (s)-[r:USES]->(t)                    â”‚   â”‚
â”‚         â”‚         SET r.weight = 0.9                         â”‚   â”‚
â”‚         â”‚                                                     â”‚   â”‚
â”‚         â”‚         Community Detection:                       â”‚   â”‚
â”‚         â”‚         CALL gds.leiden.stream(...)                â”‚   â”‚
â”‚         â”‚         Latency: ~8s for 45 chunks                 â”‚   â”‚
â”‚         â”‚                                                     â”‚   â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                     â”‚
â”‚  5. Progress Tracking                                               â”‚
â”‚     â””â”€> WebSocket updates to UI:                                  â”‚
â”‚         {                                                          â”‚
â”‚           "status": "indexing",                                    â”‚
â”‚           "overall_progress": 0.75,  # 75% complete               â”‚
â”‚           "current_node": "embedding_node",                        â”‚
â”‚           "chunks_processed": 45,                                  â”‚
â”‚           "entities_extracted": 0  # Not yet reached graph node    â”‚
â”‚         }                                                          â”‚
â”‚                                                                     â”‚
â”‚  6. Completion Response                                             â”‚
â”‚     â””â”€> HTTP 200 OK                                               â”‚
â”‚         {                                                          â”‚
â”‚           "status": "success",                                     â”‚
â”‚           "filename": "transformer_paper.pdf",                     â”‚
â”‚           "chunks_created": 45,                                    â”‚
â”‚           "entities_extracted": 87,                                â”‚
â”‚           "relationships_created": 124,                            â”‚
â”‚           "indexing_time_ms": 10000,                               â”‚
â”‚           "doc_hash": "sha256..."                                  â”‚
â”‚         }                                                          â”‚
â”‚                                                                     â”‚
â”‚  7. UI Update                                                       â”‚
â”‚     â””â”€> Gradio shows success message                              â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Total Latency: ~12s (Sequential LangGraph Pipeline, Sprint 21-22)
- Memory Check: ~100ms (Neo4j query)
- Format Routing: ~50ms (rule-based decision)
- Docling Parse: ~3s (GPU-accelerated OCR)
- Chunking: ~500ms (adaptive strategy)
- Embedding: ~2s (BGE-M3, batch of 32)
- Qdrant + BM25: ~1s (parallel upload)
- Graph Extraction: ~5s (LLM entity extraction + Neo4j)
- Community Detection: ~500ms (Leiden algorithm)

Sprint 21 Improvements:
- 3.5x faster parsing (Docling vs LlamaIndex: 120s vs 420s)
- 95% OCR accuracy (EasyOCR GPU vs 70% LlamaIndex)
- 92% table structure preservation
- 30+ format support (FormatRouter Sprint 22.3)
```

---

### Scenario 5: Unified Re-Indexing with BGE-M3 (Sprint 16)

**Admin Action:** Trigger full re-indexing after BGE-M3 migration

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Flow: Unified Re-Indexing (Admin Endpoint)                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  1. Admin Request                                                   â”‚
â”‚     â””â”€> POST /api/v1/admin/reindex?confirm=true                   â”‚
â”‚         Headers: Accept: text/event-stream                          â”‚
â”‚                                                                     â”‚
â”‚  2. Phase 1: Initialization (SSE Event)                             â”‚
â”‚     â””â”€> Validate parameters, load document list                    â”‚
â”‚         SSE: {                                                      â”‚
â”‚           "status": "in_progress",                                  â”‚
â”‚           "phase": "initialization",                                â”‚
â”‚           "documents_total": 933                                    â”‚
â”‚         }                                                          â”‚
â”‚                                                                     â”‚
â”‚  3. Phase 2: Atomic Deletion (SSE Event)                            â”‚
â”‚     â””â”€> Delete all indexes (all-or-nothing):                      â”‚
â”‚         A. Qdrant: DELETE collection "aegis-rag-documents"         â”‚
â”‚         B. BM25: DELETE cache "bm25_index.pkl"                     â”‚
â”‚         C. (Neo4j graph deletion pending Feature 16.6)             â”‚
â”‚         SSE: {                                                      â”‚
â”‚           "status": "in_progress",                                  â”‚
â”‚           "phase": "deletion",                                      â”‚
â”‚           "message": "Deleted Qdrant + BM25 indexes"               â”‚
â”‚         }                                                          â”‚
â”‚                                                                     â”‚
â”‚  4. Phase 3: Unified Chunking (SSE Events)                          â”‚
â”‚     â””â”€> For each document (parallel batches of 10):               â”‚
â”‚                                                                     â”‚
â”‚         ChunkingService.chunk(                                      â”‚
â”‚           text=document.text,                                       â”‚
â”‚           strategy="adaptive",  # Document-aware                   â”‚
â”‚           max_tokens=512,                                          â”‚
â”‚           overlap=128                                              â”‚
â”‚         )                                                          â”‚
â”‚           â†“                                                        â”‚
â”‚         Chunks with SHA-256 IDs:                                   â”‚
â”‚         [                                                          â”‚
â”‚           Chunk(                                                   â”‚
â”‚             chunk_id="a3f2e1d9c8b7",  # Deterministic SHA-256     â”‚
â”‚             text="Abstract: The dominant...",                      â”‚
â”‚             source="transformer_paper.pdf",                        â”‚
â”‚             position=0,                                            â”‚
â”‚             tokens=487                                             â”‚
â”‚           ),                                                       â”‚
â”‚           ...                                                      â”‚
â”‚         ]                                                          â”‚
â”‚                                                                     â”‚
â”‚         SSE: {                                                      â”‚
â”‚           "status": "in_progress",                                  â”‚
â”‚           "phase": "chunking",                                      â”‚
â”‚           "documents_processed": 450,                               â”‚
â”‚           "documents_total": 933,                                   â”‚
â”‚           "progress_percent": 48.2,                                 â”‚
â”‚           "eta_seconds": 1200,                                      â”‚
â”‚           "current_document": "transformer_paper.pdf"               â”‚
â”‚         }                                                          â”‚
â”‚                                                                     â”‚
â”‚  5. Phase 4: BGE-M3 Embedding Generation (SSE Events)               â”‚
â”‚     â””â”€> For each chunk (batch of 32):                             â”‚
â”‚                                                                     â”‚
â”‚         UnifiedEmbeddingService.embed_batch([                       â”‚
â”‚           "Abstract: The dominant...",                              â”‚
â”‚           "Introduction: Recurrent...",                             â”‚
â”‚           ...  # 32 chunks                                         â”‚
â”‚         ])                                                         â”‚
â”‚           â†“                                                        â”‚
â”‚         Ollama API Call:                                           â”‚
â”‚         POST http://localhost:11434/api/embed                      â”‚
â”‚         Body: {                                                    â”‚
â”‚           "model": "bge-m3",  # 1024-dim                           â”‚
â”‚           "inputs": [...]                                          â”‚
â”‚         }                                                          â”‚
â”‚           â†“                                                        â”‚
â”‚         Response: [                                                â”‚
â”‚           [0.123, -0.456, ..., 0.789],  # 1024-dim                 â”‚
â”‚           [0.234, -0.567, ..., 0.890],                             â”‚
â”‚           ...                                                      â”‚
â”‚         ]                                                          â”‚
â”‚                                                                     â”‚
â”‚         SSE: {                                                      â”‚
â”‚           "status": "in_progress",                                  â”‚
â”‚           "phase": "embedding",                                     â”‚
â”‚           "chunks_processed": 2800,                                 â”‚
â”‚           "chunks_total": 10000,                                    â”‚
â”‚           "progress_percent": 28.0,                                 â”‚
â”‚           "eta_seconds": 2400                                       â”‚
â”‚         }                                                          â”‚
â”‚                                                                     â”‚
â”‚  6. Phase 5: Multi-Index Insertion (SSE Events)                     â”‚
â”‚     â””â”€> Insert into all indexes (parallel):                       â”‚
â”‚                                                                     â”‚
â”‚         A. Qdrant Insertion                                         â”‚
â”‚            â””â”€> QdrantClient.upsert(                               â”‚
â”‚                  collection="aegis-rag-documents",                  â”‚
â”‚                  points=[                                          â”‚
â”‚                    {                                               â”‚
â”‚                      "id": "a3f2e1d9c8b7",  # SHA-256             â”‚
â”‚                      "vector": [0.123, ..., 0.789],  # 1024-dim    â”‚
â”‚                      "payload": {                                  â”‚
â”‚                        "text": "Abstract: The...",                 â”‚
â”‚                        "source": "transformer_paper.pdf",          â”‚
â”‚                        "chunk_id": "a3f2e1d9c8b7"                  â”‚
â”‚                      }                                             â”‚
â”‚                    },                                              â”‚
â”‚                    ...                                             â”‚
â”‚                  ]                                                 â”‚
â”‚                )                                                   â”‚
â”‚                                                                     â”‚
â”‚         B. BM25 Indexing (Automatic via Qdrant sync)                â”‚
â”‚            â””â”€> BM25 automatically synchronized                    â”‚
â”‚                No separate indexing needed                         â”‚
â”‚                                                                     â”‚
â”‚         C. LightRAG (Feature 16.6 - uses unified chunks)            â”‚
â”‚            â””â”€> Entity extraction per chunk                        â”‚
â”‚                Neo4j stores chunk_id in :MENTIONED_IN              â”‚
â”‚                                                                     â”‚
â”‚         SSE: {                                                      â”‚
â”‚           "status": "in_progress",                                  â”‚
â”‚           "phase": "indexing",                                      â”‚
â”‚           "indexes": {                                             â”‚
â”‚             "qdrant": "complete",                                  â”‚
â”‚             "bm25": "complete",                                    â”‚
â”‚             "neo4j": "pending"                                     â”‚
â”‚           }                                                        â”‚
â”‚         }                                                          â”‚
â”‚                                                                     â”‚
â”‚  7. Phase 6: Validation (SSE Event)                                 â”‚
â”‚     â””â”€> Verify index consistency:                                 â”‚
â”‚         - Qdrant point count == chunk count                        â”‚
â”‚         - BM25 document count == document count                    â”‚
â”‚         - Neo4j entity count > 0                                   â”‚
â”‚                                                                     â”‚
â”‚         SSE: {                                                      â”‚
â”‚           "status": "complete",                                     â”‚
â”‚           "phase": "validation",                                    â”‚
â”‚           "summary": {                                             â”‚
â”‚             "documents_processed": 933,                             â”‚
â”‚             "chunks_created": 10234,                                â”‚
â”‚             "qdrant_points": 10234,                                 â”‚
â”‚             "bm25_docs": 933,                                       â”‚
â”‚             "neo4j_entities": 1587,                                 â”‚
â”‚             "total_time_seconds": 8940,                             â”‚
â”‚             "embedding_model": "bge-m3",                            â”‚
â”‚             "embedding_dim": 1024                                   â”‚
â”‚           }                                                        â”‚
â”‚         }                                                          â”‚
â”‚                                                                     â”‚
â”‚  8. Admin Dashboard Update                                          â”‚
â”‚     â””â”€> Display completion summary                                â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Total Latency: ~2.5 hours (9,000 seconds)
- Deletion: ~30s (atomic)
- Chunking: ~1,500s (933 docs â†’ 10K chunks)
- Embedding: ~6,000s (10K chunks Ã— 25ms/chunk BGE-M3)
- Indexing: ~1,400s (parallel: Qdrant + BM25)
- Validation: ~10s

Key Improvements (Sprint 16):
- Unified chunks (ChunkingService) â†’ consistent provenance
- BGE-M3 embeddings (1024-dim) â†’ cross-layer similarity
- SSE progress â†’ real-time visibility
- Atomic deletion â†’ no inconsistent state
- Safety checks â†’ confirm=true required
```

---

## ðŸ”§ COMPONENT DETAILS

### FastAPI Endpoints

| Endpoint | Method | Purpose | Request | Response |
|----------|--------|---------|---------|----------|
| `/health` | GET | Health check | - | `{"status": "healthy"}` |
| `/ready` | GET | Readiness check | - | `{"ready": true}` |
| `/live` | GET | Liveness check | - | `{"alive": true}` |
| `/api/v1/chat` | POST | Chat query | `ChatRequest` | `ChatResponse` |
| `/api/v1/documents/upload` | POST | Document upload | `multipart/form-data` | `IngestionResponse` |
| `/api/v1/search` | POST | Raw search (no LLM) | `SearchRequest` | `SearchResponse` |
| `/api/v1/graph/export/json` | GET | Export graph as JSON | - | `GraphJSON` |
| `/api/v1/graph/export/graphml` | GET | Export as GraphML | - | `GraphML` |
| `/stats` | GET | System statistics | - | `SystemStats` |

### LangGraph State Schema

```python
from pydantic import BaseModel
from typing import List, Dict, Optional

class AgentState(BaseModel):
    """Centralized state for LangGraph agents."""

    # Input
    query: str
    session_id: str
    rag_mode: str  # "vector" | "graph" | "hybrid"

    # Router outputs
    query_type: Optional[str] = None  # "SIMPLE" | "COMPOUND" | "MULTI_HOP" | "MEMORY_QUERY"
    selected_agents: List[str] = []  # ["vector", "graph", "memory"]

    # Agent outputs
    vector_results: List[Dict] = []
    graph_results: List[Dict] = []
    memory_results: List[Dict] = []

    # Aggregation
    final_context: str = ""
    final_answer: str = ""
    sources: List[Dict] = []

    # Metadata
    metadata: Dict = {}
    error: Optional[str] = None
```

### Redis Data Structures

```python
# Session State (LangGraph Checkpointer)
KEY: "session:{session_id}:state"
VALUE: <pickled AgentState>
TTL: 86400 seconds (24 hours)

# Recent Messages (Short-Term Memory)
KEY: "session:{session_id}:messages"
VALUE: [
    {
        "role": "user",
        "content": "What is RAG?",
        "timestamp": "2025-10-22T10:15:00Z"
    },
    {
        "role": "assistant",
        "content": "RAG is...",
        "timestamp": "2025-10-22T10:15:02Z"
    }
]
TTL: 3600 seconds (1 hour)

# Embedding Cache (LRU)
KEY: "embedding:{sha256(text)}"
VALUE: [0.123, -0.456, ..., 0.789]  # 1024d vector (BGE-M3, Sprint 16)
TTL: 604800 seconds (7 days)
```

### Qdrant Collections

```python
# Vector Collection
collection_name = "aegis-rag-documents"
vector_size = 1024  # bge-m3 (Sprint 16)
distance = "Cosine"

# Point Structure
{
    "id": "doc1_chunk1",
    "vector": [0.123, -0.456, ...],  # 1024 dimensions (BGE-M3)
    "payload": {
        "text": "RAG is Retrieval-Augmented Generation...",
        "source": "rag_overview.md",
        "chunk_id": 1,
        "doc_hash": "sha256...",
        "metadata": {
            "document_type": "markdown",
            "date_added": "2025-10-22",
            "tags": ["rag", "llm", "retrieval"]
        }
    }
}

# Conversation History Collection
collection_name = "conversation-history"
vector_size = 1024  # bge-m3 (Sprint 16)

# Point Structure
{
    "id": "conv1",
    "vector": [0.234, -0.567, ...],  # 1024 dimensions (BGE-M3)
    "payload": {
        "session_id": "abc123",
        "summary": "Discussed RAG definition and use cases",
        "timestamp": "2025-10-22T10:15:00Z",
        "turns": 2
    }
}
```

### Neo4j Graph Schema

```cypher
// Entity Node
(:Entity {
    name: "Transformer",
    type: "MODEL",
    source: "transformer_paper.pdf",
    first_seen: "2025-10-22T10:15:00Z",
    confidence: 0.95
})

// Relationship Types (Sprint 34)
// 1. RELATES_TO: Semantic relationships (blue #3B82F6)
(:Entity {name: "Transformer"})-[:RELATES_TO {weight: 0.92, extraction_method: "llm"}]->(:Entity {name: "Attention Mechanism"})

// 2. MENTIONED_IN: Chunk references (gray #9CA3AF)
(:Entity {name: "Transformer"})-[:MENTIONED_IN {frequency: 15}]->(:Chunk {id: "chunk_abc123"})

// 3. HAS_SECTION: Document structure (green #10B981)
(:Document {id: "doc_123"})-[:HAS_SECTION]->(:Section {title: "Introduction", page: 1})

// Community Node
(:Community {
    id: "community1",
    topic: "Transformer Architecture",
    summary: "Models and techniques for transformer-based architectures",
    size: 15,
    created: "2025-10-22T10:15:00Z"
})

// Membership
(:Entity {name: "Transformer"})-[:BELONGS_TO]->(:Community {id: "community1"})

// Section Node (Sprint 34)
(:Section {
    id: "section_123",
    title: "Load Balancing",
    document_id: "doc_456",
    page: 2,
    level: 1  # Heading level
})
(:Section {id: "section_123"})-[:PARENT]->(:Document {id: "doc_456"})

// Graphiti Episodic Memory (via Graphiti SDK)
(:Episode {
    session_id: "abc123",
    valid_from: "2025-10-22T10:15:00Z",
    valid_to: "9999-12-31T23:59:59Z",  # Still valid
    transaction_time: "2025-10-22T10:15:00Z"
})-[:CONTAINS]->(:Fact {
    text: "User asked about RAG definition",
    confidence: 0.9
})
```

---

## ðŸ“Š API CONTRACTS

### ChatRequest (POST /api/v1/chat)

```json
{
  "query": "What is RAG?",
  "session_id": "abc123",  // Optional, generated if missing
  "rag_mode": "hybrid",    // "vector" | "graph" | "hybrid"
  "options": {
    "top_k": 5,
    "temperature": 0.7,
    "model": "llama3.2:8b"
  }
}
```

### ChatResponse

```json
{
  "answer": "RAG (Retrieval-Augmented Generation) is a technique...",
  "sources": [
    {
      "file": "rag_overview.md",
      "chunk_id": 1,
      "score": 0.95,
      "text": "RAG is Retrieval-Augmented Generation..."
    },
    {
      "file": "llama_index.md",
      "chunk_id": 3,
      "score": 0.88,
      "text": "LlamaIndex implements RAG patterns..."
    }
  ],
  "session_id": "abc123",
  "metadata": {
    "tokens": 150,
    "latency_ms": 450,
    "rag_mode": "hybrid",
    "agents_used": ["router", "vector", "aggregator"],
    "model": "llama3.2:8b"
  }
}
```

### IngestionResponse (POST /api/v1/documents/upload)

```json
{
  "status": "success",
  "filename": "transformer_paper.pdf",
  "chunks_created": 45,
  "entities_extracted": 87,
  "relationships_created": 124,
  "indexing_time_ms": 10000,
  "doc_hash": "sha256:abc123...",
  "collections_updated": ["aegis-rag-documents", "conversation-history"]
}
```

---

## ðŸŽ¯ KEY TAKEAWAYS

### Critical Data Paths
1. **User Query â†’ LLM Response:** ~400ms (simple), ~800ms (graph), ~350ms (memory)
2. **Document Upload â†’ Indexed:** ~10s (parallel), ~22s (sequential)
3. **Embedding Generation:** ~50ms (cache miss), ~5ms (cache hit)
4. **Redis State Persistence:** <10ms
5. **Neo4j Graph Query:** ~200ms (low-level), ~500ms (high-level)

### Performance Bottlenecks
1. **Entity Extraction:** Slowest part of ingestion (~8s for 45 chunks)
2. **Community Detection:** ~2s per run
3. **LLM Generation:** ~250ms (GPU), ~3.5s (CPU)

### Optimization Strategies
1. **Parallel Indexing:** 2.25x speedup (Sprint 11)
2. **LRU Embedding Cache:** 60% hit rate, ~90% latency reduction
3. **GPU Acceleration:** 15-20x LLM speedup (Sprint 11)
4. **Batch Embedding:** Process 10 chunks at once

---

## ðŸš€ PLANNED: Graph Visualization Frontend (Sprint 29)

**Status:** ðŸ“‹ PLANNED (36 SP, 7-9 days estimated)
**Sprint:** Sprint 29
**Plan Document:** [SPRINT_29_PLAN.md](sprints/SPRINT_29_PLAN.md)

### Overview

Sprint 29 will deliver comprehensive graph visualization frontend for AegisRAG's knowledge graph, enabling end users to explore query results visually and admins to analyze the entire knowledge graph with community detection.

**Technology Stack:**
- **Library:** `react-force-graph` (2D mode with WebGL)
- **API:** Existing Graph Viz API from Sprint 12 (`/api/v1/graph/viz/*`)
- **Styling:** Tailwind CSS + Lucide Icons

### Planned Features (7 Use Cases)

#### 1. Query Result Graph (End User)
**Feature 29.2 (3 SP):** View entities and relationships from query results
- "View Graph" button in StreamingAnswer component
- Modal with interactive graph of entities from retrieved context
- Highlights entities mentioned in answer
- Example: Query "How are transformers related to attention?" â†’ Shows Transformer â†’ USES â†’ Attention Mechanism

#### 2. Admin Graph Analytics
**Feature 29.3 (5 SP):** Admin-only page at `/admin/graph`
- Full knowledge graph visualization (up to 500 nodes)
- Advanced filters: Entity types, minimum degree, max nodes
- Community highlighting
- Graph statistics: Node count, edge count, avg degree

#### 3. Knowledge Graph Dashboard
**Feature 29.4 (5 SP):** Graph metrics and insights
- Quick stats: Total nodes, edges, communities, avg degree
- Entity type distribution (pie chart)
- Graph growth timeline (line chart, last 30 days)
- Top 10 communities by size
- Health metrics: Orphaned nodes, disconnected components

#### 4. Graph Explorer with Search
**Feature 29.5 (5 SP):** Interactive navigation tools
- Node search: Type entity name â†’ graph centers on node with zoom
- Filter by entity type: Show only PERSON, ORGANIZATION, etc.
- Community highlighting: Select community â†’ highlight all members
- Degree filter: Show only highly connected nodes

#### 5. Pan, Zoom, Node Interactions
**Feature 29.1 (5 SP):** Base graph viewer component
- Pan: Click + drag background
- Zoom: Mouse wheel (zoom in/out)
- Node hover: Tooltip with entity name, type, degree
- Node click: Highlight node + connected edges + open details panel
- Performance: 60 FPS with 100+ nodes (WebGL rendering)

#### 6. Embedding-based Document Search
**Feature 29.6 (8 SP):** Find documents from graph nodes
- Click node â†’ Side panel shows "Related Documents"
- Backend: Embed entity name (BGE-M3) â†’ Qdrant vector search
- Display: Top 10 documents with similarity scores
- Click document â†’ Opens document preview
- API: `POST /api/v1/graph/viz/node-documents`

#### 7. Community Document Browser
**Feature 29.7 (5 SP):** Browse documents by community
- Select community â†’ "View Community Documents" button
- Modal shows all documents mentioning entities from community
- Documents grouped by relevance
- Highlights mentioned entities in excerpts
- API: `GET /api/v1/graph/viz/communities/{id}/documents`

### Component Architecture

```
frontend/src/
â”œâ”€â”€ pages/
â”‚   â”œâ”€â”€ GraphVisualizationPage.tsx        # Main graph page
â”‚   â””â”€â”€ admin/
â”‚       â””â”€â”€ GraphAnalyticsPage.tsx        # Admin-only page
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ graph/
â”‚   â”‚   â”œâ”€â”€ GraphViewer.tsx               # Core 2D graph (react-force-graph)
â”‚   â”‚   â”œâ”€â”€ GraphControls.tsx             # Pan/Zoom/Reset controls
â”‚   â”‚   â”œâ”€â”€ GraphSearch.tsx               # Node search
â”‚   â”‚   â”œâ”€â”€ CommunityHighlight.tsx        # Community highlighting
â”‚   â”‚   â”œâ”€â”€ NodeDetailsPanel.tsx          # Selected node info + docs
â”‚   â”‚   â”œâ”€â”€ CommunityDocuments.tsx        # Community â†’ Documents
â”‚   â”‚   â”œâ”€â”€ GraphFilters.tsx              # Entity type, degree filters
â”‚   â”‚   â””â”€â”€ GraphExportButton.tsx         # Export JSON/GraphML/Cytoscape
â”‚   â””â”€â”€ dashboard/
â”‚       â”œâ”€â”€ KnowledgeGraphDashboard.tsx   # Statistics dashboard
â”‚       â”œâ”€â”€ GraphStatistics.tsx           # Node/Edge/Community counts
â”‚       â””â”€â”€ TopCommunities.tsx            # Top 10 communities
â”œâ”€â”€ api/
â”‚   â””â”€â”€ graphViz.ts                       # API client for graph viz
â””â”€â”€ hooks/
    â”œâ”€â”€ useGraphData.ts                   # Fetch graph data
    â”œâ”€â”€ useGraphSearch.ts                 # Node search logic
    â””â”€â”€ useDocumentsByNode.ts             # Embedding-based doc search
```

### API Endpoints

**Existing (Sprint 12):**
- âœ… `POST /api/v1/graph/viz/export` - Export graph (JSON/GraphML/Cytoscape)
- âœ… `GET /api/v1/graph/viz/export/formats` - Supported formats
- âœ… `POST /api/v1/graph/viz/filter` - Filter by entity types/degree
- âœ… `POST /api/v1/graph/viz/communities/highlight` - Highlight communities

**New (Sprint 29):**
- âŒ `POST /api/v1/graph/viz/query-subgraph` - Get entities from query results (Feature 29.2)
- âŒ `GET /api/v1/graph/viz/statistics` - Graph statistics (Feature 29.4)
- âŒ `POST /api/v1/graph/viz/node-documents` - Documents by entity (Feature 29.6)
- âŒ `GET /api/v1/graph/viz/communities/{id}/documents` - Community documents (Feature 29.7)

### Performance Targets

- **Graph Rendering:** <1s for 100 nodes, <3s for 500 nodes
- **Search Latency:** <200ms for node search
- **Document Lookup:** <500ms for embedding-based search
- **Frame Rate:** 60 FPS with pan/zoom (WebGL rendering)

### Success Criteria

- [ ] All 7 features implemented and tested
- [ ] GraphViewer renders 100+ nodes at 60 FPS
- [ ] End users can view query result graphs
- [ ] Admins can explore entire knowledge graph
- [ ] Document search from graph nodes works
- [ ] Community document browser functional
- [ ] Unit test coverage >80%
- [ ] E2E tests pass for all user flows
- [ ] Documentation updated (API docs, user guide)

---

**Last Updated:** 2025-12-01 (Sprint 34 - Knowledge Graph Enhancement)
**Status:** Active Development

**Architecture Changes Since Sprint 16:**
- **Sprint 21-22:** LangGraph ingestion pipeline with Docling CUDA + Format Router
- **Sprint 23:** AegisLLMProxy multi-cloud LLM routing (ADR-033)
- **Sprint 25:** Complete migration to AegisLLMProxy (Feature 25.10)
- **Sprint 28:** Frontend UX enhancements (Perplexity-style interface)
- **Sprint 34:** Knowledge graph enhancement with RELATES_TO relationships and edge visualization

**Current Architecture (Sprint 34):**
- **Embeddings:** BGE-M3 (1024-dim, Sprint 16) - Local & Cost-Free
- **LLM Routing:** AegisLLMProxy (Local Ollama â†’ Alibaba Cloud â†’ OpenAI)
- **Search Strategy:** Hybrid (Vector BGE-M3 + BM25 Keyword + RRF Fusion)
- **Graph Relationships:** RELATES_TO (semantic), MENTIONED_IN (chunk refs), HAS_SECTION (document structure)
- **Edge Visualization:** Color-coded by type (Blue: RELATES_TO, Gray: MENTIONED_IN, Green: HAS_SECTION)
- **Ingestion:** LangGraph pipeline (Docling primary, LlamaIndex fallback)
- **Document Formats:** 30+ formats (FormatRouter Sprint 22.3)
- **Relation Extraction:** Pure LLM via AegisLLMProxy (Alibaba Cloud Qwen3-32B)

**Next:** Sprint 29 (Graph Visualization Frontend - 36 SP, 7-9 days)
