# COMPONENT INTERACTION MAP
**Project:** AEGIS RAG (Agentic Enterprise Graph Intelligence System)
**Purpose:** Complete data flow documentation - how components communicate
**Last Updated:** 2025-11-18 (Sprint 28 - Frontend UX Enhancements)

---

## ðŸ“‹ TABLE OF CONTENTS

1. [System Overview](#system-overview)
2. [Request Flow Scenarios](#request-flow-scenarios)
3. [Component Details](#component-details)
4. [API Contracts](#api-contracts)
5. [Data Flow Diagrams](#data-flow-diagrams)

---

## ðŸŽ¯ SYSTEM OVERVIEW

### High-Level Architecture
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         AEGIS RAG System                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚   React Frontend (Port 5173)                        â”‚          â”‚
â”‚  â”‚   (Sprint 15 SSE, Sprint 28 Perplexity UX)          â”‚          â”‚
â”‚  â”‚   Components:                                        â”‚          â”‚
â”‚  â”‚   - SearchResultsPage (SSE streaming)               â”‚          â”‚
â”‚  â”‚   - StreamingAnswer (custom ReactMarkdown)          â”‚          â”‚
â”‚  â”‚   - Citation (inline [1][2][3] with tooltips)       â”‚          â”‚
â”‚  â”‚   - FollowUpQuestions (grid layout, responsive)     â”‚          â”‚
â”‚  â”‚   - Settings (tabbed UI, localStorage)              â”‚          â”‚
â”‚  â”‚   - SettingsContext (React Context API)             â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                 â”‚ HTTP POST /api/v1/chat (SSE)                     â”‚
â”‚                 â–¼                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚              FastAPI Backend (Port 8000)                      â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚  â”‚
â”‚  â”‚  â”‚ Health API â”‚  â”‚ Retrieval APIâ”‚  â”‚ Graph Viz APIâ”‚         â”‚  â”‚
â”‚  â”‚  â”‚ (Sprint 2) â”‚  â”‚  (Sprint 2)  â”‚  â”‚ (Sprint 12)  â”‚         â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                            â”‚                                         â”‚
â”‚                            â–¼                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚          LangGraph Multi-Agent Orchestration                  â”‚  â”‚
â”‚  â”‚                     (Sprint 4-9)                              â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚  â”‚
â”‚  â”‚  â”‚ Router  â”‚â†’ â”‚ Vector  â”‚  â”‚  Graph  â”‚  â”‚ Memory  â”‚         â”‚  â”‚
â”‚  â”‚  â”‚  Agent  â”‚  â”‚  Agent  â”‚  â”‚  Agent  â”‚  â”‚  Agent  â”‚         â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜         â”‚  â”‚
â”‚  â”‚                    â”‚            â”‚            â”‚               â”‚  â”‚
â”‚  â”‚               â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”          â”‚  â”‚
â”‚  â”‚               â”‚     Aggregator Node              â”‚          â”‚  â”‚
â”‚  â”‚               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                 â”‚                                  â”‚
â”‚                                 â–¼                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                   Storage Layer                               â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚  â”‚
â”‚  â”‚  â”‚  Redis  â”‚  â”‚ Qdrant  â”‚  â”‚ Neo4j   â”‚  â”‚ Ollama  â”‚         â”‚  â”‚
â”‚  â”‚  â”‚(Memory) â”‚  â”‚(Vector) â”‚  â”‚ (Graph) â”‚  â”‚  (LLM)  â”‚         â”‚  â”‚
â”‚  â”‚  â”‚Port 6379â”‚  â”‚Port 6333â”‚  â”‚Port 7687â”‚  â”‚Port     â”‚         â”‚  â”‚
â”‚  â”‚  â”‚         â”‚  â”‚         â”‚  â”‚         â”‚  â”‚ 11434   â”‚         â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Component Communication Patterns

| Source | Target | Protocol | Data Format | Purpose |
|--------|--------|----------|-------------|---------|
| Gradio UI | FastAPI | HTTP/REST | JSON | User queries, document upload |
| FastAPI | LangGraph | Python Call | Pydantic | Agent orchestration |
| LangGraph | Redis | Redis Protocol | Pickled State | State persistence |
| Vector Agent | Qdrant | gRPC/HTTP | Protobuf/JSON | Vector search |
| Vector Agent | BM25 | Python Call | Python objects | Keyword search |
| Graph Agent | LightRAG | Python Call | Python objects | Entity extraction |
| Graph Agent | Neo4j | Bolt Protocol | Cypher queries | Graph traversal |
| Memory Agent | Redis | Redis Protocol | JSON | Short-term memory |
| Memory Agent | Qdrant | gRPC/HTTP | Protobuf/JSON | Semantic memory |
| Memory Agent | Graphiti | Python Call | Python objects | Episodic memory |
| All Agents | Ollama | HTTP | JSON | LLM inference |

---

## ðŸ”„ REQUEST FLOW SCENARIOS

### Scenario 1: Simple Vector Search Query

**User Query:** "What is RAG?"

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Flow: Simple Vector Search (Hybrid Mode)                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  1. User Input                                                      â”‚
â”‚     â””â”€> Gradio UI: Chatbot.submit("What is RAG?")                 â”‚
â”‚                                                                     â”‚
â”‚  2. HTTP Request                                                    â”‚
â”‚     â””â”€> POST http://localhost:8000/api/v1/chat                    â”‚
â”‚         Body: {                                                     â”‚
â”‚           "query": "What is RAG?",                                 â”‚
â”‚           "session_id": "abc123",                                  â”‚
â”‚           "rag_mode": "hybrid"                                     â”‚
â”‚         }                                                          â”‚
â”‚                                                                     â”‚
â”‚  3. FastAPI Handler                                                 â”‚
â”‚     â””â”€> def chat_endpoint(request: ChatRequest)                   â”‚
â”‚         - Validate request (Pydantic)                              â”‚
â”‚         - Extract query, session_id, rag_mode                      â”‚
â”‚                                                                     â”‚
â”‚  4. LangGraph Invocation                                           â”‚
â”‚     â””â”€> graph = create_agent_graph()                              â”‚
â”‚         initial_state = {                                          â”‚
â”‚           "query": "What is RAG?",                                 â”‚
â”‚           "session_id": "abc123",                                  â”‚
â”‚           "rag_mode": "hybrid"                                     â”‚
â”‚         }                                                          â”‚
â”‚         result = await graph.ainvoke(initial_state)                â”‚
â”‚                                                                     â”‚
â”‚  5. Router Agent (Node: route_query)                               â”‚
â”‚     â””â”€> Classify query type                                       â”‚
â”‚         - Input: "What is RAG?"                                    â”‚
â”‚         - LLM Call: Ollama llama3.2:3b                             â”‚
â”‚           POST http://localhost:11434/api/generate                 â”‚
â”‚           Body: {                                                  â”‚
â”‚             "model": "llama3.2:3b",                                â”‚
â”‚             "prompt": "Classify: What is RAG?",                    â”‚
â”‚             "stream": false                                        â”‚
â”‚           }                                                        â”‚
â”‚         - Response: {"type": "SIMPLE", "intent": "definition"}     â”‚
â”‚         - Decision: Route to Vector Agent (rag_mode=hybrid)        â”‚
â”‚                                                                     â”‚
â”‚  6. Vector Agent (Node: vector_search)                             â”‚
â”‚     â””â”€> Parallel Execution: Vector + BM25                         â”‚
â”‚                                                                     â”‚
â”‚         A. Embedding Generation                                    â”‚
â”‚            â””â”€> EmbeddingService.get_embedding("What is RAG?")     â”‚
â”‚                - LRU Cache Check: MISS                             â”‚
â”‚                - Ollama Call: nomic-embed-text                     â”‚
â”‚                  POST http://localhost:11434/api/embeddings        â”‚
â”‚                  Body: {                                           â”‚
â”‚                    "model": "nomic-embed-text",                    â”‚
â”‚                    "prompt": "What is RAG?"                        â”‚
â”‚                  }                                                 â”‚
â”‚                - Response: [0.123, -0.456, ..., 0.789] (768d)      â”‚
â”‚                - Cache Store: LRU[hash("What is RAG?")] = vector   â”‚
â”‚                                                                     â”‚
â”‚         B. Vector Search (Qdrant)                                  â”‚
â”‚            â””â”€> QdrantClient.search(                               â”‚
â”‚                  collection="aegis-rag-documents",                 â”‚
â”‚                  query_vector=[0.123, -0.456, ..., 0.789],        â”‚
â”‚                  limit=10                                          â”‚
â”‚                )                                                   â”‚
â”‚                gRPC Call: localhost:6334 (or HTTP :6333)           â”‚
â”‚                Response: [                                         â”‚
â”‚                  {                                                 â”‚
â”‚                    "id": "doc1",                                   â”‚
â”‚                    "score": 0.92,                                  â”‚
â”‚                    "payload": {                                    â”‚
â”‚                      "text": "RAG is Retrieval-Augmented...",      â”‚
â”‚                      "source": "rag_overview.md"                   â”‚
â”‚                    }                                               â”‚
â”‚                  },                                                â”‚
â”‚                  ...                                               â”‚
â”‚                ]                                                   â”‚
â”‚                                                                     â”‚
â”‚         C. BM25 Search (Local)                                     â”‚
â”‚            â””â”€> BM25Search.search("What is RAG?", top_k=10)        â”‚
â”‚                - Load index: pickle.load("bm25_index.pkl")         â”‚
â”‚                - Tokenize query: ["what", "rag"]                   â”‚
â”‚                - Compute BM25 scores                               â”‚
â”‚                - Response: [                                       â”‚
â”‚                    {                                               â”‚
â”‚                      "doc_id": "doc2",                             â”‚
â”‚                      "score": 8.5,                                 â”‚
â”‚                      "text": "Retrieval Augmented Generation..."   â”‚
â”‚                    },                                              â”‚
â”‚                    ...                                             â”‚
â”‚                  ]                                                 â”‚
â”‚                                                                     â”‚
â”‚         D. Reciprocal Rank Fusion                                  â”‚
â”‚            â””â”€> RRF.fuse(                                          â”‚
â”‚                  vector_results=[...],                             â”‚
â”‚                  bm25_results=[...],                               â”‚
â”‚                  k=60                                              â”‚
â”‚                )                                                   â”‚
â”‚                Algorithm:                                          â”‚
â”‚                  for each result r in all_results:                 â”‚
â”‚                    score(r) = sum(1 / (k + rank(r, source)))       â”‚
â”‚                Response: [                                         â”‚
â”‚                  {                                                 â”‚
â”‚                    "doc_id": "doc1",                               â”‚
â”‚                    "rrf_score": 0.035,                             â”‚
â”‚                    "text": "RAG is Retrieval-Augmented...",        â”‚
â”‚                    "sources": ["vector", "bm25"]                   â”‚
â”‚                  },                                                â”‚
â”‚                  ...                                               â”‚
â”‚                ]                                                   â”‚
â”‚                                                                     â”‚
â”‚         E. Reranking (Cross-Encoder)                               â”‚
â”‚            â””â”€> Reranker.rerank(                                   â”‚
â”‚                  query="What is RAG?",                             â”‚
â”‚                  candidates=[...]                                  â”‚
â”‚                )                                                   â”‚
â”‚                Model: sentence-transformers/ms-marco-MiniLM        â”‚
â”‚                For each candidate:                                 â”‚
â”‚                  score = cross_encoder(query, candidate.text)      â”‚
â”‚                Response: [                                         â”‚
â”‚                  {                                                 â”‚
â”‚                    "doc_id": "doc1",                               â”‚
â”‚                    "rerank_score": 0.95,                           â”‚
â”‚                    "text": "RAG is Retrieval-Augmented..."         â”‚
â”‚                  },                                                â”‚
â”‚                  ...                                               â”‚
â”‚                ]                                                   â”‚
â”‚                                                                     â”‚
â”‚  7. Aggregator Node (Node: aggregate_results)                      â”‚
â”‚     â””â”€> Synthesize final answer                                   â”‚
â”‚         - Input: Top 5 reranked documents                          â”‚
â”‚         - Context: "RAG is Retrieval-Augmented Generation..."      â”‚
â”‚         - LLM Call: Ollama llama3.2:8b                             â”‚
â”‚           POST http://localhost:11434/api/generate                 â”‚
â”‚           Body: {                                                  â”‚
â”‚             "model": "llama3.2:8b",                                â”‚
â”‚             "prompt": "Answer based on context:\n[context]\n\n     â”‚
â”‚                        Question: What is RAG?",                    â”‚
â”‚             "stream": false                                        â”‚
â”‚           }                                                        â”‚
â”‚         - Response: {                                              â”‚
â”‚             "answer": "RAG (Retrieval-Augmented Generation)...",   â”‚
â”‚             "sources": ["doc1", "doc2"],                           â”‚
â”‚             "metadata": {...}                                      â”‚
â”‚           }                                                        â”‚
â”‚                                                                     â”‚
â”‚  8. State Update (Redis Checkpointer)                              â”‚
â”‚     â””â”€> Save conversation state                                   â”‚
â”‚         Redis SET session:abc123:state <pickled_state>             â”‚
â”‚         TTL: 86400 seconds (24 hours)                              â”‚
â”‚                                                                     â”‚
â”‚  9. HTTP Response                                                   â”‚
â”‚     â””â”€> FastAPI returns:                                          â”‚
â”‚         {                                                          â”‚
â”‚           "answer": "RAG (Retrieval-Augmented Generation)...",     â”‚
â”‚           "sources": [                                             â”‚
â”‚             {"file": "rag_overview.md", "score": 0.95},            â”‚
â”‚             {"file": "llama_index.md", "score": 0.88}              â”‚
â”‚           ],                                                       â”‚
â”‚           "session_id": "abc123",                                  â”‚
â”‚           "metadata": {                                            â”‚
â”‚             "tokens": 150,                                         â”‚
â”‚             "latency_ms": 450,                                     â”‚
â”‚             "rag_mode": "hybrid"                                   â”‚
â”‚           }                                                        â”‚
â”‚         }                                                          â”‚
â”‚                                                                     â”‚
â”‚  10. UI Update                                                      â”‚
â”‚      â””â”€> Gradio displays answer + sources                         â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Total Latency: ~450ms (with GPU)
- Embedding: 50ms
- Vector Search: 30ms
- BM25 Search: 20ms (parallel)
- RRF Fusion: 10ms
- Reranking: 50ms
- LLM Generation: 250ms (25 tokens @ 105 tokens/s)
- Overhead: 40ms
```

---

### Scenario 2: Graph RAG Query (Relationship Query)

**User Query:** "How are transformers related to attention mechanisms?"

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Flow: Graph RAG Query (Entity Relationships)                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  1-4. [Same as Scenario 1: User Input â†’ FastAPI â†’ LangGraph]       â”‚
â”‚                                                                     â”‚
â”‚  5. Router Agent (Node: route_query)                               â”‚
â”‚     â””â”€> Classify query type                                       â”‚
â”‚         - Input: "How are transformers related to attention?"      â”‚
â”‚         - LLM Classification: "RELATIONSHIP_QUERY"                 â”‚
â”‚         - Decision: Route to Graph Agent                           â”‚
â”‚                                                                     â”‚
â”‚  6. Graph Agent (Node: graph_search)                               â”‚
â”‚     â””â”€> LightRAG Dual-Level Retrieval                             â”‚
â”‚                                                                     â”‚
â”‚         A. Entity Extraction from Query                            â”‚
â”‚            â””â”€> LLM Call: Ollama qwen3:0.6b                        â”‚
â”‚                POST http://localhost:11434/api/generate            â”‚
â”‚                Body: {                                             â”‚
â”‚                  "model": "qwen3:0.6b",                            â”‚
â”‚                  "prompt": "Extract entities:\n                    â”‚
â”‚                             'How are transformers related to       â”‚
â”‚                              attention mechanisms?'",              â”‚
â”‚                  "stream": false                                   â”‚
â”‚                }                                                   â”‚
â”‚                Response: {                                         â”‚
â”‚                  "entities": [                                     â”‚
â”‚                    {"text": "transformers", "type": "MODEL"},      â”‚
â”‚                    {"text": "attention mechanisms", "type":        â”‚
â”‚                     "TECHNIQUE"}                                   â”‚
â”‚                  ]                                                 â”‚
â”‚                }                                                   â”‚
â”‚                                                                     â”‚
â”‚         B. Low-Level Retrieval (Entity Matching)                   â”‚
â”‚            â””â”€> Neo4j Cypher Query                                 â”‚
â”‚                Bolt Connection: localhost:7687                     â”‚
â”‚                Query:                                              â”‚
â”‚                  MATCH (e1:Entity {name: "transformers"})          â”‚
â”‚                  MATCH (e2:Entity {name: "attention mechanisms"})  â”‚
â”‚                  MATCH path = (e1)-[*1..3]-(e2)                    â”‚
â”‚                  RETURN path, relationships(path)                  â”‚
â”‚                  LIMIT 10                                          â”‚
â”‚                                                                     â”‚
â”‚                Response: [                                         â”‚
â”‚                  {                                                 â”‚
â”‚                    "path": [                                       â”‚
â”‚                      {entity: "transformers"},                     â”‚
â”‚                      {rel: "USES", weight: 0.9},                   â”‚
â”‚                      {entity: "attention mechanisms"}              â”‚
â”‚                    ]                                               â”‚
â”‚                  },                                                â”‚
â”‚                  {                                                 â”‚
â”‚                    "path": [                                       â”‚
â”‚                      {entity: "transformers"},                     â”‚
â”‚                      {rel: "COMPONENT_OF", weight: 0.8},           â”‚
â”‚                      {entity: "multi-head attention"},             â”‚
â”‚                      {rel: "IMPLEMENTS", weight: 0.95},            â”‚
â”‚                      {entity: "attention mechanisms"}              â”‚
â”‚                    ]                                               â”‚
â”‚                  }                                                 â”‚
â”‚                ]                                                   â”‚
â”‚                                                                     â”‚
â”‚         C. High-Level Retrieval (Topic/Community Matching)         â”‚
â”‚            â””â”€> Community Detection                                â”‚
â”‚                Neo4j Cypher Query:                                 â”‚
â”‚                  MATCH (e:Entity)-[:BELONGS_TO]->(c:Community)     â”‚
â”‚                  WHERE e.name IN ["transformers",                  â”‚
â”‚                                   "attention mechanisms"]          â”‚
â”‚                  RETURN c.topic, c.summary                         â”‚
â”‚                                                                     â”‚
â”‚                Response: [                                         â”‚
â”‚                  {                                                 â”‚
â”‚                    "topic": "Neural Network Architectures",        â”‚
â”‚                    "summary": "Transformers use attention          â”‚
â”‚                                mechanisms for sequence             â”‚
â”‚                                processing..."                      â”‚
â”‚                  }                                                 â”‚
â”‚                ]                                                   â”‚
â”‚                                                                     â”‚
â”‚         D. Combine Low-Level + High-Level Context                  â”‚
â”‚            â””â”€> Construct graph context:                           â”‚
â”‚                {                                                   â”‚
â”‚                  "entities": ["transformers", "attention"],        â”‚
â”‚                  "relationships": [                                â”‚
â”‚                    "transformers USES attention mechanisms",       â”‚
â”‚                    "transformers COMPONENT_OF multi-head           â”‚
â”‚                     attention"                                     â”‚
â”‚                  ],                                                â”‚
â”‚                  "community_summary": "Neural Network              â”‚
â”‚                    Architectures..."                               â”‚
â”‚                }                                                   â”‚
â”‚                                                                     â”‚
â”‚  7. Aggregator Node (Node: aggregate_results)                      â”‚
â”‚     â””â”€> Synthesize answer with graph context                      â”‚
â”‚         - LLM Call: Ollama llama3.2:8b                             â”‚
â”‚         - Context: Graph relationships + community summary         â”‚
â”‚         - Response: "Transformers are fundamentally built on       â”‚
â”‚                      attention mechanisms..."                      â”‚
â”‚                                                                     â”‚
â”‚  8-10. [Same as Scenario 1: State Update â†’ Response â†’ UI]          â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Total Latency: ~800ms (with GPU)
- Entity Extraction: 150ms
- Neo4j Low-Level Query: 200ms
- Neo4j High-Level Query: 150ms
- Context Construction: 50ms
- LLM Generation: 250ms
```

---

### Scenario 3: Memory-Augmented Query

**User Query:** "What did we discuss about RAG earlier?" (continuation of Scenario 1)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Flow: Memory-Augmented Query (3-Layer Memory)                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  1-5. [Same as previous: User Input â†’ Router]                      â”‚
â”‚       Router classifies as "MEMORY_QUERY"                          â”‚
â”‚                                                                     â”‚
â”‚  6. Memory Agent (Node: memory_search)                             â”‚
â”‚     â””â”€> 3-Layer Memory Lookup (Parallel)                          â”‚
â”‚                                                                     â”‚
â”‚         A. Layer 1: Redis (Short-Term Memory)                      â”‚
â”‚            â””â”€> Redis GET session:abc123:messages                  â”‚
â”‚                Redis Connection: localhost:6379                     â”‚
â”‚                Response: [                                         â”‚
â”‚                  {                                                 â”‚
â”‚                    "role": "user",                                 â”‚
â”‚                    "content": "What is RAG?",                      â”‚
â”‚                    "timestamp": "2025-10-22T10:15:00Z"             â”‚
â”‚                  },                                                â”‚
â”‚                  {                                                 â”‚
â”‚                    "role": "assistant",                            â”‚
â”‚                    "content": "RAG is Retrieval-Augmented...",     â”‚
â”‚                    "timestamp": "2025-10-22T10:15:02Z"             â”‚
â”‚                  }                                                 â”‚
â”‚                ]                                                   â”‚
â”‚                Latency: 5ms                                        â”‚
â”‚                                                                     â”‚
â”‚         B. Layer 2: Qdrant (Semantic Long-Term Memory)             â”‚
â”‚            â””â”€> Embed query: "What did we discuss about RAG?"      â”‚
â”‚                EmbeddingService â†’ Ollama nomic-embed-text          â”‚
â”‚                Vector: [0.234, -0.567, ...]                        â”‚
â”‚                                                                     â”‚
â”‚                QdrantClient.search(                                â”‚
â”‚                  collection="conversation-history",                â”‚
â”‚                  query_vector=[0.234, -0.567, ...],                â”‚
â”‚                  limit=5,                                          â”‚
â”‚                  filter={                                          â”‚
â”‚                    "session_id": "abc123",                         â”‚
â”‚                    "timestamp": {"$gte": "2025-10-21T00:00:00Z"}   â”‚
â”‚                  }                                                 â”‚
â”‚                )                                                   â”‚
â”‚                                                                     â”‚
â”‚                Response: [                                         â”‚
â”‚                  {                                                 â”‚
â”‚                    "conversation_id": "conv1",                     â”‚
â”‚                    "score": 0.88,                                  â”‚
â”‚                    "summary": "Discussed RAG definition and        â”‚
â”‚                                use cases"                          â”‚
â”‚                  }                                                 â”‚
â”‚                ]                                                   â”‚
â”‚                Latency: 30ms                                       â”‚
â”‚                                                                     â”‚
â”‚         C. Layer 3: Graphiti (Episodic Temporal Memory)            â”‚
â”‚            â””â”€> Graphiti.search(                                   â”‚
â”‚                  query="RAG discussion",                           â”‚
â”‚                  session_id="abc123",                              â”‚
â”‚                  temporal_filter={                                 â”‚
â”‚                    "valid_time": "2025-10-22T10:00:00Z"            â”‚
â”‚                  }                                                 â”‚
â”‚                )                                                   â”‚
â”‚                                                                     â”‚
â”‚                Neo4j Query (via Graphiti):                         â”‚
â”‚                  MATCH (e:Episode {session_id: "abc123"})          â”‚
â”‚                  WHERE e.valid_from <= timestamp() AND             â”‚
â”‚                        e.valid_to >= timestamp()                   â”‚
â”‚                  MATCH (e)-[:CONTAINS]->(f:Fact)                   â”‚
â”‚                  WHERE f.text CONTAINS "RAG"                       â”‚
â”‚                  RETURN e, f                                       â”‚
â”‚                                                                     â”‚
â”‚                Response: [                                         â”‚
â”‚                  {                                                 â”‚
â”‚                    "episode_id": "ep1",                            â”‚
â”‚                    "facts": [                                      â”‚
â”‚                      "User asked about RAG definition",            â”‚
â”‚                      "Assistant explained RAG components"          â”‚
â”‚                    ],                                              â”‚
â”‚                    "timestamp": "2025-10-22T10:15:00Z"             â”‚
â”‚                  }                                                 â”‚
â”‚                ]                                                   â”‚
â”‚                Latency: 150ms                                      â”‚
â”‚                                                                     â”‚
â”‚         D. Memory Fusion                                           â”‚
â”‚            â””â”€> Combine all 3 layers:                              â”‚
â”‚                {                                                   â”‚
â”‚                  "recent_context": [Redis messages],               â”‚
â”‚                  "similar_conversations": [Qdrant results],        â”‚
â”‚                  "episodic_facts": [Graphiti facts]                â”‚
â”‚                }                                                   â”‚
â”‚                                                                     â”‚
â”‚  7. Aggregator Node                                                â”‚
â”‚     â””â”€> Synthesize answer using memory context                    â”‚
â”‚         - LLM sees recent conversation from Redis                  â”‚
â”‚         - LLM sees similar past conversations from Qdrant          â”‚
â”‚         - LLM sees extracted facts from Graphiti                   â”‚
â”‚         - Response: "Earlier we discussed that RAG is              â”‚
â”‚                      Retrieval-Augmented Generation, which..."     â”‚
â”‚                                                                     â”‚
â”‚  8-10. [State Update â†’ Response â†’ UI]                              â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Total Latency: ~350ms
- Redis Lookup: 5ms
- Qdrant Lookup: 30ms (parallel)
- Graphiti Lookup: 150ms (parallel)
- Fusion: 15ms
- LLM Generation: 150ms
```

---

### Scenario 4: Document Ingestion Flow

**User Action:** Upload PDF document "transformer_paper.pdf"

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Flow: Document Ingestion (Parallel Indexing)                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  1. User Upload                                                     â”‚
â”‚     â””â”€> Gradio UI: File.upload("transformer_paper.pdf")           â”‚
â”‚                                                                     â”‚
â”‚  2. HTTP Request                                                    â”‚
â”‚     â””â”€> POST http://localhost:8000/api/v1/documents/upload        â”‚
â”‚         Content-Type: multipart/form-data                          â”‚
â”‚         File: transformer_paper.pdf (5MB)                          â”‚
â”‚                                                                     â”‚
â”‚  3. FastAPI Handler                                                 â”‚
â”‚     â””â”€> async def upload_document(file: UploadFile)               â”‚
â”‚         - Save to temp: /tmp/transformer_paper.pdf                 â”‚
â”‚         - Validate: PDF, <10MB                                     â”‚
â”‚         - Security: Path traversal check                           â”‚
â”‚                                                                     â”‚
â”‚  4. Unified Ingestion Pipeline (Parallel)                          â”‚
â”‚     â””â”€> UnifiedIngestionPipeline.ingest(file_path, mode="all")   â”‚
â”‚                                                                     â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚         â”‚   Parallel Execution (asyncio.gather)               â”‚   â”‚
â”‚         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚
â”‚         â”‚                                                     â”‚   â”‚
â”‚         â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚
â”‚         â”‚  â”‚  Task 1: Qdrant Indexing                     â”‚  â”‚   â”‚
â”‚         â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚
â”‚         â”‚       â”‚                                            â”‚   â”‚
â”‚         â”‚       â–¼                                            â”‚   â”‚
â”‚         â”‚  A. Load Document (LlamaIndex)                     â”‚   â”‚
â”‚         â”‚     â””â”€> SimpleDirectoryReader.load_data()         â”‚   â”‚
â”‚         â”‚         PDF Parser: pypdf                          â”‚   â”‚
â”‚         â”‚         Pages: 12                                  â”‚   â”‚
â”‚         â”‚         Text: "Attention Is All You Need..."       â”‚   â”‚
â”‚         â”‚                                                     â”‚   â”‚
â”‚         â”‚  B. Chunk Text (Adaptive Chunking)                 â”‚   â”‚
â”‚         â”‚     â””â”€> AdaptiveChunker.chunk(                    â”‚   â”‚
â”‚         â”‚           text=doc_text,                           â”‚   â”‚
â”‚         â”‚           strategy="paragraph"  # PDF detected     â”‚   â”‚
â”‚         â”‚         )                                          â”‚   â”‚
â”‚         â”‚         Chunks: [                                  â”‚   â”‚
â”‚         â”‚           "Abstract: The dominant...",             â”‚   â”‚
â”‚         â”‚           "Introduction: Recurrent...",            â”‚   â”‚
â”‚         â”‚           ...                                      â”‚   â”‚
â”‚         â”‚         ]  # 45 chunks                             â”‚   â”‚
â”‚         â”‚                                                     â”‚   â”‚
â”‚         â”‚  C. Generate Embeddings (Batch)                    â”‚   â”‚
â”‚         â”‚     â””â”€> For each chunk (parallel batches):        â”‚   â”‚
â”‚         â”‚         EmbeddingService.get_embeddings_batch([    â”‚   â”‚
â”‚         â”‚           "Abstract: The dominant...",             â”‚   â”‚
â”‚         â”‚           "Introduction: Recurrent...",            â”‚   â”‚
â”‚         â”‚           ...                                      â”‚   â”‚
â”‚         â”‚         ])                                         â”‚   â”‚
â”‚         â”‚                                                     â”‚   â”‚
â”‚         â”‚         Ollama API Call:                           â”‚   â”‚
â”‚         â”‚         POST http://localhost:11434/api/embeddings â”‚   â”‚
â”‚         â”‚         Body: {                                    â”‚   â”‚
â”‚         â”‚           "model": "nomic-embed-text",             â”‚   â”‚
â”‚         â”‚           "texts": [...]  # batch of 10            â”‚   â”‚
â”‚         â”‚         }                                          â”‚   â”‚
â”‚         â”‚                                                     â”‚   â”‚
â”‚         â”‚         Response: [                                â”‚   â”‚
â”‚         â”‚           [0.123, -0.456, ...],  # 768d            â”‚   â”‚
â”‚         â”‚           [0.234, -0.567, ...],                    â”‚   â”‚
â”‚         â”‚           ...                                      â”‚   â”‚
â”‚         â”‚         ]                                          â”‚   â”‚
â”‚         â”‚         Cache: Store in LRU cache                  â”‚   â”‚
â”‚         â”‚                                                     â”‚   â”‚
â”‚         â”‚  D. Upload to Qdrant                               â”‚   â”‚
â”‚         â”‚     â””â”€> QdrantClient.upsert(                      â”‚   â”‚
â”‚         â”‚           collection="aegis-rag-documents",        â”‚   â”‚
â”‚         â”‚           points=[                                 â”‚   â”‚
â”‚         â”‚             {                                      â”‚   â”‚
â”‚         â”‚               "id": "doc1_chunk1",                 â”‚   â”‚
â”‚         â”‚               "vector": [0.123, -0.456, ...],      â”‚   â”‚
â”‚         â”‚               "payload": {                         â”‚   â”‚
â”‚         â”‚                 "text": "Abstract: The...",        â”‚   â”‚
â”‚         â”‚                 "source": "transformer_paper.pdf", â”‚   â”‚
â”‚         â”‚                 "chunk_id": 1,                     â”‚   â”‚
â”‚         â”‚                 "doc_hash": "sha256..."            â”‚   â”‚
â”‚         â”‚               }                                    â”‚   â”‚
â”‚         â”‚             },                                     â”‚   â”‚
â”‚         â”‚             ...  # 45 points                       â”‚   â”‚
â”‚         â”‚           ]                                        â”‚   â”‚
â”‚         â”‚         )                                          â”‚   â”‚
â”‚         â”‚         gRPC Call: localhost:6334                  â”‚   â”‚
â”‚         â”‚         Latency: ~2s for 45 chunks                 â”‚   â”‚
â”‚         â”‚                                                     â”‚   â”‚
â”‚         â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚
â”‚         â”‚  â”‚  Task 2: BM25 Indexing                       â”‚  â”‚   â”‚
â”‚         â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚
â”‚         â”‚       â”‚                                            â”‚   â”‚
â”‚         â”‚       â–¼                                            â”‚   â”‚
â”‚         â”‚  A. Tokenize Chunks                                â”‚   â”‚
â”‚         â”‚     â””â”€> For each chunk:                           â”‚   â”‚
â”‚         â”‚         tokens = tokenize(chunk.text)              â”‚   â”‚
â”‚         â”‚         ["attention", "all", "need", ...]          â”‚   â”‚
â”‚         â”‚                                                     â”‚   â”‚
â”‚         â”‚  B. Update BM25 Index                              â”‚   â”‚
â”‚         â”‚     â””â”€> BM25Search.add_documents([                â”‚   â”‚
â”‚         â”‚           {                                        â”‚   â”‚
â”‚         â”‚             "id": "doc1_chunk1",                   â”‚   â”‚
â”‚         â”‚             "tokens": ["attention", "all", ...],   â”‚   â”‚
â”‚         â”‚             "text": "Abstract: The..."             â”‚   â”‚
â”‚         â”‚           },                                       â”‚   â”‚
â”‚         â”‚           ...  # 45 chunks                         â”‚   â”‚
â”‚         â”‚         ])                                         â”‚   â”‚
â”‚         â”‚                                                     â”‚   â”‚
â”‚         â”‚  C. Save Index to Disk                             â”‚   â”‚
â”‚         â”‚     â””â”€> pickle.dump(bm25_index,                   â”‚   â”‚
â”‚         â”‚                      "data/bm25/bm25_index.pkl")   â”‚   â”‚
â”‚         â”‚         Latency: ~500ms for 45 chunks              â”‚   â”‚
â”‚         â”‚                                                     â”‚   â”‚
â”‚         â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚
â”‚         â”‚  â”‚  Task 3: LightRAG Indexing                   â”‚  â”‚   â”‚
â”‚         â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚
â”‚         â”‚       â”‚                                            â”‚   â”‚
â”‚         â”‚       â–¼                                            â”‚   â”‚
â”‚         â”‚  A. Entity Extraction (per chunk)                  â”‚   â”‚
â”‚         â”‚     â””â”€> For each chunk:                           â”‚   â”‚
â”‚         â”‚         LLM Call: Ollama llama3.2:3b               â”‚   â”‚
â”‚         â”‚         POST http://localhost:11434/api/generate   â”‚   â”‚
â”‚         â”‚         Body: {                                    â”‚   â”‚
â”‚         â”‚           "model": "llama3.2:3b",                  â”‚   â”‚
â”‚         â”‚           "prompt": "Extract entities from:\n      â”‚   â”‚
â”‚         â”‚                      'Abstract: The dominant...'", â”‚   â”‚
â”‚         â”‚           "stream": false                          â”‚   â”‚
â”‚         â”‚         }                                          â”‚   â”‚
â”‚         â”‚                                                     â”‚   â”‚
â”‚         â”‚         Response: {                                â”‚   â”‚
â”‚         â”‚           "entities": [                            â”‚   â”‚
â”‚         â”‚             {                                      â”‚   â”‚
â”‚         â”‚               "text": "Transformer",               â”‚   â”‚
â”‚         â”‚               "type": "MODEL"                      â”‚   â”‚
â”‚         â”‚             },                                     â”‚   â”‚
â”‚         â”‚             {                                      â”‚   â”‚
â”‚         â”‚               "text": "Attention Mechanism",       â”‚   â”‚
â”‚         â”‚               "type": "TECHNIQUE"                  â”‚   â”‚
â”‚         â”‚             }                                      â”‚   â”‚
â”‚         â”‚           ],                                       â”‚   â”‚
â”‚         â”‚           "relationships": [                       â”‚   â”‚
â”‚         â”‚             {                                      â”‚   â”‚
â”‚         â”‚               "source": "Transformer",             â”‚   â”‚
â”‚         â”‚               "rel": "USES",                       â”‚   â”‚
â”‚         â”‚               "target": "Attention Mechanism"      â”‚   â”‚
â”‚         â”‚             }                                      â”‚   â”‚
â”‚         â”‚           ]                                        â”‚   â”‚
â”‚         â”‚         }                                          â”‚   â”‚
â”‚         â”‚         Latency: ~500ms per chunk Ã— 45 chunks      â”‚   â”‚
â”‚         â”‚                  = 22.5s (parallel batches â†’ 8s)   â”‚   â”‚
â”‚         â”‚                                                     â”‚   â”‚
â”‚         â”‚  B. Store in Neo4j                                 â”‚   â”‚
â”‚         â”‚     â””â”€> For each entity:                          â”‚   â”‚
â”‚         â”‚         Neo4j Cypher:                              â”‚   â”‚
â”‚         â”‚           MERGE (e:Entity {name: "Transformer"})   â”‚   â”‚
â”‚         â”‚           SET e.type = "MODEL",                    â”‚   â”‚
â”‚         â”‚               e.source = "transformer_paper.pdf"   â”‚   â”‚
â”‚         â”‚                                                     â”‚   â”‚
â”‚         â”‚         For each relationship:                     â”‚   â”‚
â”‚         â”‚           MATCH (s:Entity {name: "Transformer"})   â”‚   â”‚
â”‚         â”‚           MATCH (t:Entity {name: "Attention..."})  â”‚   â”‚
â”‚         â”‚           MERGE (s)-[r:USES]->(t)                  â”‚   â”‚
â”‚         â”‚           SET r.weight = 0.9                       â”‚   â”‚
â”‚         â”‚                                                     â”‚   â”‚
â”‚         â”‚         Bolt Connection: localhost:7687            â”‚   â”‚
â”‚         â”‚         Latency: ~3s for 45 chunks                 â”‚   â”‚
â”‚         â”‚                                                     â”‚   â”‚
â”‚         â”‚  C. Community Detection (Post-Indexing)            â”‚   â”‚
â”‚         â”‚     â””â”€> Run Leiden Algorithm (graspologic)        â”‚   â”‚
â”‚         â”‚         Neo4j Cypher:                              â”‚   â”‚
â”‚         â”‚           CALL gds.graph.project(...)              â”‚   â”‚
â”‚         â”‚           CALL gds.leiden.stream(...)              â”‚   â”‚
â”‚         â”‚                                                     â”‚   â”‚
â”‚         â”‚         Communities: [                             â”‚   â”‚
â”‚         â”‚           {                                        â”‚   â”‚
â”‚         â”‚             "id": "community1",                    â”‚   â”‚
â”‚         â”‚             "topic": "Transformer Architecture",   â”‚   â”‚
â”‚         â”‚             "members": [                           â”‚   â”‚
â”‚         â”‚               "Transformer",                       â”‚   â”‚
â”‚         â”‚               "Attention Mechanism",               â”‚   â”‚
â”‚         â”‚               "Multi-Head Attention"               â”‚   â”‚
â”‚         â”‚             ]                                      â”‚   â”‚
â”‚         â”‚           }                                        â”‚   â”‚
â”‚         â”‚         ]                                          â”‚   â”‚
â”‚         â”‚         Latency: ~2s                               â”‚   â”‚
â”‚         â”‚                                                     â”‚   â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                     â”‚
â”‚  5. Progress Tracking                                               â”‚
â”‚     â””â”€> WebSocket updates to UI:                                  â”‚
â”‚         {                                                          â”‚
â”‚           "status": "indexing",                                    â”‚
â”‚           "progress": {                                            â”‚
â”‚             "qdrant": "45/45 chunks",                              â”‚
â”‚             "bm25": "45/45 chunks",                                â”‚
â”‚             "lightrag": "30/45 chunks"                             â”‚
â”‚           }                                                        â”‚
â”‚         }                                                          â”‚
â”‚                                                                     â”‚
â”‚  6. Completion Response                                             â”‚
â”‚     â””â”€> HTTP 200 OK                                               â”‚
â”‚         {                                                          â”‚
â”‚           "status": "success",                                     â”‚
â”‚           "filename": "transformer_paper.pdf",                     â”‚
â”‚           "chunks_created": 45,                                    â”‚
â”‚           "entities_extracted": 87,                                â”‚
â”‚           "relationships_created": 124,                            â”‚
â”‚           "indexing_time_ms": 10000,                               â”‚
â”‚           "doc_hash": "sha256..."                                  â”‚
â”‚         }                                                          â”‚
â”‚                                                                     â”‚
â”‚  7. UI Update                                                       â”‚
â”‚     â””â”€> Gradio shows success message                              â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Total Latency: ~10s (parallel indexing)
- Qdrant: ~2s
- BM25: ~500ms (parallel)
- LightRAG: ~8s (parallel entity extraction)
- Community Detection: ~2s (runs after indexing)

Sequential would be: ~22.5s
Speedup: 2.25x
```

---

### Scenario 5: Unified Re-Indexing with BGE-M3 (Sprint 16)

**Admin Action:** Trigger full re-indexing after BGE-M3 migration

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Flow: Unified Re-Indexing (Admin Endpoint)                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  1. Admin Request                                                   â”‚
â”‚     â””â”€> POST /api/v1/admin/reindex?confirm=true                   â”‚
â”‚         Headers: Accept: text/event-stream                          â”‚
â”‚                                                                     â”‚
â”‚  2. Phase 1: Initialization (SSE Event)                             â”‚
â”‚     â””â”€> Validate parameters, load document list                    â”‚
â”‚         SSE: {                                                      â”‚
â”‚           "status": "in_progress",                                  â”‚
â”‚           "phase": "initialization",                                â”‚
â”‚           "documents_total": 933                                    â”‚
â”‚         }                                                          â”‚
â”‚                                                                     â”‚
â”‚  3. Phase 2: Atomic Deletion (SSE Event)                            â”‚
â”‚     â””â”€> Delete all indexes (all-or-nothing):                      â”‚
â”‚         A. Qdrant: DELETE collection "aegis-rag-documents"         â”‚
â”‚         B. BM25: DELETE cache "bm25_index.pkl"                     â”‚
â”‚         C. (Neo4j graph deletion pending Feature 16.6)             â”‚
â”‚         SSE: {                                                      â”‚
â”‚           "status": "in_progress",                                  â”‚
â”‚           "phase": "deletion",                                      â”‚
â”‚           "message": "Deleted Qdrant + BM25 indexes"               â”‚
â”‚         }                                                          â”‚
â”‚                                                                     â”‚
â”‚  4. Phase 3: Unified Chunking (SSE Events)                          â”‚
â”‚     â””â”€> For each document (parallel batches of 10):               â”‚
â”‚                                                                     â”‚
â”‚         ChunkingService.chunk(                                      â”‚
â”‚           text=document.text,                                       â”‚
â”‚           strategy="adaptive",  # Document-aware                   â”‚
â”‚           max_tokens=512,                                          â”‚
â”‚           overlap=128                                              â”‚
â”‚         )                                                          â”‚
â”‚           â†“                                                        â”‚
â”‚         Chunks with SHA-256 IDs:                                   â”‚
â”‚         [                                                          â”‚
â”‚           Chunk(                                                   â”‚
â”‚             chunk_id="a3f2e1d9c8b7",  # Deterministic SHA-256     â”‚
â”‚             text="Abstract: The dominant...",                      â”‚
â”‚             source="transformer_paper.pdf",                        â”‚
â”‚             position=0,                                            â”‚
â”‚             tokens=487                                             â”‚
â”‚           ),                                                       â”‚
â”‚           ...                                                      â”‚
â”‚         ]                                                          â”‚
â”‚                                                                     â”‚
â”‚         SSE: {                                                      â”‚
â”‚           "status": "in_progress",                                  â”‚
â”‚           "phase": "chunking",                                      â”‚
â”‚           "documents_processed": 450,                               â”‚
â”‚           "documents_total": 933,                                   â”‚
â”‚           "progress_percent": 48.2,                                 â”‚
â”‚           "eta_seconds": 1200,                                      â”‚
â”‚           "current_document": "transformer_paper.pdf"               â”‚
â”‚         }                                                          â”‚
â”‚                                                                     â”‚
â”‚  5. Phase 4: BGE-M3 Embedding Generation (SSE Events)               â”‚
â”‚     â””â”€> For each chunk (batch of 32):                             â”‚
â”‚                                                                     â”‚
â”‚         UnifiedEmbeddingService.embed_batch([                       â”‚
â”‚           "Abstract: The dominant...",                              â”‚
â”‚           "Introduction: Recurrent...",                             â”‚
â”‚           ...  # 32 chunks                                         â”‚
â”‚         ])                                                         â”‚
â”‚           â†“                                                        â”‚
â”‚         Ollama API Call:                                           â”‚
â”‚         POST http://localhost:11434/api/embed                      â”‚
â”‚         Body: {                                                    â”‚
â”‚           "model": "bge-m3",  # 1024-dim                           â”‚
â”‚           "inputs": [...]                                          â”‚
â”‚         }                                                          â”‚
â”‚           â†“                                                        â”‚
â”‚         Response: [                                                â”‚
â”‚           [0.123, -0.456, ..., 0.789],  # 1024-dim                 â”‚
â”‚           [0.234, -0.567, ..., 0.890],                             â”‚
â”‚           ...                                                      â”‚
â”‚         ]                                                          â”‚
â”‚                                                                     â”‚
â”‚         SSE: {                                                      â”‚
â”‚           "status": "in_progress",                                  â”‚
â”‚           "phase": "embedding",                                     â”‚
â”‚           "chunks_processed": 2800,                                 â”‚
â”‚           "chunks_total": 10000,                                    â”‚
â”‚           "progress_percent": 28.0,                                 â”‚
â”‚           "eta_seconds": 2400                                       â”‚
â”‚         }                                                          â”‚
â”‚                                                                     â”‚
â”‚  6. Phase 5: Multi-Index Insertion (SSE Events)                     â”‚
â”‚     â””â”€> Insert into all indexes (parallel):                       â”‚
â”‚                                                                     â”‚
â”‚         A. Qdrant Insertion                                         â”‚
â”‚            â””â”€> QdrantClient.upsert(                               â”‚
â”‚                  collection="aegis-rag-documents",                  â”‚
â”‚                  points=[                                          â”‚
â”‚                    {                                               â”‚
â”‚                      "id": "a3f2e1d9c8b7",  # SHA-256             â”‚
â”‚                      "vector": [0.123, ..., 0.789],  # 1024-dim    â”‚
â”‚                      "payload": {                                  â”‚
â”‚                        "text": "Abstract: The...",                 â”‚
â”‚                        "source": "transformer_paper.pdf",          â”‚
â”‚                        "chunk_id": "a3f2e1d9c8b7"                  â”‚
â”‚                      }                                             â”‚
â”‚                    },                                              â”‚
â”‚                    ...                                             â”‚
â”‚                  ]                                                 â”‚
â”‚                )                                                   â”‚
â”‚                                                                     â”‚
â”‚         B. BM25 Indexing (Automatic via Qdrant sync)                â”‚
â”‚            â””â”€> BM25 automatically synchronized                    â”‚
â”‚                No separate indexing needed                         â”‚
â”‚                                                                     â”‚
â”‚         C. LightRAG (Feature 16.6 - uses unified chunks)            â”‚
â”‚            â””â”€> Entity extraction per chunk                        â”‚
â”‚                Neo4j stores chunk_id in :MENTIONED_IN              â”‚
â”‚                                                                     â”‚
â”‚         SSE: {                                                      â”‚
â”‚           "status": "in_progress",                                  â”‚
â”‚           "phase": "indexing",                                      â”‚
â”‚           "indexes": {                                             â”‚
â”‚             "qdrant": "complete",                                  â”‚
â”‚             "bm25": "complete",                                    â”‚
â”‚             "neo4j": "pending"                                     â”‚
â”‚           }                                                        â”‚
â”‚         }                                                          â”‚
â”‚                                                                     â”‚
â”‚  7. Phase 6: Validation (SSE Event)                                 â”‚
â”‚     â””â”€> Verify index consistency:                                 â”‚
â”‚         - Qdrant point count == chunk count                        â”‚
â”‚         - BM25 document count == document count                    â”‚
â”‚         - Neo4j entity count > 0                                   â”‚
â”‚                                                                     â”‚
â”‚         SSE: {                                                      â”‚
â”‚           "status": "complete",                                     â”‚
â”‚           "phase": "validation",                                    â”‚
â”‚           "summary": {                                             â”‚
â”‚             "documents_processed": 933,                             â”‚
â”‚             "chunks_created": 10234,                                â”‚
â”‚             "qdrant_points": 10234,                                 â”‚
â”‚             "bm25_docs": 933,                                       â”‚
â”‚             "neo4j_entities": 1587,                                 â”‚
â”‚             "total_time_seconds": 8940,                             â”‚
â”‚             "embedding_model": "bge-m3",                            â”‚
â”‚             "embedding_dim": 1024                                   â”‚
â”‚           }                                                        â”‚
â”‚         }                                                          â”‚
â”‚                                                                     â”‚
â”‚  8. Admin Dashboard Update                                          â”‚
â”‚     â””â”€> Display completion summary                                â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Total Latency: ~2.5 hours (9,000 seconds)
- Deletion: ~30s (atomic)
- Chunking: ~1,500s (933 docs â†’ 10K chunks)
- Embedding: ~6,000s (10K chunks Ã— 25ms/chunk BGE-M3)
- Indexing: ~1,400s (parallel: Qdrant + BM25)
- Validation: ~10s

Key Improvements (Sprint 16):
- Unified chunks (ChunkingService) â†’ consistent provenance
- BGE-M3 embeddings (1024-dim) â†’ cross-layer similarity
- SSE progress â†’ real-time visibility
- Atomic deletion â†’ no inconsistent state
- Safety checks â†’ confirm=true required
```

---

## ðŸ”§ COMPONENT DETAILS

### FastAPI Endpoints

| Endpoint | Method | Purpose | Request | Response |
|----------|--------|---------|---------|----------|
| `/health` | GET | Health check | - | `{"status": "healthy"}` |
| `/ready` | GET | Readiness check | - | `{"ready": true}` |
| `/live` | GET | Liveness check | - | `{"alive": true}` |
| `/api/v1/chat` | POST | Chat query | `ChatRequest` | `ChatResponse` |
| `/api/v1/documents/upload` | POST | Document upload | `multipart/form-data` | `IngestionResponse` |
| `/api/v1/search` | POST | Raw search (no LLM) | `SearchRequest` | `SearchResponse` |
| `/api/v1/graph/export/json` | GET | Export graph as JSON | - | `GraphJSON` |
| `/api/v1/graph/export/graphml` | GET | Export as GraphML | - | `GraphML` |
| `/stats` | GET | System statistics | - | `SystemStats` |

### LangGraph State Schema

```python
from pydantic import BaseModel
from typing import List, Dict, Optional

class AgentState(BaseModel):
    """Centralized state for LangGraph agents."""

    # Input
    query: str
    session_id: str
    rag_mode: str  # "vector" | "graph" | "hybrid"

    # Router outputs
    query_type: Optional[str] = None  # "SIMPLE" | "COMPOUND" | "MULTI_HOP" | "MEMORY_QUERY"
    selected_agents: List[str] = []  # ["vector", "graph", "memory"]

    # Agent outputs
    vector_results: List[Dict] = []
    graph_results: List[Dict] = []
    memory_results: List[Dict] = []

    # Aggregation
    final_context: str = ""
    final_answer: str = ""
    sources: List[Dict] = []

    # Metadata
    metadata: Dict = {}
    error: Optional[str] = None
```

### Redis Data Structures

```python
# Session State (LangGraph Checkpointer)
KEY: "session:{session_id}:state"
VALUE: <pickled AgentState>
TTL: 86400 seconds (24 hours)

# Recent Messages (Short-Term Memory)
KEY: "session:{session_id}:messages"
VALUE: [
    {
        "role": "user",
        "content": "What is RAG?",
        "timestamp": "2025-10-22T10:15:00Z"
    },
    {
        "role": "assistant",
        "content": "RAG is...",
        "timestamp": "2025-10-22T10:15:02Z"
    }
]
TTL: 3600 seconds (1 hour)

# Embedding Cache (LRU)
KEY: "embedding:{sha256(text)}"
VALUE: [0.123, -0.456, ..., 0.789]  # 768d vector
TTL: 604800 seconds (7 days)
```

### Qdrant Collections

```python
# Vector Collection
collection_name = "aegis-rag-documents"
vector_size = 768  # nomic-embed-text
distance = "Cosine"

# Point Structure
{
    "id": "doc1_chunk1",
    "vector": [0.123, -0.456, ...],  # 768 dimensions
    "payload": {
        "text": "RAG is Retrieval-Augmented Generation...",
        "source": "rag_overview.md",
        "chunk_id": 1,
        "doc_hash": "sha256...",
        "metadata": {
            "document_type": "markdown",
            "date_added": "2025-10-22",
            "tags": ["rag", "llm", "retrieval"]
        }
    }
}

# Conversation History Collection
collection_name = "conversation-history"
vector_size = 768

# Point Structure
{
    "id": "conv1",
    "vector": [0.234, -0.567, ...],
    "payload": {
        "session_id": "abc123",
        "summary": "Discussed RAG definition and use cases",
        "timestamp": "2025-10-22T10:15:00Z",
        "turns": 2
    }
}
```

### Neo4j Graph Schema

```cypher
// Entity Node
(:Entity {
    name: "Transformer",
    type: "MODEL",
    source: "transformer_paper.pdf",
    first_seen: "2025-10-22T10:15:00Z",
    confidence: 0.95
})

// Relationship
(:Entity {name: "Transformer"})-[:USES {weight: 0.9}]->(:Entity {name: "Attention Mechanism"})

// Community Node
(:Community {
    id: "community1",
    topic: "Transformer Architecture",
    summary: "Models and techniques for transformer-based architectures",
    size: 15,
    created: "2025-10-22T10:15:00Z"
})

// Membership
(:Entity {name: "Transformer"})-[:BELONGS_TO]->(:Community {id: "community1"})

// Graphiti Episodic Memory (via Graphiti SDK)
(:Episode {
    session_id: "abc123",
    valid_from: "2025-10-22T10:15:00Z",
    valid_to: "9999-12-31T23:59:59Z",  # Still valid
    transaction_time: "2025-10-22T10:15:00Z"
})-[:CONTAINS]->(:Fact {
    text: "User asked about RAG definition",
    confidence: 0.9
})
```

---

## ðŸ“Š API CONTRACTS

### ChatRequest (POST /api/v1/chat)

```json
{
  "query": "What is RAG?",
  "session_id": "abc123",  // Optional, generated if missing
  "rag_mode": "hybrid",    // "vector" | "graph" | "hybrid"
  "options": {
    "top_k": 5,
    "temperature": 0.7,
    "model": "llama3.2:8b"
  }
}
```

### ChatResponse

```json
{
  "answer": "RAG (Retrieval-Augmented Generation) is a technique...",
  "sources": [
    {
      "file": "rag_overview.md",
      "chunk_id": 1,
      "score": 0.95,
      "text": "RAG is Retrieval-Augmented Generation..."
    },
    {
      "file": "llama_index.md",
      "chunk_id": 3,
      "score": 0.88,
      "text": "LlamaIndex implements RAG patterns..."
    }
  ],
  "session_id": "abc123",
  "metadata": {
    "tokens": 150,
    "latency_ms": 450,
    "rag_mode": "hybrid",
    "agents_used": ["router", "vector", "aggregator"],
    "model": "llama3.2:8b"
  }
}
```

### IngestionResponse (POST /api/v1/documents/upload)

```json
{
  "status": "success",
  "filename": "transformer_paper.pdf",
  "chunks_created": 45,
  "entities_extracted": 87,
  "relationships_created": 124,
  "indexing_time_ms": 10000,
  "doc_hash": "sha256:abc123...",
  "collections_updated": ["aegis-rag-documents", "conversation-history"]
}
```

---

## ðŸŽ¯ KEY TAKEAWAYS

### Critical Data Paths
1. **User Query â†’ LLM Response:** ~400ms (simple), ~800ms (graph), ~350ms (memory)
2. **Document Upload â†’ Indexed:** ~10s (parallel), ~22s (sequential)
3. **Embedding Generation:** ~50ms (cache miss), ~5ms (cache hit)
4. **Redis State Persistence:** <10ms
5. **Neo4j Graph Query:** ~200ms (low-level), ~500ms (high-level)

### Performance Bottlenecks
1. **Entity Extraction:** Slowest part of ingestion (~8s for 45 chunks)
2. **Community Detection:** ~2s per run
3. **LLM Generation:** ~250ms (GPU), ~3.5s (CPU)

### Optimization Strategies
1. **Parallel Indexing:** 2.25x speedup (Sprint 11)
2. **LRU Embedding Cache:** 60% hit rate, ~90% latency reduction
3. **GPU Acceleration:** 15-20x LLM speedup (Sprint 11)
4. **Batch Embedding:** Process 10 chunks at once

---

**Last Updated:** 2025-10-28 (Sprint 16)
**Status:** Active Development
**Sprint 16 Changes:** Unified chunking, BGE-M3 standardization, admin re-indexing
**Next:** Sprint 17 (TBD)
