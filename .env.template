# ==============================================================================
# AEGIS RAG - Environment Configuration Template
# ==============================================================================
# Copy this file to .env and fill in your actual values
# NEVER commit .env to version control!

# ==============================================================================
# Application Settings
# ==============================================================================
APP_NAME=aegis-rag
APP_VERSION=0.1.0
ENVIRONMENT=development  # development, staging, production
DEBUG=false
LOG_LEVEL=INFO  # DEBUG, INFO, WARNING, ERROR, CRITICAL
JSON_LOGS=false  # Set to true for production

# ==============================================================================
# API Server Configuration
# ==============================================================================
API_HOST=0.0.0.0
API_PORT=8000
API_WORKERS=1
API_RELOAD=true  # Auto-reload in development

# ==============================================================================
# LLM Configuration - Ollama (Primary)
# ==============================================================================
# Ollama Server
OLLAMA_BASE_URL=http://localhost:11434

# Ollama Models (pull these first: ollama pull <model>)
# Latest models as of 2025: llama3.2:3b and llama3.1:8b (128K context)
OLLAMA_MODEL_GENERATION=llama3.1:8b  # For answer generation (128K context, best reasoning)
OLLAMA_MODEL_QUERY=llama3.2:3b  # For query understanding (fast, multilingual)
OLLAMA_MODEL_EMBEDDING=nomic-embed-text  # For embeddings (768-dim)

# ==============================================================================
# Sprint 61: Native Embeddings & Reranking (Performance Optimization)
# ==============================================================================
# Embedding Backend Configuration (Feature 61.1)
# Options: 'sentence-transformers' (default, 5x faster) or 'ollama' (legacy)
EMBEDDING_BACKEND=sentence-transformers

# Sentence-Transformers Configuration (Native BGE-M3)
ST_MODEL_NAME=BAAI/bge-m3  # BGE-M3 embeddings (1024-dim, multilingual)
ST_DEVICE=auto  # Device: 'auto', 'cuda', or 'cpu'
ST_BATCH_SIZE=64  # Batch size for GPU processing

# Reranking Backend Configuration (Feature 61.2)
# Options: 'cross_encoder' (default, 17x faster) or 'llm' (legacy)
RERANKING_BACKEND=cross_encoder

# Cross-Encoder Configuration (BGE-reranker-v2-m3)
CE_MODEL_NAME=BAAI/bge-reranker-v2-m3  # Multilingual reranker (~560MB)
CE_DEVICE=auto  # Device: 'auto', 'cuda', or 'cpu'
CE_MAX_LENGTH=512  # Maximum token length for query-document pairs

# ==============================================================================
# Azure OpenAI (Optional - Production)
# ==============================================================================
# Set to true to use Azure OpenAI instead of Ollama
USE_AZURE_LLM=false

# Azure OpenAI Credentials (only if USE_AZURE_LLM=true)
# AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com
# AZURE_OPENAI_API_KEY=your-api-key-here
# AZURE_OPENAI_MODEL=gpt-4o
# AZURE_OPENAI_DEPLOYMENT=your-deployment-name

# ==============================================================================
# Anthropic Claude (Optional - Fallback)
# ==============================================================================
# ANTHROPIC_API_KEY=sk-ant-your-key-here

# ==============================================================================
# Sprint 23: Multi-Cloud LLM Execution (Mozilla ANY-LLM)
# ==============================================================================
# Three-tier intelligent routing strategy:
#   Tier 1: Local Ollama (FREE, 70% of tasks)
#   Tier 2: Ollama Cloud (~$0.001 per 1,000 tokens, 20% of tasks)
#   Tier 3: OpenAI API (~$0.015 per 1,000 tokens, 10% of tasks)
#
# Related ADR: ADR-033 (Mozilla ANY-LLM Integration)
# Budget Configuration: config/llm_config.yml

# Ollama Cloud (Tier 2: High-quality extraction, batch processing)
# Sign up: https://ollama.cloud
# OLLAMA_CLOUD_BASE_URL=https://ollama.cloud
# OLLAMA_CLOUD_API_KEY=your-ollama-cloud-api-key-here

# OpenAI API (Tier 3: Critical quality tasks - legal, medical)
# Sign up: https://platform.openai.com/api-keys
# Supports OpenAI-compatible APIs: OpenAI, DeepSeek, Claude, etc.
# OPENAI_API_KEY=sk-your-openai-api-key-here
# OPENAI_ORGANIZATION=  # Optional

# Budget Management (enforced by AegisLLMProxy)
# Monthly limits defined in config/llm_config.yml:
#   - Ollama Cloud: $120/month (default)
#   - OpenAI: $80/month (default)
# Automatic fallback to local when budget exceeded

# ==============================================================================
# Qdrant Vector Database
# ==============================================================================
QDRANT_HOST=localhost
QDRANT_PORT=6333
QDRANT_GRPC_PORT=6334
QDRANT_COLLECTION=documents_v1
QDRANT_USE_GRPC=false

# Optional: Qdrant Cloud API Key
# QDRANT_API_KEY=your-qdrant-api-key

# ==============================================================================
# Neo4j Graph Database
# ==============================================================================
NEO4J_URI=bolt://localhost:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=aegis-rag-neo4j-password  # CHANGE THIS!
NEO4J_DATABASE=neo4j

# ==============================================================================
# Redis Cache & Short-Term Memory
# ==============================================================================
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_DB=0
REDIS_TTL=3600  # Default TTL in seconds

# Optional: Redis Password
# REDIS_PASSWORD=your-redis-password

# ==============================================================================
# Docling Container Configuration (Sprint 21: GPU-Accelerated OCR, Sprint 30)
# ==============================================================================
# Docling provides GPU-accelerated PDF parsing with OCR (EasyOCR)
# Docker Port Mapping: Host 8080 -> Container 5001
DOCLING_BASE_URL=http://localhost:8080
DOCLING_TIMEOUT_SECONDS=900  # HTTP timeout for large PDFs (15 minutes, Sprint 30: increased for complex documents)
DOCLING_MAX_RETRIES=3  # Retry count for transient failures

# ==============================================================================
# LangSmith Observability (Optional)
# ==============================================================================
LANGSMITH_TRACING=false
# LANGSMITH_API_KEY=your-langsmith-api-key
# LANGSMITH_PROJECT=aegis-rag

# ==============================================================================
# Model Context Protocol (MCP)
# ==============================================================================
MCP_SERVER_PORT=3000
MCP_AUTH_ENABLED=false

# ==============================================================================
# Performance Configuration
# ==============================================================================
MAX_CONCURRENT_REQUESTS=50
REQUEST_TIMEOUT=60  # seconds

# ==============================================================================
# Retrieval Configuration
# ==============================================================================
RETRIEVAL_TOP_K=5
RETRIEVAL_SCORE_THRESHOLD=0.7

# ==============================================================================
# Security (Production)
# ==============================================================================
# JWT_SECRET_KEY=your-secret-key-here-use-long-random-string
# JWT_ALGORITHM=HS256
# JWT_EXPIRATION_MINUTES=60

# ==============================================================================
# CORS (Production)
# ==============================================================================
# ALLOWED_ORIGINS=https://your-frontend.com,https://your-admin.com

# ==============================================================================
# Monitoring (Production)
# ==============================================================================
# SENTRY_DSN=your-sentry-dsn
# PROMETHEUS_ENABLED=true

# ==============================================================================
# Notes
# ==============================================================================
# 1. For local development with Ollama (default):
#    - Ensure Ollama is running: ollama serve
#    - Pull required models:
#      ollama pull llama3.2:3b
#      ollama pull llama3.2:8b
#      ollama pull nomic-embed-text
#
# 2. For production with Azure OpenAI:
#    - Set USE_AZURE_LLM=true
#    - Configure AZURE_OPENAI_* variables
#
# 3. Sprint 23: Multi-Cloud LLM Setup (Optional)
#    - Local Ollama (Tier 1) works out of the box - no API keys needed
#    - Ollama Cloud (Tier 2, optional):
#      * Sign up at https://ollama.cloud
#      * Get API key from dashboard
#      * Set OLLAMA_CLOUD_API_KEY in .env
#    - OpenAI API (Tier 3, optional):
#      * Sign up at https://platform.openai.com
#      * Create API key
#      * Set OPENAI_API_KEY in .env
#    - Routing Strategy (automatic):
#      * PII/HIPAA data: Always local (GDPR/HIPAA compliance)
#      * Embeddings: Always local (BGE-M3 excellent locally)
#      * Critical quality + high complexity: OpenAI (if budget available)
#      * High quality + high complexity: Ollama Cloud (if budget available)
#      * Batch processing (>10 docs): Ollama Cloud (if budget available)
#      * Default: Local Ollama (70% of tasks)
#    - Budget configuration in config/llm_config.yml
#
# 4. Start services with: docker compose up -d
#    - Qdrant UI: http://localhost:6333/dashboard
#    - Neo4j Browser: http://localhost:7474
#    - Grafana: http://localhost:3000
#    - Prometheus: http://localhost:9090
#
# 5. Security Best Practices:
#    - Change all default passwords!
#    - Use strong random values for secrets
#    - Never commit .env to git
#    - Use secret managers in production (Vault, AWS Secrets Manager)
