services:
  # ============================================================================
  # Ollama - Local LLM Server (Primary)
  # ============================================================================
  ollama:
    image: ollama/ollama:latest
    container_name: aegis-ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
      # Sprint 20 Feature 20.4: Keep models in VRAM for 30 minutes (prevents 76s reloads)
      - OLLAMA_KEEP_ALIVE=30m
      # Sprint 21 Feature 21.6: Only load 1 model at a time (sequential pipeline)
      # This ensures full memory available for qwen3-vl (3.0 GiB) and gemma-3-4b-it (2.5 GiB)
      - OLLAMA_MAX_LOADED_MODELS=1
    # GPU Support for NVIDIA RTX 3060 (Sprint 11: 15-20x speedup vs CPU)
    # Required for LightRAG entity extraction with llama3.2:3b (otherwise 360s timeout)
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
        limits:
          memory: 8G  # System RAM limit (allows 3.0GB + overhead for model loading)
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    restart: unless-stopped
    networks:
      - aegis-network

  # ============================================================================
  # Docling - GPU-Accelerated Document Parsing (Sprint 21)
  # ============================================================================
  # IMPORTANT: This service uses the "ingestion" profile to avoid always-on GPU usage
  # Start with: docker compose --profile ingestion up -d docling
  # Stop with:  docker compose stop docling
  #
  # Memory Management:
  # - RTX 3060: 6GB VRAM (80% = 4.8GB allocated to Docling)
  # - System RAM: ~4.4GB available (container lifecycle managed by LangGraph)
  # - Sequential execution: Start → Parse batch → Stop → Free VRAM for next stage
  #
  # Known Issues:
  # - GPU memory leak in batch processing (monitor with nvidia-smi)
  # - Workaround: Restart container after 10-20 docs (handled by BatchOrchestrator)
  # ============================================================================
  docling:
    # Official Docling-Serve with CUDA 12.4 support
    image: quay.io/docling-project/docling-serve-cu124:latest
    container_name: aegis-docling
    profiles:
      - ingestion  # Only start when explicitly requested (not part of default stack)
    ports:
      - "8080:5001"  # HTTP API for parsing (container uses 5001 internally)
    volumes:
      # Cache volumes to persist downloaded models (OCR, layout, table detection)
      - docling_hf_cache:/root/.cache/huggingface
      - docling_ocr_cache:/root/.EasyOCR
      # Optional: Mount document directory for direct access
      # - ./data/documents:/input:ro
    environment:
      # Port Configuration (docling-serve defaults to 5001)
      - PORT=5001  # Internal port (mapped to 8080 externally)

      # CUDA Configuration
      - CUDA_VISIBLE_DEVICES=0  # Use first GPU (RTX 3060)
      - DOCLING_GPU_MEMORY_FRACTION=0.8  # Allocate 80% of VRAM (4.8GB)

      # Model Configuration
      - DOCLING_LAZY_LOADING=true  # Load models on-demand to reduce startup time
      - DOCLING_OCR_ENGINE=easyocr  # EasyOCR with GPU acceleration
      - DOCLING_TABLE_STRUCTURE=true  # Enable table structure preservation
      - DOCLING_LAYOUT_ANALYSIS=true  # Enable layout detection (headings, lists)

      # Feature 21.6: Image Extraction for VLM Processing
      - DOCLING_GENERATE_PICTURE_IMAGES=true  # Extract images for Qwen3-VL
      - DOCLING_IMAGES_SCALE=2.0  # Image scaling factor (2x for better VLM quality)

      # Performance Tuning
      - NUM_WORKERS=2  # Parallel workers (limited by VRAM, 2 = ~2.4GB each)
      - DOCLING_BATCH_SIZE=1  # Process 1 doc at a time (prevents memory leak)
      - DOCLING_TIMEOUT=300  # 5 min timeout for large PDFs with OCR

      # Logging
      - LOG_LEVEL=INFO

    # GPU Resource Allocation
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1  # Use 1 GPU (RTX 3060)
              capabilities: [gpu]
        limits:
          memory: 3G  # System RAM limit (leaves 1.4GB for other containers)

    # Health Check
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5001/health"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 60s  # First model load takes ~30-45s

    # Restart policy: Keep container running (Sprint 33 - user request)
    restart: unless-stopped
    networks:
      - aegis-network

  # ============================================================================
  # Qdrant - Vector Database
  # ============================================================================
  qdrant:
    image: qdrant/qdrant:v1.11.0
    container_name: aegis-qdrant
    ports:
      - "6333:6333"  # HTTP API
      - "6334:6334"  # gRPC
    volumes:
      - qdrant_data:/qdrant/storage
    environment:
      - QDRANT__SERVICE__GRPC_PORT=6334
      - QDRANT__LOG_LEVEL=INFO
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:6333/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    restart: unless-stopped
    networks:
      - aegis-network

  # ============================================================================
  # Neo4j - Graph Database
  # ============================================================================
  neo4j:
    image: neo4j:5.24-community
    container_name: aegis-neo4j
    ports:
      - "7474:7474"  # HTTP
      - "7687:7687"  # Bolt
    volumes:
      - neo4j_data:/data
      - neo4j_logs:/logs
      - neo4j_import:/var/lib/neo4j/import
      - neo4j_plugins:/plugins
    environment:
      - NEO4J_AUTH=neo4j/aegis-rag-neo4j-password
      - NEO4J_PLUGINS=["apoc", "graph-data-science"]
      - NEO4J_dbms_memory_pagecache_size=1G
      - NEO4J_dbms_memory_heap_initial__size=1G
      - NEO4J_dbms_memory_heap_max__size=2G
      - NEO4J_dbms_security_procedures_unrestricted=apoc.*,gds.*
      - NEO4J_dbms_security_procedures_allowlist=apoc.*,gds.*
      - NEO4J_apoc_export_file_enabled=true
      - NEO4J_apoc_import_file_enabled=true
      - NEO4J_apoc_import_file_use__neo4j__config=true
    healthcheck:
      test: ["CMD", "cypher-shell", "-u", "neo4j", "-p", "aegis-rag-neo4j-password", "RETURN 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    restart: unless-stopped
    networks:
      - aegis-network

  # ============================================================================
  # Redis - Cache & Short-Term Memory
  # ============================================================================
  redis:
    image: redis:7-alpine
    container_name: aegis-redis
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    command: >
      redis-server
      --appendonly yes
      --appendfsync everysec
      --maxmemory 2gb
      --maxmemory-policy allkeys-lru
      --save 60 1000
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    restart: unless-stopped
    networks:
      - aegis-network

  # ============================================================================
  # Prometheus - Metrics Collection
  # ============================================================================
  prometheus:
    image: prom/prometheus:latest
    container_name: aegis-prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./config/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
      - '--web.enable-lifecycle'
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:9090/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    restart: unless-stopped
    networks:
      - aegis-network
    depends_on:
      - qdrant
      - neo4j
      - redis

  # ============================================================================
  # Grafana - Metrics Visualization
  # ============================================================================
  grafana:
    image: grafana/grafana:latest
    container_name: aegis-grafana
    ports:
      - "3000:3000"
    volumes:
      - grafana_data:/var/lib/grafana
      - ./config/grafana-datasources.yml:/etc/grafana/provisioning/datasources/datasources.yml:ro
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=aegis-rag-grafana
      - GF_INSTALL_PLUGINS=redis-datasource
      - GF_SERVER_ROOT_URL=http://localhost:3000
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    restart: unless-stopped
    networks:
      - aegis-network
    depends_on:
      - prometheus

# ============================================================================
# Volumes
# ============================================================================
volumes:
  ollama_data:
    driver: local
  qdrant_data:
    driver: local
  neo4j_data:
    driver: local
  neo4j_logs:
    driver: local
  neo4j_import:
    driver: local
  neo4j_plugins:
    driver: local
  redis_data:
    driver: local
  prometheus_data:
    driver: local
  grafana_data:
    driver: local
  # Sprint 21: Docling model caches (OCR, layout, table detection)
  docling_hf_cache:
    driver: local
  docling_ocr_cache:
    driver: local

# ============================================================================
# Networks
# ============================================================================
networks:
  aegis-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.25.0.0/16
