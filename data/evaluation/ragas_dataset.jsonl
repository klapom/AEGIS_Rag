{"question": "What is RAG and how does it work?", "ground_truth": "RAG (Retrieval-Augmented Generation) is a technique that combines information retrieval with text generation. It retrieves relevant documents from a knowledge base and uses them to augment the context provided to a language model for generating more accurate and grounded responses.", "contexts": ["Retrieval-Augmented Generation (RAG) is an AI framework that combines the power of large language models with external knowledge retrieval. The system first searches a vector database for relevant documents, then uses those documents as context for generating responses.", "RAG works by embedding queries and documents into a vector space, performing similarity search to find relevant documents, and then passing those documents along with the query to an LLM for generation."], "metadata": {"difficulty": "easy", "category": "rag_basics", "expected_retrieval": "vector"}}
{"question": "How does vector search differ from traditional keyword search?", "ground_truth": "Vector search uses semantic embeddings to find documents based on meaning and context, while traditional keyword search matches exact terms. Vector search can find semantically similar documents even when different words are used, whereas keyword search requires exact or fuzzy matches of the query terms.", "contexts": ["Vector search represents text as dense numerical vectors (embeddings) that capture semantic meaning. When searching, the query is also embedded and documents are ranked by vector similarity (e.g., cosine similarity).", "Traditional keyword search like BM25 relies on term frequency and inverse document frequency to rank documents. It matches exact words but doesn't understand semantic relationships between different terms."], "metadata": {"difficulty": "medium", "category": "search_methods", "expected_retrieval": "vector"}}
{"question": "What is BM25 and when should it be used?", "ground_truth": "BM25 (Best Matching 25) is a probabilistic keyword-based ranking function widely used in information retrieval. It should be used when exact term matches are important, for queries with specific technical terms or proper nouns, or when combined with vector search in hybrid retrieval systems.", "contexts": ["BM25 is a ranking function based on term frequency (TF) and inverse document frequency (IDF). It calculates a relevance score based on how often query terms appear in documents and how rare those terms are across the corpus.", "BM25 works best for queries with specific keywords and technical terminology. It excels at exact matching but doesn't capture semantic similarity like vector search does."], "metadata": {"difficulty": "medium", "category": "search_methods", "expected_retrieval": "bm25"}}
{"question": "What is hybrid search and what are its advantages?", "ground_truth": "Hybrid search combines vector search (semantic similarity) with keyword search (BM25) to leverage the strengths of both approaches. Its advantages include better coverage (finds documents through multiple methods), improved precision (filters false positives), and robustness across different query types.", "contexts": ["Hybrid search uses both vector embeddings for semantic matching and BM25 for keyword matching. Results are typically combined using Reciprocal Rank Fusion (RRF) which merges rankings from both methods.", "The main advantage of hybrid search is that it captures both semantic similarity and exact keyword matches. This makes it more robust than using either method alone, especially for diverse query types."], "metadata": {"difficulty": "medium", "category": "hybrid_search", "expected_retrieval": "hybrid"}}
{"question": "What is Reciprocal Rank Fusion (RRF)?", "ground_truth": "Reciprocal Rank Fusion (RRF) is a method for combining rankings from multiple retrieval systems. It calculates a fused score for each document based on its rank in each ranking list using the formula: score = sum(1/(k + rank_i)) where k is a constant (typically 60) and rank_i is the document's rank in list i.", "contexts": ["RRF (Reciprocal Rank Fusion) is an unsupervised rank fusion method that combines multiple ranked lists. For each document, it sums 1/(k+r) across all rankings where r is the document's rank.", "RRF is particularly effective because it doesn't require score normalization and gives higher weight to documents that appear near the top of multiple rankings. The k parameter (usually 60) controls how quickly the contribution decreases with rank."], "metadata": {"difficulty": "hard", "category": "hybrid_search", "expected_retrieval": "hybrid"}}
{"question": "What is cross-encoder reranking?", "ground_truth": "Cross-encoder reranking is a technique that uses a BERT-based model to score query-document pairs by jointly encoding them. Unlike bi-encoders (used for vector search), cross-encoders process the query and document together, producing more accurate relevance scores but at higher computational cost. It's typically applied as a second stage after initial retrieval.", "contexts": ["Cross-encoders are transformer models that take both query and document as input and output a relevance score. They're more accurate than vector similarity but slower because they process each pair individually.", "Reranking with cross-encoders is typically done on a small set of top candidates (e.g., top 20) retrieved by faster methods like vector or hybrid search. Models like ms-marco-MiniLM are commonly used."], "metadata": {"difficulty": "hard", "category": "reranking", "expected_retrieval": "hybrid"}}
{"question": "What is Qdrant and why use it?", "ground_truth": "Qdrant is an open-source vector database optimized for similarity search and vector storage. It's used for efficiently storing document embeddings and performing fast nearest-neighbor searches. Qdrant supports filtering, payloads, and high-throughput operations, making it ideal for production RAG systems.", "contexts": ["Qdrant is a vector database written in Rust that provides fast similarity search using HNSW (Hierarchical Navigable Small World) graphs. It supports filtering, has low memory footprint, and can handle billions of vectors.", "Qdrant offers both gRPC and REST APIs, supports multiple distance metrics (cosine, dot product, Euclidean), and provides features like payload indexing and quantization for production deployments."], "metadata": {"difficulty": "medium", "category": "vector_db", "expected_retrieval": "vector"}}
{"question": "What embedding model should I use for RAG?", "ground_truth": "For local/open-source RAG, nomic-embed-text is recommended (Ollama-compatible, 768 dimensions, good performance). For cloud-based systems, OpenAI's text-embedding-3-large (3072 dimensions) or text-embedding-3-small (1536 dimensions) are excellent choices. Consider model size, dimensionality, performance benchmarks, and deployment constraints.", "contexts": ["Popular embedding models include: nomic-embed-text (open-source, 768d, good for local deployment), OpenAI text-embedding-3-large (3072d, high performance), and sentence-transformers models like all-MiniLM-L6-v2.", "Model selection depends on: deployment environment (local vs cloud), dimensionality (impacts storage and speed), performance on your domain, and cost. Benchmark on your specific data before choosing."], "metadata": {"difficulty": "medium", "category": "embeddings", "expected_retrieval": "vector"}}
{"question": "How do I chunk documents for RAG?", "ground_truth": "Document chunking involves splitting long documents into smaller segments for embedding. Best practices include: chunk size of 512-1024 tokens, overlap of 50-100 tokens between chunks, respecting document structure (paragraphs, sections), and adapting chunk size based on content type (code, PDFs, markdown).", "contexts": ["Effective chunking strategies: use semantic boundaries (paragraphs, sections), maintain context with overlap, keep chunk size under model context limits, and preserve metadata (source, page numbers).", "Chunk size tradeoffs: smaller chunks (256-512 tokens) provide precise retrieval but may lack context; larger chunks (1024-2048 tokens) maintain context but may include irrelevant information."], "metadata": {"difficulty": "medium", "category": "document_processing", "expected_retrieval": "vector"}}
{"question": "What is query decomposition in RAG?", "ground_truth": "Query decomposition is a technique that breaks complex queries into simpler sub-queries. Each sub-query is processed independently, and results are aggregated. This improves retrieval for multi-faceted questions and allows parallel processing of different query aspects.", "contexts": ["Query decomposition uses an LLM to split complex queries like 'Compare X and Y' into separate queries: 'What is X?' and 'What is Y?'. Each sub-query retrieves relevant documents independently.", "The benefits of query decomposition include: better coverage for complex questions, reduced false negatives, ability to handle multi-hop reasoning, and more focused retrieval per sub-query."], "metadata": {"difficulty": "hard", "category": "advanced_retrieval", "expected_retrieval": "hybrid"}}
{"question": "Explain the difference between bi-encoders and cross-encoders", "ground_truth": "Bi-encoders encode queries and documents separately into fixed-size vectors, enabling fast similarity search via vector databases. Cross-encoders jointly encode query-document pairs for more accurate relevance scoring but require processing each pair individually, making them slower. Bi-encoders are used for initial retrieval, cross-encoders for reranking.", "contexts": ["Bi-encoders (dual encoders) produce independent embeddings for queries and documents. This allows pre-computing document embeddings and fast similarity search using vector operations.", "Cross-encoders process query and document together through attention mechanisms, capturing interaction between terms. This provides better relevance scores but can't pre-compute document representations."], "metadata": {"difficulty": "hard", "category": "embeddings", "expected_retrieval": "hybrid"}}
{"question": "What is HNSW and why is it used in vector search?", "ground_truth": "HNSW (Hierarchical Navigable Small World) is a graph-based algorithm for approximate nearest neighbor search. It organizes vectors in a hierarchical graph structure, enabling fast similarity search with high recall. It's used in vector databases like Qdrant because it provides excellent speed-accuracy tradeoffs for high-dimensional spaces.", "contexts": ["HNSW builds a multi-layer graph where each layer contains increasingly sparse subsets of vectors. Search starts at the top layer and navigates down, using greedy search at each level to find nearest neighbors.", "HNSW provides sub-linear search complexity and high recall (>95%) while being memory-efficient. It's the most popular ANN (approximate nearest neighbor) algorithm used in production vector databases."], "metadata": {"difficulty": "hard", "category": "vector_db", "expected_retrieval": "vector"}}
{"question": "What is the purpose of metadata filtering in RAG?", "ground_truth": "Metadata filtering allows constraining retrieval to specific document subsets based on attributes like source, date, author, or document type. This improves relevance by focusing search on appropriate documents, reduces noise, and enables user-controlled filtering. It's implemented as pre-filtering or post-filtering in vector databases.", "contexts": ["Metadata filtering in RAG systems lets users specify conditions like 'only search PDF documents from 2024' or 'exclude draft documents'. This is implemented using payload filters in vector databases.", "Benefits of metadata filtering: improved precision, faster search (smaller search space), user control over sources, and compliance (e.g., restricting to authorized documents)."], "metadata": {"difficulty": "medium", "category": "advanced_retrieval", "expected_retrieval": "hybrid"}}
{"question": "How does temperature affect LLM generation in RAG?", "ground_truth": "Temperature controls randomness in LLM text generation. Lower values (0.0-0.3) make output more deterministic and focused, ideal for factual RAG responses. Higher values (0.7-1.0) increase creativity and diversity but may reduce factual accuracy. For RAG systems, temperatures of 0.0-0.3 are typically recommended.", "contexts": ["Temperature is a sampling parameter that affects token selection probability. At temperature 0, the model always selects the highest probability token (greedy decoding). Higher temperatures flatten the probability distribution.", "In RAG applications, low temperature (0.0-0.2) ensures the model stays grounded in retrieved contexts and produces consistent, factual answers. High temperature may cause hallucination or deviation from source material."], "metadata": {"difficulty": "medium", "category": "llm_generation", "expected_retrieval": "vector"}}
{"question": "What is the context window and why does it matter for RAG?", "ground_truth": "The context window is the maximum number of tokens an LLM can process in a single input. For RAG, it limits how many retrieved documents can be included in the prompt. Larger context windows (32K-128K tokens) allow more documents and longer contexts but increase latency and cost. Must balance retrieval quantity with context window constraints.", "contexts": ["Context window sizes vary by model: GPT-3.5 (4K-16K), GPT-4 (8K-128K), Claude (200K), Llama models (4K-128K depending on version). Larger windows enable including more retrieved documents.", "RAG systems must consider: number of documents to retrieve, average document length, prompt template size, and desired response length when managing context windows. Exceeding limits causes truncation or errors."], "metadata": {"difficulty": "medium", "category": "llm_generation", "expected_retrieval": "vector"}}
{"question": "What evaluation metrics should I use for RAG systems?", "ground_truth": "Key RAG evaluation metrics include: Context Precision (relevance of retrieved contexts), Context Recall (coverage of necessary information), Faithfulness (answer grounding in contexts), Answer Relevance (answer quality), and retrieval metrics like MRR, NDCG, Precision@K, and Recall@K. RAGAS framework provides automated evaluation using these metrics.", "contexts": ["RAGAS (RAG Assessment) metrics: Context Precision measures how many retrieved contexts are relevant, Context Recall checks if all ground truth information was retrieved, Faithfulness evaluates if answers are grounded in contexts.", "Traditional IR metrics for RAG: Mean Reciprocal Rank (MRR), Normalized Discounted Cumulative Gain (NDCG), Precision@K, Recall@K. These evaluate retrieval quality independent of generation."], "metadata": {"difficulty": "hard", "category": "evaluation", "expected_retrieval": "hybrid"}}
{"question": "What is prompt engineering in the context of RAG?", "ground_truth": "Prompt engineering for RAG involves designing effective prompts that combine retrieved contexts with user queries to guide LLM generation. Key aspects include: context formatting (how documents are presented), instruction clarity (what to do with contexts), answer constraints (style, length, citation requirements), and handling cases when contexts are insufficient.", "contexts": ["Effective RAG prompts structure: system message (role definition), retrieved contexts (formatted clearly with sources), user query, and generation instructions (e.g., 'Answer based only on provided contexts').", "Prompt engineering techniques for RAG: few-shot examples, explicit citation requirements, handling 'I don't know' cases, output formatting (JSON, markdown), and context prioritization (most relevant first)."], "metadata": {"difficulty": "medium", "category": "prompt_engineering", "expected_retrieval": "vector"}}
{"question": "How do I handle hallucinations in RAG systems?", "ground_truth": "Reduce RAG hallucinations by: using low temperature (0.0-0.2), explicitly instructing the model to only use provided contexts, implementing faithfulness scoring to detect unsupported claims, using retrieval confidence thresholds to ensure quality contexts, and enabling the model to say 'I don't know' when information is insufficient.", "contexts": ["Hallucination mitigation strategies: prompt the model to cite sources, use chain-of-thought reasoning to trace claims to contexts, implement post-generation verification, and set relevance thresholds for retrieved documents.", "System-level approaches: confidence scoring for retrieved contexts, multiple retrieval attempts with query reformulation, hybrid search for better context quality, and reranking to prioritize most relevant documents."], "metadata": {"difficulty": "hard", "category": "rag_challenges", "expected_retrieval": "hybrid"}}
{"question": "What is the difference between dense and sparse retrieval?", "ground_truth": "Dense retrieval uses learned vector embeddings (typically from neural networks) to represent queries and documents in a continuous vector space. Sparse retrieval uses discrete representations like TF-IDF or BM25 with most dimensions being zero. Dense retrieval captures semantic similarity, sparse retrieval captures lexical overlap. Hybrid systems combine both.", "contexts": ["Dense retrieval: neural models (BERT, sentence transformers) encode text into fixed-size dense vectors. Similarity is measured via cosine or dot product. Captures semantic relationships and synonyms.", "Sparse retrieval: traditional IR methods like TF-IDF and BM25 represent documents as sparse vectors over vocabulary. Each dimension represents a term's importance. Fast and interpretable but misses semantic similarity."], "metadata": {"difficulty": "medium", "category": "search_methods", "expected_retrieval": "hybrid"}}
{"question": "What are the main components of a production RAG system?", "ground_truth": "A production RAG system consists of: 1) Document ingestion pipeline (chunking, embedding, storage), 2) Vector database (e.g., Qdrant, Pinecone), 3) Retrieval engine (vector, keyword, or hybrid search), 4) Reranking (optional cross-encoder), 5) LLM for generation, 6) Monitoring and evaluation, 7) API layer, and 8) Caching for performance.", "contexts": ["RAG system architecture: Ingestion pipeline loads and processes documents, embedding service converts text to vectors, vector DB stores embeddings, retrieval engine finds relevant documents, and LLM generates responses using retrieved context.", "Production considerations: implement caching (Redis) for common queries, monitor retrieval quality metrics, add authentication and rate limiting, use async processing for ingestion, implement graceful fallbacks, and enable observability (logging, tracing)."], "metadata": {"difficulty": "hard", "category": "rag_basics", "expected_retrieval": "hybrid"}}
{"question": "How do I optimize RAG system performance?", "ground_truth": "RAG performance optimization strategies: 1) Use hybrid search for better retrieval, 2) Implement reranking for top-k candidates, 3) Cache frequent queries and embeddings, 4) Optimize chunk size and overlap, 5) Use quantization for embeddings, 6) Implement parallel processing for multi-query scenarios, 7) Monitor and tune retrieval parameters (top_k, thresholds), and 8) Use appropriate hardware (GPUs for embeddings).", "contexts": ["Performance bottlenecks in RAG: embedding generation (use batch processing), vector search (tune HNSW parameters), LLM generation (use streaming), and data loading (implement efficient chunking and caching).", "Optimization techniques: pre-compute and cache document embeddings, use approximate nearest neighbor algorithms (HNSW, IVF), implement result caching with TTL, enable parallel retrieval for multiple sub-queries, and use quantization (product quantization) for large collections."], "metadata": {"difficulty": "hard", "category": "rag_optimization", "expected_retrieval": "hybrid"}}
{"question": "What is semantic caching in RAG?", "ground_truth": "Semantic caching stores query results based on semantic similarity rather than exact matches. When a new query arrives, the system checks if a semantically similar cached query exists (using embedding similarity). If found and similarity exceeds a threshold, the cached result is returned, saving retrieval and generation costs.", "contexts": ["Semantic caching implementation: embed incoming query, search cache for similar queries (using vector similarity), if match found above threshold (e.g., 0.95), return cached response. Cache entries have TTL (time-to-live).", "Benefits of semantic caching: reduced latency (no retrieval/generation needed), lower costs (fewer LLM calls), handling paraphrased queries, and improved user experience. Works well for common questions with slight variations."], "metadata": {"difficulty": "hard", "category": "rag_optimization", "expected_retrieval": "vector"}}
{"question": "What is the role of LLMs in RAG beyond generation?", "ground_truth": "LLMs in RAG are used for: 1) Query understanding and reformulation, 2) Query decomposition (splitting complex queries), 3) Context summarization (condensing long documents), 4) Metadata extraction from documents, 5) Evaluation (judging relevance, faithfulness), 6) Response generation, and 7) Self-correction and verification.", "contexts": ["LLMs as query processors: reformulate vague queries for better retrieval, expand queries with synonyms, decompose complex multi-part questions, and classify query intent to select appropriate retrieval strategies.", "LLMs in RAG evaluation: judge retrieved document relevance, assess answer faithfulness to contexts, score context coverage, and verify factual claims. Tools like RAGAS use LLMs for automated evaluation."], "metadata": {"difficulty": "hard", "category": "advanced_retrieval", "expected_retrieval": "hybrid"}}
