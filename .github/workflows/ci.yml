name: CI Pipeline - Quality Gates

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main, develop]

env:
  PYTHON_VERSION: "3.11"

jobs:
  # ============================================================
  # JOB 1: CODE QUALITY (Linting, Formatting, Type Checking)
  # ============================================================
  code-quality:
    name: 🔍 Code Quality
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
      
      - name: Install Poetry
        run: |
          pip install --upgrade pip
          pip install poetry

      - name: Install Dependencies
        run: |
          poetry install --with dev
      
      - name: Run Ruff Linter
        run: |
          echo "Running Ruff Linter..."
          poetry run ruff check src/ --output-format=github

      - name: Run Black Formatter Check
        run: |
          echo "Checking Black formatting..."
          poetry run black --check --diff src/

      - name: Run MyPy Type Checker
        run: |
          echo "Running MyPy type checking..."
          poetry run mypy src/ --config-file=pyproject.toml
        continue-on-error: true  # Don't fail build, but show warnings

      - name: Run Bandit Security Scanner
        run: |
          echo "Running Bandit security scan..."
          poetry run bandit -r src/ -ll -f json -o bandit-report.json
          poetry run bandit -r src/ -ll
      
      - name: Upload Bandit Report
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: bandit-report
          path: bandit-report.json

  # ============================================================
  # JOB 2: NAMING CONVENTIONS CHECK
  # ============================================================
  naming-conventions:
    name: 📝 Naming Conventions
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Check Naming Conventions
        run: |
          echo "Checking naming conventions..."
          python scripts/check_naming.py src/**/*.py

  # ============================================================
  # JOB 3: ADR VALIDATION
  # ============================================================
  adr-validation:
    name: 📚 ADR Validation
    runs-on: ubuntu-latest
    if: github.event_name == 'pull_request'
    
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Full history for diff
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Check if ADR needed
        run: |
          echo "Checking if ADR is required..."
          python scripts/check_adr.py
        continue-on-error: true  # Warning only, not blocking

  # ============================================================
  # JOB 4: UNIT TESTS
  # ============================================================
  unit-tests:
    name: 🧪 Unit Tests
    runs-on: ubuntu-latest

    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install Poetry
        run: |
          curl -sSL https://install.python-poetry.org | python3 -
          echo "$HOME/.local/bin" >> $GITHUB_PATH

      - name: Install Dependencies
        run: |
          poetry install --no-interaction --no-ansi

      - name: Run Unit Tests
        run: |
          echo "Running unit tests..."
          poetry run pytest tests/unit/ tests/components/ tests/api/ \
            --cov=src \
            --cov-report=xml \
            --cov-report=html \
            --cov-report=term-missing \
            --cov-fail-under=80 \
            --junitxml=test-results/unit-results.xml \
            --timeout=300 \
            --timeout-method=thread \
            -v

      - name: Upload Coverage Report
        uses: codecov/codecov-action@v4
        with:
          file: ./coverage.xml
          flags: unittests
          name: codecov-unit
          fail_ci_if_error: true  # Sprint 13: Fail if coverage upload fails

      - name: Upload Test Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: unit-test-results
          path: |
            test-results/
            coverage.xml
            htmlcov/

  # ============================================================
  # JOB 5: INTEGRATION TESTS (with Docker Services)
  # ============================================================
  integration-tests:
    name: 🔗 Integration Tests
    runs-on: ubuntu-latest
    
    services:
      qdrant:
        image: qdrant/qdrant:v1.11.0
        ports:
          - 6333:6333
        options: >-
          --health-cmd "timeout 1 bash -c ':> /dev/tcp/127.0.0.1/6333' || exit 1"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 10
      
      neo4j:
        image: neo4j:5.24-community
        ports:
          - 7687:7687
          - 7474:7474
        env:
          NEO4J_AUTH: neo4j/testpassword
          NEO4J_server_memory_heap_initial__size: 512m
          NEO4J_server_memory_heap_max__size: 1g
        options: >-
          --health-cmd "wget --no-verbose --tries=1 --spider http://localhost:7474 || exit 1"
          --health-interval 10s
          --health-timeout 10s
          --health-retries 20
          --health-start-period 60s
      
      redis:
        image: redis:7-alpine
        ports:
          - 6379:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

      # Sprint 12: Add Ollama service for LLM integration tests
      ollama:
        image: ollama/ollama:latest
        ports:
          - 11434:11434
        options: >-
          --health-cmd "curl -f http://localhost:11434/api/version || exit 1"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 10

    steps:
      - name: Checkout Code
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install Poetry
        run: |
          curl -sSL https://install.python-poetry.org | python3 -
          echo "$HOME/.local/bin" >> $GITHUB_PATH

      - name: Install Dependencies
        run: |
          poetry install --no-interaction --no-ansi
      
      - name: Wait for Services
        run: |
          echo "Waiting for Qdrant..."
          timeout 60 bash -c 'until curl -f http://localhost:6333/ 2>/dev/null; do sleep 2; done'
          echo "✅ Qdrant ready"

          echo "Waiting for Neo4j HTTP endpoint..."
          timeout 120 bash -c 'until curl -f http://localhost:7474/ 2>/dev/null; do echo "Attempt $((++count))/60: Neo4j not ready..."; sleep 2; done'
          echo "✅ Neo4j HTTP ready"

          echo "Waiting for Neo4j Bolt to be fully ready..."
          sleep 15
          timeout 60 bash -c 'until echo "RETURN 1 AS result" | cypher-shell -u neo4j -p testpassword -a bolt://localhost:7687 --format plain 2>/dev/null | grep -q "1"; do echo "Bolt not ready, waiting..."; sleep 3; done'
          echo "✅ Neo4j Bolt ready"

          echo "Waiting for Redis..."
          timeout 60 bash -c 'until redis-cli ping 2>/dev/null; do sleep 2; done'
          echo "✅ Redis ready"

          # Sprint 12: Wait for Ollama to be ready
          echo "Waiting for Ollama..."
          timeout 120 bash -c 'until curl -f http://localhost:11434/api/version 2>/dev/null; do sleep 2; done'
          echo "✅ Ollama ready"

          # Sprint 12: Pull required models
          echo "Pulling Ollama models..."
          docker exec ollama ollama pull nomic-embed-text
          docker exec ollama ollama pull llama3.2:3b
          echo "✅ Models ready"

      - name: Run Integration Tests
        timeout-minutes: 20  # Increased from default 10 to 20
        env:
          QDRANT_HOST: localhost
          QDRANT_PORT: 6333
          NEO4J_URI: bolt://localhost:7687
          NEO4J_USER: neo4j
          NEO4J_PASSWORD: testpassword
          REDIS_HOST: localhost
          REDIS_PORT: 6379
          OLLAMA_BASE_URL: http://localhost:11434  # Add Ollama config
        run: |
          echo "Running integration tests..."
          poetry run pytest tests/integration/ \
            --cov=src \
            --cov-report=xml \
            --cov-report=html \
            --cov-append \
            --timeout=300 \
            --timeout-method=thread \
            --junitxml=test-results/integration-results.xml \
            -v
      
      - name: Upload Coverage Report
        uses: codecov/codecov-action@v4
        with:
          file: ./coverage.xml
          flags: integration
          name: codecov-integration
          fail_ci_if_error: true  # Sprint 13: Fail if coverage upload fails

      - name: Upload Test Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: integration-test-results
          path: |
            test-results/
            coverage.xml
            htmlcov/

  # ============================================================
  # JOB 6: SECURITY SCANNING (Dependencies)
  # ============================================================
  security-scan:
    name: 🔒 Security Scan
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Install Safety
        run: pip install safety
      
      - name: Run Safety Check
        run: |
          echo "Checking dependencies for vulnerabilities..."
          safety check --json --output safety-report.json || true
          safety check
        continue-on-error: true
      
      - name: Upload Safety Report
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: safety-report
          path: safety-report.json

  # ============================================================
  # JOB 7: DOCKER BUILD TEST
  # ============================================================
  docker-build:
    name: 🐳 Docker Build
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4
      
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
      
      - name: Build Docker Image
        uses: docker/build-push-action@v5
        with:
          context: .
          file: ./docker/Dockerfile.api
          push: false
          load: true
          tags: aegis-rag-api:test
          cache-from: type=gha,scope=aegis-rag  # Add scope for better caching
          cache-to: type=gha,mode=max,scope=aegis-rag
      
      - name: Test Docker Image
        run: |
          docker run --rm aegis-rag-api:test python -c "import src; print('Import successful')"

  # ============================================================
  # JOB 8: DOCUMENTATION CHECK
  # ============================================================
  documentation:
    name: 📖 Documentation
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4
      
      - name: Check Markdown Links
        run: |
          npm install -g markdown-link-check
          # Find all .md files excluding docs/archive/ directory
          find . -name '*.md' -not -path './docs/archive/*' -not -path './node_modules/*' | while read file; do
            echo "Checking $file"
            markdown-link-check "$file" -c .markdown-link-check.json -q || exit 1
          done
      
      - name: Check Docstrings
        run: |
          pip install pydocstyle
          pydocstyle src/ --convention=google --add-ignore=D100,D104,D105,D107
        continue-on-error: true

  # ============================================================
  # JOB 9: OVERALL QUALITY GATE
  # ============================================================
  quality-gate:
    name: ✅ Quality Gate
    runs-on: ubuntu-latest
    needs: 
      - code-quality
      - naming-conventions
      - unit-tests
      - integration-tests
      - security-scan
      - docker-build
    if: always()
    
    steps:
      - name: Check Previous Jobs
        run: |
          echo "Checking if all quality gates passed..."
          
          if [[ "${{ needs.code-quality.result }}" != "success" ]]; then
            echo "❌ Code Quality failed"
            exit 1
          fi
          
          if [[ "${{ needs.naming-conventions.result }}" != "success" ]]; then
            echo "❌ Naming Conventions failed"
            exit 1
          fi
          
          if [[ "${{ needs.unit-tests.result }}" != "success" ]]; then
            echo "❌ Unit Tests failed"
            exit 1
          fi
          
          if [[ "${{ needs.integration-tests.result }}" != "success" ]]; then
            echo "❌ Integration Tests failed"
            exit 1
          fi
          
          if [[ "${{ needs.docker-build.result }}" != "success" ]]; then
            echo "❌ Docker Build failed"
            exit 1
          fi
          
          echo "✅ All quality gates passed!"
      
      - name: Comment on PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const summary = `
            ## ✅ Quality Gates Passed
            
            All checks completed successfully:
            - ✅ Code Quality
            - ✅ Naming Conventions
            - ✅ Unit Tests (>80% coverage)
            - ✅ Integration Tests
            - ✅ Security Scan
            - ✅ Docker Build
            
            Ready for review! 🚀
            `;
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: summary
            });

  # ============================================================
  # JOB 10: PERFORMANCE BENCHMARKS (Optional, nur bei Main)
  # ============================================================
  performance:
    name: ⚡ Performance Benchmarks
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install Poetry
        run: |
          curl -sSL https://install.python-poetry.org | python3 -
          echo "$HOME/.local/bin" >> $GITHUB_PATH

      - name: Install Dependencies
        run: |
          poetry install --no-interaction --no-ansi

      - name: Run Benchmarks
        run: |
          echo "Running performance benchmarks..."
          poetry run python tests/performance/benchmark.py
      
      - name: Upload Benchmark Results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results
          path: benchmark-results.json
