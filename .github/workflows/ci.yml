name: CI Pipeline - Unified Quality Gates

on:
  push:
    branches: [main, develop, sprint-*, feature/*]
  pull_request:
    branches: [main, develop]

# Prevent concurrent runs on same branch
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

env:
  PYTHON_VERSION: "3.12"
  NODE_VERSION: "20"

jobs:
  # ============================================================
  # JOB 1: CODE QUALITY (Linting, Formatting, Type Checking)
  # ============================================================
  code-quality:
    name: 🔍 Code Quality
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
      
      - name: Install Poetry
        run: |
          pip install --upgrade pip
          pip install poetry

      - name: Install Dependencies
        run: |
          poetry install --with dev
      
      - name: Run Ruff Linter
        run: |
          echo "Running Ruff Linter..."
          poetry run ruff check src/ --output-format=github

      - name: Run Black Formatter Check
        run: |
          echo "Checking Black formatting..."
          poetry run black --check --diff src/

      - name: Run MyPy Type Checker
        run: |
          echo "Running MyPy type checking..."
          poetry run mypy src/ --config-file=pyproject.toml
        continue-on-error: true  # Don't fail build, but show warnings

      - name: Run Bandit Security Scanner
        run: |
          echo "Running Bandit security scan..."
          poetry run bandit -r src/ -ll -f json -o bandit-report.json || true
          poetry run bandit -r src/ -ll || true
      
      - name: Upload Bandit Report
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: bandit-report
          path: bandit-report.json

  # ============================================================
  # JOB 2: PYTHON IMPORT VALIDATION (Sprint 18)
  # ============================================================
  python-import-validation:
    name: 🔍 Python Import Validation
    runs-on: ubuntu-latest

    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install Poetry
        run: |
          pip install --upgrade pip
          pip install poetry

      - name: Install Dependencies
        run: |
          poetry install --with dev

      - name: Validate All Python Imports
        run: |
          echo "🔍 Checking all Python files for import errors..."
          failed_files=""

          for file in $(find src -name "*.py" -type f); do
            echo "Checking $file..."
            if ! poetry run python -c "import sys; sys.path.insert(0, '.'); import $(echo $file | sed 's/\//./g' | sed 's/\.py$//' | sed 's/^src\.//')" 2>/dev/null; then
              echo "❌ Import error in $file"
              poetry run python -c "import sys; sys.path.insert(0, '.'); __import__('$(echo $file | sed 's/\//./g' | sed 's/\.py$//' | sed 's/^src\.//')')" 2>&1 || true
              failed_files="$failed_files\n$file"
            fi
          done

          if [ -n "$failed_files" ]; then
            echo -e "\n❌ Import validation failed for:$failed_files"
            exit 1
          fi

          echo "✅ All imports validated successfully"

      - name: Validate Pydantic Models
        run: |
          echo "🔍 Checking Pydantic model imports..."
          poetry run python -c "
          from pathlib import Path
          import re

          errors = []

          for py_file in Path('src').rglob('*.py'):
              content = py_file.read_text()

              # Check if BaseModel is used but not imported
              if 'BaseModel' in content and 'from pydantic' not in content:
                  errors.append(f'{py_file}: Uses BaseModel but missing pydantic import')

          if errors:
              print('\n❌ Pydantic import errors:')
              print('\n'.join(errors))
              exit(1)

          print('✅ All Pydantic imports validated')
          "

  # ============================================================
  # JOB 3: NAMING CONVENTIONS CHECK
  # ============================================================
  naming-conventions:
    name: 📝 Naming Conventions
    runs-on: ubuntu-latest

    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Check Naming Conventions
        run: |
          echo "Checking naming conventions..."
          python scripts/check_naming.py src/**/*.py

  # ============================================================
  # JOB 4: ADR VALIDATION
  # ============================================================
  adr-validation:
    name: 📚 ADR Validation
    runs-on: ubuntu-latest
    if: github.event_name == 'pull_request'

    steps:
      - name: Checkout Code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Full history for diff

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Check if ADR needed
        run: |
          echo "Checking if ADR is required..."
          python scripts/check_adr.py
        continue-on-error: true  # Warning only, not blocking

  # ============================================================
  # JOB 5: UNIT TESTS
  # ============================================================
  unit-tests:
    name: 🧪 Unit Tests
    runs-on: ubuntu-latest

    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install Poetry
        run: |
          curl -sSL https://install.python-poetry.org | python3 -
          echo "$HOME/.local/bin" >> $GITHUB_PATH

      - name: Install Dependencies
        run: |
          poetry install --no-interaction --no-ansi

      - name: Run Unit Tests
        run: |
          echo "Running unit tests..."
          poetry run pytest tests/unit/ tests/components/ tests/api/ \
            --cov=src \
            --cov-report=xml \
            --cov-report=html \
            --cov-report=term-missing \
            --cov-fail-under=50 \
            --junitxml=test-results/unit-results.xml \
            --timeout=300 \
            --timeout-method=thread \
            -v \
            -m "not integration"

      - name: Upload Coverage Report
        uses: codecov/codecov-action@v4
        with:
          file: ./coverage.xml
          flags: unittests
          name: codecov-unit
          fail_ci_if_error: true  # Sprint 13: Fail if coverage upload fails

      - name: Upload Test Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: unit-test-results
          path: |
            test-results/
            coverage.xml
            htmlcov/

  # ============================================================
  # JOB 6: INTEGRATION TESTS (with Docker Services)
  # ============================================================
  integration-tests:
    name: 🔗 Integration Tests
    runs-on: ubuntu-latest

    services:
      qdrant:
        image: qdrant/qdrant:v1.11.0
        ports:
          - 6333:6333
        options: >-
          --health-cmd "timeout 1 bash -c ':> /dev/tcp/127.0.0.1/6333' || exit 1"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 10
      
      neo4j:
        image: neo4j:5.24-community
        ports:
          - 7687:7687
          - 7474:7474
        env:
          NEO4J_AUTH: neo4j/testpassword
          NEO4J_server_memory_heap_initial__size: 512m
          NEO4J_server_memory_heap_max__size: 1g
        options: >-
          --health-cmd "wget --no-verbose --tries=1 --spider http://localhost:7474 || exit 1"
          --health-interval 10s
          --health-timeout 10s
          --health-retries 20
          --health-start-period 60s
      
      redis:
        image: redis:7-alpine
        ports:
          - 6379:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

      # Sprint 12: Add Ollama service for LLM integration tests
      ollama:
        image: ollama/ollama:latest
        ports:
          - 11434:11434
        options: >-
          --health-cmd "curl -f http://localhost:11434/api/version || exit 1"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 10

    steps:
      - name: Free Disk Space (Ubuntu)
        uses: jlumbroso/free-disk-space@main
        with:
          # Remove large packages we don't need
          tool-cache: true
          android: true
          dotnet: true
          haskell: true
          large-packages: true
          docker-images: false  # Keep Docker images for our services
          swap-storage: true

      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install Poetry
        run: |
          curl -sSL https://install.python-poetry.org | python3 -
          echo "$HOME/.local/bin" >> $GITHUB_PATH

      - name: Install Dependencies
        run: |
          poetry install --no-interaction --no-ansi
      
      - name: Wait for Services
        run: |
          echo "Waiting for Qdrant..."
          timeout 60 bash -c 'until curl -f http://localhost:6333/ 2>/dev/null; do sleep 2; done'
          echo "✅ Qdrant ready"

          echo "Waiting for Neo4j HTTP endpoint..."
          timeout 120 bash -c 'until curl -f http://localhost:7474/ 2>/dev/null; do echo "Attempt $((++count))/60: Neo4j not ready..."; sleep 2; done'
          echo "✅ Neo4j HTTP ready"

          echo "Waiting for Neo4j Bolt to be fully ready..."
          sleep 15
          timeout 60 bash -c 'until echo "RETURN 1 AS result" | cypher-shell -u neo4j -p testpassword -a bolt://localhost:7687 --format plain 2>/dev/null | grep -q "1"; do echo "Bolt not ready, waiting..."; sleep 3; done'
          echo "✅ Neo4j Bolt ready"

          echo "Waiting for Redis..."
          timeout 60 bash -c 'until redis-cli ping 2>/dev/null; do sleep 2; done'
          echo "✅ Redis ready"

          # Sprint 12: Wait for Ollama to be ready
          echo "Waiting for Ollama..."
          timeout 120 bash -c 'until curl -f http://localhost:11434/api/version 2>/dev/null; do sleep 2; done'
          echo "✅ Ollama ready"

          # Sprint 12: Pull required models
          echo "Pulling Ollama models..."
          docker exec ollama ollama pull nomic-embed-text
          docker exec ollama ollama pull llama3.2:3b
          echo "✅ Models ready"

      - name: Run Integration Tests
        timeout-minutes: 20  # Increased from default 10 to 20
        env:
          QDRANT_HOST: localhost
          QDRANT_PORT: 6333
          NEO4J_URI: bolt://localhost:7687
          NEO4J_USER: neo4j
          NEO4J_PASSWORD: testpassword
          REDIS_HOST: localhost
          REDIS_PORT: 6379
          OLLAMA_BASE_URL: http://localhost:11434  # Add Ollama config
        run: |
          echo "Running integration tests..."
          poetry run pytest tests/integration/ \
            --cov=src \
            --cov-report=xml \
            --cov-report=html \
            --cov-append \
            --timeout=300 \
            --timeout-method=thread \
            --junitxml=test-results/integration-results.xml \
            -v
      
      - name: Upload Coverage Report
        uses: codecov/codecov-action@v4
        with:
          file: ./coverage.xml
          flags: integration
          name: codecov-integration
          fail_ci_if_error: true  # Sprint 13: Fail if coverage upload fails

      - name: Upload Test Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: integration-test-results
          path: |
            test-results/
            coverage.xml
            htmlcov/

  # ============================================================
  # JOB 7: FRONTEND BUILD & TYPE CHECK (Sprint 18)
  # ============================================================
  frontend-build:
    name: ⚛️ Frontend Build & Type Check
    runs-on: ubuntu-latest
    continue-on-error: true  # Optional if frontend doesn't exist

    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Check if frontend exists
        id: check_frontend
        run: |
          if [ -d "frontend" ] && [ -f "frontend/package.json" ]; then
            echo "exists=true" >> $GITHUB_OUTPUT
          else
            echo "exists=false" >> $GITHUB_OUTPUT
            echo "⚠️  Frontend directory not found, skipping frontend jobs"
          fi

      - name: Setup Node.js
        if: steps.check_frontend.outputs.exists == 'true'
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          cache-dependency-path: frontend/package-lock.json

      - name: Install Dependencies
        if: steps.check_frontend.outputs.exists == 'true'
        working-directory: frontend
        run: npm ci

      - name: TypeScript Type Check
        if: steps.check_frontend.outputs.exists == 'true'
        working-directory: frontend
        run: npm run type-check

      - name: Build Frontend
        if: steps.check_frontend.outputs.exists == 'true'
        working-directory: frontend
        run: npm run build
        env:
          VITE_API_BASE_URL: http://localhost:8000

      - name: Check Bundle Size
        if: steps.check_frontend.outputs.exists == 'true'
        working-directory: frontend
        run: |
          echo "📊 Checking bundle size..."
          BUNDLE_SIZE=$(du -sh dist | cut -f1)
          echo "Bundle size: $BUNDLE_SIZE"

          # Warn if bundle > 2MB
          SIZE_BYTES=$(du -sb dist | cut -f1)
          if [ $SIZE_BYTES -gt 2097152 ]; then
            echo "⚠️ Warning: Bundle size exceeds 2MB"
          else
            echo "✅ Bundle size OK"
          fi

      - name: Upload Build Artifacts
        if: steps.check_frontend.outputs.exists == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: frontend-dist
          path: frontend/dist

  # ============================================================
  # JOB 8: FRONTEND UNIT TESTS (Sprint 18)
  # ============================================================
  frontend-unit-tests:
    name: ⚛️ Frontend Unit Tests
    runs-on: ubuntu-latest
    continue-on-error: true  # Optional if frontend doesn't exist

    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          cache-dependency-path: frontend/package-lock.json

      - name: Install Dependencies
        working-directory: frontend
        run: npm ci

      - name: Run Unit Tests
        working-directory: frontend
        run: npm run test:unit
        env:
          CI: true

      - name: Upload Coverage
        uses: codecov/codecov-action@v4
        with:
          file: frontend/coverage/coverage-final.json
          flags: frontend-unit
          name: codecov-frontend-unit

  # ============================================================
  # JOB 9: FRONTEND E2E TESTS (Sprint 18)
  # ============================================================
  frontend-e2e-tests:
    name: 🎭 Frontend E2E Tests
    runs-on: ubuntu-latest
    continue-on-error: true  # Optional if frontend doesn't exist

    services:
      qdrant:
        image: qdrant/qdrant:v1.11.0
        ports:
          - 6333:6333

      neo4j:
        image: neo4j:5.24-community
        ports:
          - 7687:7687
          - 7474:7474
        env:
          NEO4J_AUTH: neo4j/testpassword

      redis:
        image: redis:7-alpine
        ports:
          - 6379:6379

      ollama:
        image: ollama/ollama:latest
        ports:
          - 11434:11434

    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          cache-dependency-path: frontend/package-lock.json

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install Poetry
        run: |
          pip install poetry
          echo "$HOME/.local/bin" >> $GITHUB_PATH

      - name: Install Backend Dependencies
        run: poetry install --no-interaction

      - name: Wait for Services
        run: |
          timeout 60 bash -c 'until curl -f http://localhost:6333/ 2>/dev/null; do sleep 2; done'
          echo "✅ Qdrant ready"

          timeout 120 bash -c 'until curl -f http://localhost:7474/ 2>/dev/null; do sleep 2; done'
          echo "✅ Neo4j ready"

          timeout 60 bash -c 'until redis-cli ping 2>/dev/null; do sleep 2; done'
          echo "✅ Redis ready"

      - name: Start Backend Server
        run: |
          poetry run uvicorn src.api.main:app --host 0.0.0.0 --port 8000 &
          timeout 60 bash -c 'until curl -f http://localhost:8000/health 2>/dev/null; do sleep 2; done'
          echo "✅ Backend ready"
        env:
          QDRANT_HOST: localhost
          NEO4J_URI: bolt://localhost:7687
          NEO4J_USER: neo4j
          NEO4J_PASSWORD: testpassword
          REDIS_HOST: localhost

      - name: Install Frontend Dependencies
        working-directory: frontend
        run: npm ci

      - name: Run E2E Tests
        working-directory: frontend
        run: npm test -- --run
        env:
          VITE_API_BASE_URL: http://localhost:8000

      - name: Upload E2E Test Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: e2e-test-results
          path: frontend/test-results/

      - name: Upload E2E Screenshots
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: e2e-screenshots
          path: frontend/screenshots/

  # ============================================================
  # JOB 10: API CONTRACT TESTS (Sprint 18)
  # ============================================================
  api-contract-tests:
    name: 📋 API Contract Validation
    runs-on: ubuntu-latest

    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install Poetry
        run: pip install poetry

      - name: Install Dependencies
        run: poetry install

      - name: Generate OpenAPI Schema
        run: |
          poetry run python -c "
          from src.api.main import app
          import json

          schema = app.openapi()
          with open('openapi-generated.json', 'w') as f:
              json.dump(schema, f, indent=2)
          "

      - name: Validate OpenAPI Schema
        run: |
          npm install -g @apidevtools/swagger-cli
          swagger-cli validate openapi-generated.json

      - name: Upload OpenAPI Schema
        uses: actions/upload-artifact@v4
        with:
          name: openapi-schema
          path: openapi-generated.json

  # ============================================================
  # JOB 11: DEPENDENCY AUDIT (Sprint 18)
  # ============================================================
  dependency-audit:
    name: 📦 Dependency Audit
    runs-on: ubuntu-latest

    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}

      - name: Frontend Dependency Audit
        working-directory: frontend
        run: |
          echo "🔍 Auditing frontend dependencies..."
          npm audit --audit-level=moderate || true
          npm audit --json > ../frontend-audit.json || true

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install Poetry
        run: pip install poetry

      - name: Backend Dependency Audit
        run: |
          echo "🔍 Auditing backend dependencies..."
          pip install safety
          poetry export -f requirements.txt --output requirements.txt
          safety check --file requirements.txt --json > backend-audit.json || true

      - name: Upload Audit Reports
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: dependency-audit-reports
          path: |
            frontend-audit.json
            backend-audit.json

  # ============================================================
  # JOB 12: SECURITY SCANNING (Dependencies)
  # ============================================================
  security-scan:
    name: 🔒 Security Scan
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Install Safety
        run: pip install safety
      
      - name: Run Safety Check
        run: |
          echo "Checking dependencies for vulnerabilities..."
          safety check --json --output safety-report.json || true
          safety check
        continue-on-error: true
      
      - name: Upload Safety Report
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: safety-report
          path: safety-report.json

  # ============================================================
  # JOB 13: DOCKER BUILD TEST
  # ============================================================
  docker-build:
    name: 🐳 Docker Build
    runs-on: ubuntu-latest
    continue-on-error: true  # Don't block CI if Docker fails (works locally)

    steps:
      - name: Free Disk Space (Ubuntu)
        uses: jlumbroso/free-disk-space@main
        with:
          tool-cache: true
          android: true
          dotnet: true
          haskell: true
          large-packages: true
          docker-images: false  # Keep base images for Docker build
          swap-storage: true

      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
      
      - name: Build Docker Image
        uses: docker/build-push-action@v5
        with:
          context: .
          file: ./docker/Dockerfile.api
          push: false
          load: true
          tags: aegis-rag-api:test
          cache-from: type=gha,scope=aegis-rag  # Add scope for better caching
          cache-to: type=gha,mode=max,scope=aegis-rag
      
      - name: Test Docker Image
        run: |
          docker run --rm aegis-rag-api:test python -c "import src; print('Import successful')"

  # ============================================================
  # JOB 14: DOCUMENTATION CHECK
  # ============================================================
  documentation:
    name: 📖 Documentation
    runs-on: ubuntu-latest
    continue-on-error: true  # Don't block CI for documentation issues

    steps:
      - name: Checkout Code
        uses: actions/checkout@v4
      
      - name: Check Markdown Links
        run: |
          npm install -g markdown-link-check
          # Find all .md files excluding docs/archive/ and internal guides with anchor links
          find . -name '*.md' \
            -not -path './docs/archive/*' \
            -not -path './docs/CONTEXT_REFRESH_MASTER_GUIDE.md' \
            -not -path './node_modules/*' | while read file; do
            echo "Checking $file"
            markdown-link-check "$file" -c .markdown-link-check.json -q || exit 1
          done
      
      - name: Check Docstrings
        run: |
          pip install pydocstyle
          pydocstyle src/ --convention=google --add-ignore=D100,D104,D105,D107
        continue-on-error: true

  # ============================================================
  # JOB 15: UNIFIED QUALITY GATE
  # ============================================================
  quality-gate:
    name: ✅ Unified Quality Gate
    runs-on: ubuntu-latest
    needs:
      - code-quality
      - python-import-validation
      - naming-conventions
      - adr-validation
      - unit-tests
      - integration-tests
      - frontend-build
      - frontend-unit-tests
      - frontend-e2e-tests
      - api-contract-tests
      - dependency-audit
      - security-scan
      - docker-build
    if: always()
    
    steps:
      - name: Check Previous Jobs
        run: |
          echo "🔍 Checking all quality gates..."

          ALL_PASSED=true

          if [[ "${{ needs.code-quality.result }}" != "success" ]]; then
            echo "❌ Code Quality failed"
            ALL_PASSED=false
          fi

          if [[ "${{ needs.python-import-validation.result }}" != "success" ]]; then
            echo "❌ Python Import Validation failed"
            ALL_PASSED=false
          fi

          if [[ "${{ needs.naming-conventions.result }}" != "success" ]]; then
            echo "❌ Naming Conventions failed"
            ALL_PASSED=false
          fi

          if [[ "${{ needs.adr-validation.result }}" != "success" && "${{ needs.adr-validation.result }}" != "skipped" ]]; then
            echo "❌ ADR Validation failed"
            ALL_PASSED=false
          fi

          if [[ "${{ needs.unit-tests.result }}" != "success" ]]; then
            echo "❌ Unit Tests failed"
            ALL_PASSED=false
          fi

          if [[ "${{ needs.integration-tests.result }}" != "success" ]]; then
            echo "❌ Integration Tests failed"
            ALL_PASSED=false
          fi

          if [[ "${{ needs.frontend-build.result }}" == "failure" ]]; then
            echo "⚠️  Frontend Build failed (non-blocking)"
          fi

          if [[ "${{ needs.frontend-unit-tests.result }}" == "failure" ]]; then
            echo "⚠️  Frontend Unit Tests failed (non-blocking)"
          fi

          if [[ "${{ needs.frontend-e2e-tests.result }}" == "failure" ]]; then
            echo "⚠️  Frontend E2E Tests failed (non-blocking)"
          fi

          if [[ "${{ needs.api-contract-tests.result }}" != "success" ]]; then
            echo "❌ API Contract Tests failed"
            ALL_PASSED=false
          fi

          if [[ "${{ needs.dependency-audit.result }}" != "success" ]]; then
            echo "⚠️  Dependency Audit has warnings (non-blocking)"
          fi

          if [[ "${{ needs.security-scan.result }}" != "success" ]]; then
            echo "⚠️  Security Scan has warnings (non-blocking)"
          fi

          if [[ "${{ needs.docker-build.result }}" != "success" ]]; then
            echo "⚠️  Docker Build failed (non-blocking)"
          fi

          if [ "$ALL_PASSED" = false ]; then
            echo ""
            echo "❌ QUALITY GATE FAILED"
            exit 1
          fi

          echo ""
          echo "✅ All critical quality gates passed!"
      
      - name: Comment on PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const summary = `
            ## ✅ Unified Quality Gate Passed

            ### Backend Tests
            - ✅ Code Quality (Ruff + Black + MyPy)
            - ✅ Python Import Validation
            - ✅ Naming Conventions
            - ✅ Unit Tests (>50% coverage)
            - ✅ Integration Tests

            ### Frontend Tests
            - ✅ Frontend Build & Type Check
            - ✅ Frontend Unit Tests
            - ✅ Frontend E2E Tests

            ### API & Security
            - ✅ API Contract Validation
            - ✅ Dependency Audit
            - ✅ Security Scan

            ### Infrastructure
            - ✅ Docker Build

            **Ready for review!** 🚀
            `;

            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: summary
            });

  # ============================================================
  # JOB 16: PERFORMANCE BENCHMARKS (Optional, nur bei Main)
  # ============================================================
  performance:
    name: ⚡ Performance Benchmarks
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install Poetry
        run: |
          curl -sSL https://install.python-poetry.org | python3 -
          echo "$HOME/.local/bin" >> $GITHUB_PATH

      - name: Install Dependencies
        run: |
          poetry install --no-interaction --no-ansi

      - name: Run Benchmarks
        run: |
          echo "Running performance benchmarks..."
          poetry run python tests/performance/benchmark.py
      
      - name: Upload Benchmark Results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results
          path: benchmark-results.json
