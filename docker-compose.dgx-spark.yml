# =============================================================================
# docker-compose.dgx-spark.yml - AegisRAG for NVIDIA DGX Spark
# =============================================================================
# Optimized for DGX Spark (Grace Blackwell, ARM64, 128GB Unified Memory)
#
# Memory Distribution (128GB total):
#   - Qwen3 32B (LLM + Entity Extraction): ~20GB
#   - Qwen3-VL 32B (Vision):               ~21GB
#   - Docling (OCR + Layout):               ~4GB
#   - Qdrant (Vector DB):                   ~8GB
#   - Neo4j (Graph DB):                     ~8GB
#   - Redis (Cache):                        ~4GB
#   - System + Overhead:                   ~10GB
#   - Reserve:                             ~53GB (for concurrent model loading)
#
# Usage:
#   # Start all services
#   docker compose -f docker-compose.dgx-spark.yml up -d
#
#   # Start with ingestion profile (includes Docling)
#   docker compose -f docker-compose.dgx-spark.yml --profile ingestion up -d
#
#   # Pull Qwen3 models (run once after Ollama starts)
#   docker exec aegis-ollama ollama pull qwen3:32b
#   docker exec aegis-ollama ollama pull qwen3-vl:32b
# =============================================================================

services:
  # ============================================================================
  # Ollama - Local LLM Server with Qwen3 Models
  # ============================================================================
  # Models to pull after startup:
  #   - qwen3:32b        (20GB) - Generation & Entity/Relation Extraction
  #   - qwen3-vl:32b     (21GB) - Vision Language Model for images/PDFs
  #   - nomic-embed-text (274MB) - Embeddings (optional, we use BGE-M3)
  # ============================================================================
  ollama:
    image: ollama/ollama:latest
    container_name: aegis-ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
      # DGX Spark: Keep models loaded longer (128GB allows it)
      - OLLAMA_KEEP_ALIVE=60m
      # Allow 2 models loaded simultaneously (Qwen3 + Qwen3-VL)
      # With 128GB unified memory, both fit comfortably
      - OLLAMA_MAX_LOADED_MODELS=2
      # Increase context window for better extraction quality
      - OLLAMA_NUM_CTX=8192
      # Use all available CPU cores for prompt processing
      - OLLAMA_NUM_THREAD=20
      # Force ALL model layers to GPU (DGX Spark unified memory)
      # -1 = use all available GPU layers, prevents CPU fallback
      - OLLAMA_NUM_GPU=-1
    # GPU Support for DGX Spark (Blackwell GPU)
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
        limits:
          # Allow up to 64GB for LLM operations
          # Qwen3-32B (20GB) + Qwen3-VL-32B (21GB) + overhead
          memory: 64G
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    restart: unless-stopped
    networks:
      - aegis-network

  # ============================================================================
  # Docling - GPU-Accelerated Document Parsing (ARM64 Build)
  # ============================================================================
  # TESTED: 2025-12-06 - GPU MODE WORKING on DGX Spark GB10!
  #
  # Requirements:
  #   - NGC PyTorch 25.09 base image (CUDA 13.0, sm_121 support)
  #   - Build: docker build -f docker/Dockerfile.docling-spark -t aegis-docling-spark:latest .
  #
  # Performance (GPU vs CPU):
  #   - GPU: 9-page PDF in ~12s (layout + table structure on cuda:0)
  #   - CPU: 9-page PDF in ~60s (3-5x slower)
  # ============================================================================
  docling:
    # Custom ARM64 image with NGC 25.09 base (CUDA 13.0 for sm_121)
    image: aegis-docling-spark:latest
    build:
      context: .
      dockerfile: docker/Dockerfile.docling-spark
    container_name: aegis-docling
    profiles:
      - ingestion  # Only start when explicitly requested
    ports:
      - "8080:5001"
    volumes:
      # Cache volumes to persist downloaded models
      - docling_hf_cache:/root/.cache/huggingface
      - docling_ocr_cache:/root/.EasyOCR
    environment:
      # Port Configuration
      - PORT=5001

      # GPU Configuration: ENABLED for Blackwell GB10 (sm_121)
      # ============================================================================
      # STATUS: GPU mode WORKING with NGC 25.09 (CUDA 13.0)
      # TESTED: 2025-12-06
      #
      # Solution: NGC PyTorch 25.09 container has CUDA 13.0.1 which supports:
      #   sm_80, sm_86, sm_90, sm_100, sm_110, sm_120, compute_120
      # The GB10 (sm_121) runs successfully despite the warning about 12.1
      #
      # Previously failed with:
      # - NGC 25.03 (CUDA 12.8): nvrtc error - only sm_120, not sm_121
      # - PyTorch Nightly cu128: same issue
      # ============================================================================
      - DOCLING_DEVICE=cuda

      # PyTorch/CUDA optimizations for DGX Spark
      - TORCH_CUDA_ARCH_LIST=12.1a
      - TRITON_PTXAS_PATH=/usr/local/cuda/bin/ptxas

      # Model Configuration
      - DOCLING_LAZY_LOADING=true
      # OCR: auto selects best available (EasyOCR GPU or RapidOCR CPU)
      - DOCLING_OCR_ENGINE=auto
      - DOCLING_TABLE_STRUCTURE=true
      - DOCLING_LAYOUT_ANALYSIS=true

      # Image Extraction for VLM Processing
      - DOCLING_GENERATE_PICTURE_IMAGES=true
      - DOCLING_IMAGES_SCALE=2.0

      # DGX Spark: GPU mode is faster, but keep reasonable timeouts
      - NUM_WORKERS=4
      - DOCLING_BATCH_SIZE=4
      - DOCLING_TIMEOUT=300
      - DOCLING_SERVE_MAX_SYNC_WAIT=300

      # Logging
      - LOG_LEVEL=INFO

    # GPU Support for DGX Spark (Blackwell GB10)
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
        limits:
          # GPU mode uses ~4-6GB VRAM for models
          memory: 16G

    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5001/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s

    restart: unless-stopped
    networks:
      - aegis-network

  # ============================================================================
  # Qdrant - Vector Database
  # ============================================================================
  # DGX Spark: Increased memory for larger vector collections
  # ============================================================================
  qdrant:
    image: qdrant/qdrant:v1.11.0
    container_name: aegis-qdrant
    ports:
      - "6333:6333"  # HTTP API
      - "6334:6334"  # gRPC
    volumes:
      - qdrant_data:/qdrant/storage
    environment:
      - QDRANT__SERVICE__GRPC_PORT=6334
      - QDRANT__LOG_LEVEL=INFO
      # DGX Spark: Optimize for larger collections
      - QDRANT__STORAGE__PERFORMANCE__MAX_SEARCH_THREADS=8
    deploy:
      resources:
        limits:
          memory: 8G  # Increased from 4G for larger collections
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:6333/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    restart: unless-stopped
    networks:
      - aegis-network

  # ============================================================================
  # Neo4j - Graph Database
  # ============================================================================
  # DGX Spark: Increased heap for complex graph queries
  # ============================================================================
  neo4j:
    image: neo4j:5.24-community
    container_name: aegis-neo4j
    ports:
      - "7474:7474"  # HTTP
      - "7687:7687"  # Bolt
    volumes:
      - neo4j_data:/data
      - neo4j_logs:/logs
      - neo4j_import:/var/lib/neo4j/import
      - neo4j_plugins:/plugins
    environment:
      - NEO4J_AUTH=neo4j/aegis-rag-neo4j-password
      - NEO4J_PLUGINS=["apoc", "graph-data-science"]
      # DGX Spark: Increased memory allocation
      - NEO4J_dbms_memory_pagecache_size=2G
      - NEO4J_dbms_memory_heap_initial__size=2G
      - NEO4J_dbms_memory_heap_max__size=4G
      - NEO4J_dbms_security_procedures_unrestricted=apoc.*,gds.*
      - NEO4J_dbms_security_procedures_allowlist=apoc.*,gds.*
      - NEO4J_apoc_export_file_enabled=true
      - NEO4J_apoc_import_file_enabled=true
      - NEO4J_apoc_import_file_use__neo4j__config=true
    deploy:
      resources:
        limits:
          memory: 8G  # Increased from 4G
    healthcheck:
      test: ["CMD", "cypher-shell", "-u", "neo4j", "-p", "aegis-rag-neo4j-password", "RETURN 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    restart: unless-stopped
    networks:
      - aegis-network

  # ============================================================================
  # Redis - Cache & Short-Term Memory
  # ============================================================================
  redis:
    image: redis:7-alpine
    container_name: aegis-redis
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    command: >
      redis-server
      --appendonly yes
      --appendfsync everysec
      --maxmemory 4gb
      --maxmemory-policy allkeys-lru
      --save 60 1000
    deploy:
      resources:
        limits:
          memory: 4G  # Increased from 2G
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    restart: unless-stopped
    networks:
      - aegis-network

  # ============================================================================
  # Prometheus - Metrics Collection
  # ============================================================================
  prometheus:
    image: prom/prometheus:latest
    container_name: aegis-prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./config/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
      - '--web.enable-lifecycle'
    deploy:
      resources:
        limits:
          memory: 1G
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:9090/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    restart: unless-stopped
    networks:
      - aegis-network
    depends_on:
      - qdrant
      - neo4j
      - redis

  # ============================================================================
  # Grafana - Metrics Visualization
  # ============================================================================
  grafana:
    image: grafana/grafana:latest
    container_name: aegis-grafana
    ports:
      - "3000:3000"
    volumes:
      - grafana_data:/var/lib/grafana
      - ./config/grafana-datasources.yml:/etc/grafana/provisioning/datasources/datasources.yml:ro
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=aegis-rag-grafana
      - GF_INSTALL_PLUGINS=redis-datasource
      - GF_SERVER_ROOT_URL=http://localhost:3000
    deploy:
      resources:
        limits:
          memory: 512M
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    restart: unless-stopped
    networks:
      - aegis-network
    depends_on:
      - prometheus

  # ============================================================================
  # AEGIS RAG API - FastAPI Backend
  # ============================================================================
  api:
    build:
      context: .
      dockerfile: docker/Dockerfile.api
    image: aegis-rag-api:latest
    container_name: aegis-api
    ports:
      - "8000:8000"
    volumes:
      - ./src:/app/src:ro
      - ./config:/app/config:ro
      - ./data:/app/data:rw  # Documents for indexing
    environment:
      - ENVIRONMENT=development
      - DEBUG=false
      - LOG_LEVEL=INFO
      - API_HOST=0.0.0.0
      - API_PORT=8000
      - API_WORKERS=4
      # LLM Configuration
      - OLLAMA_BASE_URL=http://ollama:11434
      - OLLAMA_MODEL_GENERATION=qwen3:32b
      - OLLAMA_MODEL_QUERY=qwen3:32b
      - OLLAMA_MODEL_EXTRACTION=qwen3:32b
      - OLLAMA_MODEL_VLM=qwen3-vl:32b
      - LIGHTRAG_LLM_MODEL=qwen3:32b
      - GEMMA_MODEL=qwen3:32b
      - VLM_BACKEND=ollama
      - EMBEDDING_BACKEND=sentence-transformers
      - ST_MODEL_NAME=BAAI/bge-m3
      - ST_DEVICE=auto
      - ST_BATCH_SIZE=64
      # Databases
      - QDRANT_HOST=qdrant
      - QDRANT_PORT=6333
      - QDRANT_GRPC_PORT=6334
      - QDRANT_COLLECTION=documents_v1
      - QDRANT_USE_GRPC=true
      - NEO4J_URI=bolt://neo4j:7687
      - NEO4J_USER=neo4j
      - NEO4J_PASSWORD=aegis-rag-neo4j-password
      - NEO4J_DATABASE=neo4j
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_DB=0
      - REDIS_TTL=3600
      # Redis Memory URL for working memory (must use Docker service name!)
      - REDIS_MEMORY_URL=redis://redis:6379/0
      # Docling
      - DOCLING_BASE_URL=http://docling:5001
      - DOCLING_TIMEOUT_SECONDS=900
      - DOCLING_MAX_RETRIES=3
      # Observability
      - LANGSMITH_TRACING=false
      - PROMETHEUS_ENABLED=true
      # MCP
      - MCP_SERVER_PORT=3000
      - MCP_AUTH_ENABLED=false
    depends_on:
      ollama:
        condition: service_healthy
      qdrant:
        condition: service_started
      neo4j:
        condition: service_healthy
      redis:
        condition: service_healthy
      prometheus:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    restart: unless-stopped
    networks:
      - aegis-network
    deploy:
      resources:
        limits:
          memory: 8G

  # ============================================================================
  # AEGIS RAG Test Runner - For running pytest in container
  # ============================================================================
  # Usage:
  #   # Run all tests
  #   docker compose -f docker-compose.dgx-spark.yml run --rm test
  #
  #   # Run specific test file
  #   docker compose -f docker-compose.dgx-spark.yml run --rm test pytest tests/unit/components/llm_proxy/test_vlm_factory.py -v
  #
  #   # Run with coverage
  #   docker compose -f docker-compose.dgx-spark.yml run --rm test pytest tests/ --cov=src --cov-report=term-missing
  # ============================================================================
  test:
    build:
      context: .
      dockerfile: docker/Dockerfile.api-test
    image: aegis-rag-test:latest
    container_name: aegis-test
    profiles:
      - test  # Only start when explicitly requested
    volumes:
      - ./src:/app/src:ro
      - ./tests:/app/tests:ro
      - ./config:/app/config:ro
      - ./pyproject.toml:/app/pyproject.toml:ro
    environment:
      - ENVIRONMENT=development
      - DEBUG=true
      - LOG_LEVEL=DEBUG
      # LLM Configuration (same as api service)
      - OLLAMA_BASE_URL=http://ollama:11434
      - OLLAMA_MODEL_GENERATION=qwen3:32b
      - OLLAMA_MODEL_QUERY=qwen3:32b
      - OLLAMA_MODEL_EXTRACTION=qwen3:32b
      - OLLAMA_MODEL_VLM=qwen3-vl:32b
      - LIGHTRAG_LLM_MODEL=qwen3:32b
      - GEMMA_MODEL=qwen3:32b
      - VLM_BACKEND=ollama
      - EMBEDDING_BACKEND=sentence-transformers
      - ST_MODEL_NAME=BAAI/bge-m3
      # Databases
      - QDRANT_HOST=qdrant
      - QDRANT_PORT=6333
      - NEO4J_URI=bolt://neo4j:7687
      - NEO4J_USER=neo4j
      - NEO4J_PASSWORD=aegis-rag-neo4j-password
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      # Test Configuration
      - PYTEST_ADDOPTS=-v --tb=short
    depends_on:
      - ollama
      - qdrant
      - neo4j
      - redis
    networks:
      - aegis-network
    deploy:
      resources:
        limits:
          memory: 4G

# ============================================================================
# Volumes
# ============================================================================
volumes:
  ollama_data:
    driver: local
  qdrant_data:
    driver: local
  neo4j_data:
    driver: local
  neo4j_logs:
    driver: local
  neo4j_import:
    driver: local
  neo4j_plugins:
    driver: local
  redis_data:
    driver: local
  prometheus_data:
    driver: local
  grafana_data:
    driver: local
  docling_hf_cache:
    driver: local
  docling_ocr_cache:
    driver: local

# ============================================================================
# Networks
# ============================================================================
networks:
  aegis-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.26.0.0/16
