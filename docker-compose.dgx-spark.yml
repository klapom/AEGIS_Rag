# =============================================================================
# docker-compose.dgx-spark.yml - AegisRAG for NVIDIA DGX Spark
# =============================================================================
# Optimized for DGX Spark (Grace Blackwell, ARM64, 128GB Unified Memory)
#
# Memory Distribution (128GB total):
#   - Nemotron3 Nano (LLM Generation):     ~15GB
#   - GPT-OSS 20B (Query + Extraction):    ~12GB
#   - Qwen3-VL 32B (Vision):               ~21GB
#   - Docling (OCR + Layout):               ~4GB
#   - Qdrant (Vector DB):                   ~8GB
#   - Neo4j (Graph DB):                     ~8GB
#   - Redis (Cache):                        ~4GB
#   - System + Overhead:                   ~10GB
#   - Reserve:                             ~46GB (for concurrent model loading)
#
# Usage:
#   # Start all services
#   docker compose -f docker-compose.dgx-spark.yml up -d
#
#   # Start with ingestion profile (includes Docling)
#   docker compose -f docker-compose.dgx-spark.yml --profile ingestion up -d
#
#   # Pull models (run once after Ollama starts)
#   docker exec aegis-ollama ollama pull nemotron-no-think:latest
#   docker exec aegis-ollama ollama pull gpt-oss:20b
#   docker exec aegis-ollama ollama pull qwen3-vl:32b
# =============================================================================

services:
  # ============================================================================
  # Ollama - Local LLM Server
  # ============================================================================
  # Models to pull after startup:
  #   - nemotron-no-think:latest (15GB) - LLM Generation (Nemotron3 Nano 30/3a)
  #   - gpt-oss:20b              (12GB) - Query & Entity/Relation Extraction
  #   - qwen3-vl:32b             (21GB) - Vision Language Model for images/PDFs
  # Note: Embeddings use native sentence-transformers (BGE-M3), not Ollama
  # ============================================================================
  ollama:
    image: ollama/ollama:latest
    container_name: aegis-ollama
    # Sprint 105: Increase shared memory for large contexts (32K) and parallel requests
    # Default 64MB is too small for OLLAMA_NUM_CTX=32768 + OLLAMA_NUM_PARALLEL=2
    # Prevents sporadic hangs/timeouts with large prompts
    shm_size: "2g"
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
      # DGX Spark: Keep models loaded longer (128GB allows it)
      - OLLAMA_KEEP_ALIVE=60m
      # Allow 2 models loaded simultaneously (Qwen3 + Qwen3-VL)
      # With 128GB unified memory, both fit comfortably
      - OLLAMA_MAX_LOADED_MODELS=2
      # Sprint 61 Feature 61.3: Parallel request handling
      # Sprint 92: Reduced from 4 to 2 for GPU memory compatibility with 32K context
      # 32K context Ã— 4 parallel exceeded GPU memory allocation on DGX Spark
      - OLLAMA_NUM_PARALLEL=2
      # Sprint 61 Feature 61.3: Request queue size for high-throughput scenarios
      - OLLAMA_MAX_QUEUE=512
      # Sprint 75: 32K for RAGAS evaluation with long contexts (gpt-oss:20b supports 128K)
      # 32768 tokens = ~131K chars (covers even complex multi-document scenarios)
      - OLLAMA_NUM_CTX=32768
      # Sprint 92: Disable flash attention for DGX Spark Blackwell GPU compatibility
      - OLLAMA_FLASH_ATTENTION=false
      # Use all available CPU cores for prompt processing
      - OLLAMA_NUM_THREAD=20
      # Force ALL model layers to GPU (DGX Spark unified memory)
      # -1 = use all available GPU layers, prevents CPU fallback
      - OLLAMA_NUM_GPU=-1
    # GPU Support for DGX Spark (Blackwell GPU)
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
        limits:
          # Allow up to 64GB for LLM operations
          # Qwen3-32B (20GB) + Qwen3-VL-32B (21GB) + overhead
          memory: 64G
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    restart: unless-stopped
    networks:
      - aegis-network

  # ============================================================================
  # Docling - GPU-Accelerated Document Parsing (ARM64 Build)
  # ============================================================================
  # TESTED: 2025-12-06 - GPU MODE WORKING on DGX Spark GB10!
  #
  # Requirements:
  #   - NGC PyTorch 25.09 base image (CUDA 13.0, sm_121 support)
  #   - Build: docker build -f docker/Dockerfile.docling-spark -t aegis-docling-spark:latest .
  #
  # Performance (GPU vs CPU):
  #   - GPU: 9-page PDF in ~12s (layout + table structure on cuda:0)
  #   - CPU: 9-page PDF in ~60s (3-5x slower)
  # ============================================================================
  docling:
    # Custom ARM64 image with NGC 25.09 base (CUDA 13.0 for sm_121)
    image: aegis-docling-spark:latest
    build:
      context: .
      dockerfile: docker/Dockerfile.docling-spark
    container_name: aegis-docling
    # Sprint 105: Increase shared memory for PyTorch OCR operations
    # Default 64MB is too small for GPU-based document processing
    shm_size: "1g"
    profiles:
      - ingestion  # Only start when explicitly requested
    ports:
      - "8080:5001"
    volumes:
      # Cache volumes to persist downloaded models
      - docling_hf_cache:/root/.cache/huggingface
      - docling_ocr_cache:/root/.EasyOCR
    environment:
      # Port Configuration
      - PORT=5001

      # GPU Configuration: ENABLED for Blackwell GB10 (sm_121)
      # ============================================================================
      # STATUS: GPU mode WORKING with NGC 25.09 (CUDA 13.0)
      # TESTED: 2025-12-06
      #
      # Solution: NGC PyTorch 25.09 container has CUDA 13.0.1 which supports:
      #   sm_80, sm_86, sm_90, sm_100, sm_110, sm_120, compute_120
      # The GB10 (sm_121) runs successfully despite the warning about 12.1
      #
      # Previously failed with:
      # - NGC 25.03 (CUDA 12.8): nvrtc error - only sm_120, not sm_121
      # - PyTorch Nightly cu128: same issue
      # ============================================================================
      - DOCLING_DEVICE=cuda

      # PyTorch/CUDA optimizations for DGX Spark
      - TORCH_CUDA_ARCH_LIST=12.1a
      - TRITON_PTXAS_PATH=/usr/local/cuda/bin/ptxas

      # Model Configuration
      - DOCLING_LAZY_LOADING=true
      # OCR: auto selects best available (EasyOCR GPU or RapidOCR CPU)
      - DOCLING_OCR_ENGINE=auto
      - DOCLING_TABLE_STRUCTURE=true
      - DOCLING_LAYOUT_ANALYSIS=true

      # Image Extraction for VLM Processing
      - DOCLING_GENERATE_PICTURE_IMAGES=true
      - DOCLING_IMAGES_SCALE=2.0

      # DGX Spark: GPU mode is faster, but keep reasonable timeouts
      - NUM_WORKERS=4
      - DOCLING_BATCH_SIZE=4
      - DOCLING_TIMEOUT=300
      - DOCLING_SERVE_MAX_SYNC_WAIT=300

      # Logging
      - LOG_LEVEL=INFO

    # GPU Support for DGX Spark (Blackwell GB10)
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
        limits:
          # GPU mode uses ~4-6GB VRAM for models
          memory: 16G

    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5001/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s

    restart: unless-stopped
    networks:
      - aegis-network

  # ============================================================================
  # Qdrant - Vector Database
  # ============================================================================
  # DGX Spark: Increased memory for larger vector collections
  # ============================================================================
  qdrant:
    image: qdrant/qdrant:v1.11.0
    container_name: aegis-qdrant
    ports:
      - "6333:6333"  # HTTP API
      - "6334:6334"  # gRPC
    volumes:
      - qdrant_data:/qdrant/storage
    environment:
      - QDRANT__SERVICE__GRPC_PORT=6334
      - QDRANT__LOG_LEVEL=INFO
      # DGX Spark: Optimize for larger collections
      - QDRANT__STORAGE__PERFORMANCE__MAX_SEARCH_THREADS=8
    deploy:
      resources:
        limits:
          memory: 8G  # Increased from 4G for larger collections
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:6333/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    restart: unless-stopped
    networks:
      - aegis-network

  # ============================================================================
  # Neo4j - Graph Database
  # ============================================================================
  # DGX Spark: Increased heap for complex graph queries
  # ============================================================================
  neo4j:
    image: neo4j:5.24-community
    container_name: aegis-neo4j
    ports:
      - "7474:7474"  # HTTP
      - "7687:7687"  # Bolt
    volumes:
      - neo4j_data:/data
      - neo4j_logs:/logs
      - neo4j_import:/var/lib/neo4j/import
      - neo4j_plugins:/plugins
    environment:
      - NEO4J_AUTH=neo4j/aegis-rag-neo4j-password
      - NEO4J_PLUGINS=["apoc", "graph-data-science"]
      # DGX Spark: Increased memory allocation
      - NEO4J_dbms_memory_pagecache_size=2G
      - NEO4J_dbms_memory_heap_initial__size=2G
      - NEO4J_dbms_memory_heap_max__size=4G
      - NEO4J_dbms_security_procedures_unrestricted=apoc.*,gds.*
      - NEO4J_dbms_security_procedures_allowlist=apoc.*,gds.*
      - NEO4J_apoc_export_file_enabled=true
      - NEO4J_apoc_import_file_enabled=true
      - NEO4J_apoc_import_file_use__neo4j__config=true
    deploy:
      resources:
        limits:
          memory: 8G  # Increased from 4G
    healthcheck:
      test: ["CMD", "cypher-shell", "-u", "neo4j", "-p", "aegis-rag-neo4j-password", "RETURN 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    restart: unless-stopped
    networks:
      - aegis-network

  # ============================================================================
  # Redis - Cache & Short-Term Memory
  # ============================================================================
  redis:
    image: redis:7-alpine
    container_name: aegis-redis
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    command: >
      redis-server
      --appendonly yes
      --appendfsync everysec
      --maxmemory 8gb
      --maxmemory-policy allkeys-lru
      --save 60 1000
      --maxmemory-samples 10
      --lazyfree-lazy-eviction yes
    deploy:
      resources:
        limits:
          memory: 8G  # Increased from 4G for Sprint 68 Feature 68.3
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    restart: unless-stopped
    networks:
      - aegis-network

  # ============================================================================
  # Prometheus - Metrics Collection
  # ============================================================================
  prometheus:
    image: prom/prometheus:latest
    container_name: aegis-prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./config/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./prometheus/alerts.yml:/etc/prometheus/alerts.yml:ro
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
      - '--web.enable-lifecycle'
    deploy:
      resources:
        limits:
          memory: 1G
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:9090/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    restart: unless-stopped
    networks:
      - aegis-network
    depends_on:
      - qdrant
      - neo4j
      - redis

  # ============================================================================
  # Grafana - Metrics Visualization
  # ============================================================================
  grafana:
    image: grafana/grafana:latest
    container_name: aegis-grafana
    ports:
      - "3000:3000"
    volumes:
      - grafana_data:/var/lib/grafana
      - ./config/grafana-datasources.yml:/etc/grafana/provisioning/datasources/datasources.yml:ro
      - ./config/grafana/dashboards.yml:/etc/grafana/provisioning/dashboards/dashboards.yml:ro
      - ./config/grafana:/etc/grafana/provisioning/dashboards/aegis:ro
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=aegis-rag-grafana
      - GF_INSTALL_PLUGINS=redis-datasource
      - GF_SERVER_ROOT_URL=http://localhost:3000
      - GF_DASHBOARDS_DEFAULT_HOME_DASHBOARD_PATH=/etc/grafana/provisioning/dashboards/aegis/aegis_overview_sprint69.json
    deploy:
      resources:
        limits:
          memory: 512M
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    restart: unless-stopped
    networks:
      - aegis-network
    depends_on:
      - prometheus

  # ============================================================================
  # AEGIS RAG API - FastAPI Backend
  # ============================================================================
  api:
    build:
      context: .
      # Sprint 65 Feature 65.2: CUDA-enabled API for BGE-M3 GPU acceleration
      # Use Dockerfile.api-cuda for 10x faster embeddings (280ms saved per query)
      # Use Dockerfile.api for CPU-only deployment (no GPU required)
      dockerfile: docker/Dockerfile.api-cuda
    image: aegis-rag-api-cuda:latest
    container_name: aegis-api
    # Sprint 105: Increase shared memory for PyTorch/BGE-M3 embeddings
    # Default 64MB is too small for GPU operations with large batches
    shm_size: "2g"
    ports:
      - "8000:8000"
    volumes:
      - ./src:/app/src:ro
      - ./config:/app/config:ro
      - ./data:/app/data:rw  # Documents for indexing
      - ./skills:/app/skills:rw  # Sprint 99: Skills directory for Skill Management
    environment:
      - ENVIRONMENT=development
      - DEBUG=false
      - LOG_LEVEL=INFO
      - API_HOST=0.0.0.0
      - API_PORT=8000
      - API_WORKERS=4
      # Sprint 92: CORS Origins for Docker frontend deployment
      # Browser on port 80 needs to access API on port 8000 (cross-origin)
      # Note: pydantic-settings requires JSON array format for list[str] fields
      - CORS_ORIGINS=["http://localhost","http://localhost:80","http://localhost:3000","http://localhost:5173","http://localhost:5179","http://${DGX_SPARK_IP:-192.168.178.10}","http://${DGX_SPARK_IP:-192.168.178.10}:80"]
      # Sprint 81: C-LARA SetFit Intent Classifier (A/B Testing)
      # Set to 'true' to use C-LARA SetFit model (95.22% accuracy, ~40ms)
      # Set to 'false' to use legacy LLM-based classifier
      - USE_SETFIT_CLASSIFIER=${USE_SETFIT_CLASSIFIER:-true}
      # LLM Configuration
      - OLLAMA_BASE_URL=http://ollama:11434
      - OLLAMA_MODEL_GENERATION=${OLLAMA_MODEL_GENERATION:-nemotron-no-think:latest}
      - OLLAMA_MODEL_QUERY=${OLLAMA_MODEL_QUERY:-gpt-oss:20b}
      - OLLAMA_MODEL_EXTRACTION=${OLLAMA_MODEL_EXTRACTION:-gpt-oss:20b}
      - OLLAMA_MODEL_VLM=${OLLAMA_MODEL_VLM:-qwen3-vl:32b}
      - LIGHTRAG_LLM_MODEL=${OLLAMA_MODEL_GENERATION:-nemotron-no-think:latest}
      - GEMMA_MODEL=${OLLAMA_MODEL_GENERATION:-nemotron-no-think:latest}
      - VLM_BACKEND=ollama
      - EMBEDDING_BACKEND=flag-embedding  # Sprint 87: Multi-vector (dense + sparse)
      - ST_MODEL_NAME=BAAI/bge-m3
      - ST_DEVICE=cuda
      - ST_BATCH_SIZE=64
      # HuggingFace Model Caching (Sprint 76 Feature 76.4)
      # Cache BGE-M3 and other HF models in data/models/ to avoid re-downloads
      - HF_HOME=/app/data/models
      - TRANSFORMERS_CACHE=/app/data/models
      - SENTENCE_TRANSFORMERS_HOME=/app/data/models
      # Databases
      - QDRANT_HOST=qdrant
      - QDRANT_PORT=6333
      - QDRANT_GRPC_PORT=6334
      - QDRANT_COLLECTION=documents_v1
      - QDRANT_USE_GRPC=true
      - NEO4J_URI=bolt://neo4j:7687
      - NEO4J_USER=neo4j
      - NEO4J_PASSWORD=aegis-rag-neo4j-password
      - NEO4J_DATABASE=neo4j
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_DB=0
      - REDIS_TTL=3600
      # Redis Memory URL for working memory (must use Docker service name!)
      - REDIS_MEMORY_URL=redis://redis:6379/0
      # Docling
      - DOCLING_BASE_URL=http://docling:5001
      - DOCLING_TIMEOUT_SECONDS=900
      - DOCLING_MAX_RETRIES=3
      # Observability
      # Sprint 115 Feature 115.6: LangSmith Tracing (Optional)
      # Enable LLM call tracing for debugging timeout issues
      # Get API key from: https://smith.langchain.com/settings
      - LANGSMITH_TRACING=${LANGSMITH_TRACING:-false}
      - LANGSMITH_API_KEY=${LANGSMITH_API_KEY:-}
      - LANGSMITH_PROJECT=${LANGSMITH_PROJECT:-aegis-rag-sprint115}
      - LANGSMITH_ENDPOINT=${LANGSMITH_ENDPOINT:-https://api.smith.langchain.com}
      - PROMETHEUS_ENABLED=true
      # MCP
      - MCP_SERVER_PORT=3000
      - MCP_AUTH_ENABLED=false
      # Reranking (disabled - sentence-transformers not installed in container)
      - VECTOR_AGENT_USE_RERANKING=false
    depends_on:
      ollama:
        condition: service_healthy
      qdrant:
        condition: service_started
      neo4j:
        condition: service_healthy
      redis:
        condition: service_healthy
      prometheus:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    restart: unless-stopped
    networks:
      - aegis-network
    # Sprint 65 Feature 65.2: GPU support for BGE-M3 embeddings
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1  # Use 1 GPU for embeddings
              capabilities: [gpu]
        limits:
          memory: 16G  # Increased from 8G for CUDA runtime + model

  # ============================================================================
  # AEGIS RAG Frontend - React/Vite Development Server
  # ============================================================================
  # Sprint 92: Auto-start frontend with Docker
  #
  # Features:
  #   - Hot-reload development mode
  #   - Auto-connects to API service
  #   - Accessible on port 5179
  #
  # Usage:
  #   # Start with main services
  #   docker compose -f docker-compose.dgx-spark.yml up -d
  #
  #   # View logs
  #   docker logs -f aegis-frontend
  # ============================================================================
  frontend:
    build:
      context: .
      dockerfile: docker/Dockerfile.frontend
      target: development
    image: aegis-frontend:latest
    container_name: aegis-frontend
    ports:
      - "80:5179"  # Sprint 92: Expose on port 80 for easy access
    volumes:
      # Mount source for hot-reload
      - ./frontend/src:/app/src:ro
      - ./frontend/public:/app/public:ro
      - ./frontend/index.html:/app/index.html:ro
    environment:
      - NODE_ENV=development
      # API URL - IMPORTANT: Use external IP for browser access!
      # The browser runs on user's machine, not in Docker, so it needs
      # the external IP address of the DGX Spark server.
      # Set DGX_SPARK_IP in your .env file or use default
      - VITE_API_BASE_URL=http://${DGX_SPARK_IP:-192.168.178.10}:8000
    depends_on:
      api:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:5179"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    restart: unless-stopped
    networks:
      - aegis-network
    deploy:
      resources:
        limits:
          memory: 1G

  # ============================================================================
  # AEGIS RAG Test Runner - For running pytest in container
  # ============================================================================
  # Usage:
  #   # Run all tests
  #   docker compose -f docker-compose.dgx-spark.yml run --rm test
  #
  #   # Run specific test file
  #   docker compose -f docker-compose.dgx-spark.yml run --rm test pytest tests/unit/components/llm_proxy/test_vlm_factory.py -v
  #
  #   # Run with coverage
  #   docker compose -f docker-compose.dgx-spark.yml run --rm test pytest tests/ --cov=src --cov-report=term-missing
  # ============================================================================
  test:
    build:
      context: .
      dockerfile: docker/Dockerfile.api-test
    image: aegis-rag-test:latest
    container_name: aegis-test
    profiles:
      - test  # Only start when explicitly requested
    volumes:
      - ./src:/app/src:ro
      - ./tests:/app/tests:ro
      - ./config:/app/config:ro
      - ./pyproject.toml:/app/pyproject.toml:ro
    environment:
      - ENVIRONMENT=development
      - DEBUG=true
      - LOG_LEVEL=DEBUG
      # LLM Configuration (same as api service)
      - OLLAMA_BASE_URL=http://ollama:11434
      - OLLAMA_MODEL_GENERATION=${OLLAMA_MODEL_GENERATION:-nemotron-no-think:latest}
      - OLLAMA_MODEL_QUERY=${OLLAMA_MODEL_QUERY:-gpt-oss:20b}
      - OLLAMA_MODEL_EXTRACTION=${OLLAMA_MODEL_EXTRACTION:-gpt-oss:20b}
      - OLLAMA_MODEL_VLM=${OLLAMA_MODEL_VLM:-qwen3-vl:32b}
      - LIGHTRAG_LLM_MODEL=${OLLAMA_MODEL_GENERATION:-nemotron-no-think:latest}
      - GEMMA_MODEL=${OLLAMA_MODEL_GENERATION:-nemotron-no-think:latest}
      - VLM_BACKEND=ollama
      - EMBEDDING_BACKEND=flag-embedding  # Sprint 87: Multi-vector (dense + sparse)
      - ST_MODEL_NAME=BAAI/bge-m3
      # Databases
      - QDRANT_HOST=qdrant
      - QDRANT_PORT=6333
      - NEO4J_URI=bolt://neo4j:7687
      - NEO4J_USER=neo4j
      - NEO4J_PASSWORD=aegis-rag-neo4j-password
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      # Test Configuration
      - PYTEST_ADDOPTS=-v --tb=short
    depends_on:
      - ollama
      - qdrant
      - neo4j
      - redis
    networks:
      - aegis-network
    deploy:
      resources:
        limits:
          memory: 4G

# ============================================================================
# Volumes
# ============================================================================
volumes:
  ollama_data:
    driver: local
  qdrant_data:
    driver: local
  neo4j_data:
    driver: local
  neo4j_logs:
    driver: local
  neo4j_import:
    driver: local
  neo4j_plugins:
    driver: local
  redis_data:
    driver: local
  prometheus_data:
    driver: local
  grafana_data:
    driver: local
  docling_hf_cache:
    driver: local
  docling_ocr_cache:
    driver: local

# ============================================================================
# Networks
# ============================================================================
networks:
  aegis-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.26.0.0/16
