# =============================================================================
# docker-compose.dgx-spark.yml - AegisRAG for NVIDIA DGX Spark
# =============================================================================
# Optimized for DGX Spark (Grace Blackwell, ARM64, 128GB Unified Memory)
#
# Memory Distribution (128GB total):
#   - Qwen3 32B (LLM + Entity Extraction): ~20GB
#   - Qwen3-VL 32B (Vision):               ~21GB
#   - Docling (OCR + Layout):               ~4GB
#   - Qdrant (Vector DB):                   ~8GB
#   - Neo4j (Graph DB):                     ~8GB
#   - Redis (Cache):                        ~4GB
#   - System + Overhead:                   ~10GB
#   - Reserve:                             ~53GB (for concurrent model loading)
#
# Usage:
#   # Start all services
#   docker compose -f docker-compose.dgx-spark.yml up -d
#
#   # Start with ingestion profile (includes Docling)
#   docker compose -f docker-compose.dgx-spark.yml --profile ingestion up -d
#
#   # Pull Qwen3 models (run once after Ollama starts)
#   docker exec aegis-ollama ollama pull qwen3:32b
#   docker exec aegis-ollama ollama pull qwen3-vl:32b
# =============================================================================

services:
  # ============================================================================
  # Ollama - Local LLM Server with Qwen3 Models
  # ============================================================================
  # Models to pull after startup:
  #   - qwen3:32b        (20GB) - Generation & Entity/Relation Extraction
  #   - qwen3-vl:32b     (21GB) - Vision Language Model for images/PDFs
  #   - nomic-embed-text (274MB) - Embeddings (optional, we use BGE-M3)
  # ============================================================================
  ollama:
    image: ollama/ollama:latest
    container_name: aegis-ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
      # DGX Spark: Keep models loaded longer (128GB allows it)
      - OLLAMA_KEEP_ALIVE=60m
      # Allow 2 models loaded simultaneously (Qwen3 + Qwen3-VL)
      # With 128GB unified memory, both fit comfortably
      - OLLAMA_MAX_LOADED_MODELS=2
      # Increase context window for better extraction quality
      - OLLAMA_NUM_CTX=8192
      # Use all available CPU cores for prompt processing
      - OLLAMA_NUM_THREAD=20
    # GPU Support for DGX Spark (Blackwell GPU)
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
        limits:
          # Allow up to 64GB for LLM operations
          # Qwen3-32B (20GB) + Qwen3-VL-32B (21GB) + overhead
          memory: 64G
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    restart: unless-stopped
    networks:
      - aegis-network

  # ============================================================================
  # Docling - GPU-Accelerated Document Parsing (ARM64 Build)
  # ============================================================================
  # IMPORTANT: Requires custom ARM64 image built from Dockerfile.docling-spark
  #
  # Build the image first:
  #   docker build -f docker/Dockerfile.docling-spark -t aegis-docling-spark:latest .
  #
  # Memory: ~4GB for OCR models (EasyOCR + Layout + Table Structure)
  # ============================================================================
  docling:
    # Custom ARM64 image (build from docker/Dockerfile.docling-spark)
    image: aegis-docling-spark:latest
    build:
      context: .
      dockerfile: docker/Dockerfile.docling-spark
    container_name: aegis-docling
    profiles:
      - ingestion  # Only start when explicitly requested
    ports:
      - "8080:5001"
    volumes:
      # Cache volumes to persist downloaded models
      - docling_hf_cache:/root/.cache/huggingface
      - docling_ocr_cache:/root/.EasyOCR
    environment:
      # Port Configuration
      - PORT=5001

      # GPU Configuration for DGX Spark
      - CUDA_VISIBLE_DEVICES=0

      # Model Configuration
      - DOCLING_LAZY_LOADING=true
      - DOCLING_OCR_ENGINE=easyocr
      - DOCLING_TABLE_STRUCTURE=true
      - DOCLING_LAYOUT_ANALYSIS=true

      # Image Extraction for VLM Processing
      - DOCLING_GENERATE_PICTURE_IMAGES=true
      - DOCLING_IMAGES_SCALE=2.0

      # DGX Spark: More workers since we have more memory
      - NUM_WORKERS=4
      - DOCLING_BATCH_SIZE=2
      - DOCLING_TIMEOUT=300

      # Logging
      - LOG_LEVEL=INFO

    # GPU Resource Allocation
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
        limits:
          memory: 8G  # More RAM available on DGX Spark

    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5001/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s

    restart: unless-stopped
    networks:
      - aegis-network

  # ============================================================================
  # Qdrant - Vector Database
  # ============================================================================
  # DGX Spark: Increased memory for larger vector collections
  # ============================================================================
  qdrant:
    image: qdrant/qdrant:v1.11.0
    container_name: aegis-qdrant
    ports:
      - "6333:6333"  # HTTP API
      - "6334:6334"  # gRPC
    volumes:
      - qdrant_data:/qdrant/storage
    environment:
      - QDRANT__SERVICE__GRPC_PORT=6334
      - QDRANT__LOG_LEVEL=INFO
      # DGX Spark: Optimize for larger collections
      - QDRANT__STORAGE__PERFORMANCE__MAX_SEARCH_THREADS=8
    deploy:
      resources:
        limits:
          memory: 8G  # Increased from 4G for larger collections
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:6333/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    restart: unless-stopped
    networks:
      - aegis-network

  # ============================================================================
  # Neo4j - Graph Database
  # ============================================================================
  # DGX Spark: Increased heap for complex graph queries
  # ============================================================================
  neo4j:
    image: neo4j:5.24-community
    container_name: aegis-neo4j
    ports:
      - "7474:7474"  # HTTP
      - "7687:7687"  # Bolt
    volumes:
      - neo4j_data:/data
      - neo4j_logs:/logs
      - neo4j_import:/var/lib/neo4j/import
      - neo4j_plugins:/plugins
    environment:
      - NEO4J_AUTH=neo4j/aegis-rag-neo4j-password
      - NEO4J_PLUGINS=["apoc", "graph-data-science"]
      # DGX Spark: Increased memory allocation
      - NEO4J_dbms_memory_pagecache_size=2G
      - NEO4J_dbms_memory_heap_initial__size=2G
      - NEO4J_dbms_memory_heap_max__size=4G
      - NEO4J_dbms_security_procedures_unrestricted=apoc.*,gds.*
      - NEO4J_dbms_security_procedures_allowlist=apoc.*,gds.*
      - NEO4J_apoc_export_file_enabled=true
      - NEO4J_apoc_import_file_enabled=true
      - NEO4J_apoc_import_file_use__neo4j__config=true
    deploy:
      resources:
        limits:
          memory: 8G  # Increased from 4G
    healthcheck:
      test: ["CMD", "cypher-shell", "-u", "neo4j", "-p", "aegis-rag-neo4j-password", "RETURN 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    restart: unless-stopped
    networks:
      - aegis-network

  # ============================================================================
  # Redis - Cache & Short-Term Memory
  # ============================================================================
  redis:
    image: redis:7-alpine
    container_name: aegis-redis
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    command: >
      redis-server
      --appendonly yes
      --appendfsync everysec
      --maxmemory 4gb
      --maxmemory-policy allkeys-lru
      --save 60 1000
    deploy:
      resources:
        limits:
          memory: 4G  # Increased from 2G
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    restart: unless-stopped
    networks:
      - aegis-network

  # ============================================================================
  # Prometheus - Metrics Collection
  # ============================================================================
  prometheus:
    image: prom/prometheus:latest
    container_name: aegis-prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./config/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
      - '--web.enable-lifecycle'
    deploy:
      resources:
        limits:
          memory: 1G
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:9090/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    restart: unless-stopped
    networks:
      - aegis-network
    depends_on:
      - qdrant
      - neo4j
      - redis

  # ============================================================================
  # Grafana - Metrics Visualization
  # ============================================================================
  grafana:
    image: grafana/grafana:latest
    container_name: aegis-grafana
    ports:
      - "3000:3000"
    volumes:
      - grafana_data:/var/lib/grafana
      - ./config/grafana-datasources.yml:/etc/grafana/provisioning/datasources/datasources.yml:ro
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=aegis-rag-grafana
      - GF_INSTALL_PLUGINS=redis-datasource
      - GF_SERVER_ROOT_URL=http://localhost:3000
    deploy:
      resources:
        limits:
          memory: 512M
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    restart: unless-stopped
    networks:
      - aegis-network
    depends_on:
      - prometheus

# ============================================================================
# Volumes
# ============================================================================
volumes:
  ollama_data:
    driver: local
  qdrant_data:
    driver: local
  neo4j_data:
    driver: local
  neo4j_logs:
    driver: local
  neo4j_import:
    driver: local
  neo4j_plugins:
    driver: local
  redis_data:
    driver: local
  prometheus_data:
    driver: local
  grafana_data:
    driver: local
  docling_hf_cache:
    driver: local
  docling_ocr_cache:
    driver: local

# ============================================================================
# Networks
# ============================================================================
networks:
  aegis-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.26.0.0/16
